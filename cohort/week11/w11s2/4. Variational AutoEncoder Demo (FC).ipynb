{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2403114e",
   "metadata": {},
   "source": [
    "# 4. Variational AutoEncoder Demo (FC)\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69dfad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02170e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# CUDA check\n",
    "CUDA = True\n",
    "device = \"cuda\" if (torch.cuda.is_available() and CUDA) else \"cpu\"\n",
    "print(torch.cuda.is_available())\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028eb66",
   "metadata": {},
   "source": [
    "### Dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db55cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "# - ToTensor\n",
    "# - Image Normalization\n",
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2151db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train datasets/dataloaders\n",
    "train_set = torchvision.datasets.MNIST(root='./data', \\\n",
    "                                       train = True, \\\n",
    "                                       download = True, \\\n",
    "                                       transform = transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, \\\n",
    "                                           batch_size = 32, \\\n",
    "                                           shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144fa8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test datasets/dataloaders\n",
    "test_set = torchvision.datasets.MNIST(root = './data', \\\n",
    "                                      train = False, \\\n",
    "                                      download = True, \\\n",
    "                                      transform = transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, \\\n",
    "                                          batch_size = 4, \\\n",
    "                                          shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5fdad0",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Conv2d and Conv2dTranspose layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07a642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Variational AutoEncoder Model for MNIST\n",
    "class MNIST_VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_channels, init_channels, kernel_size, latent_dim):\n",
    "        super().__init__()\n",
    " \n",
    "        # Encoder with stacked Conv\n",
    "        self.enc1 = nn.Conv2d(image_channels, init_channels, kernel_size, \\\n",
    "                              stride = 2, padding = 1)\n",
    "        self.enc2 = nn.Conv2d(init_channels, init_channels*2, kernel_size, \\\n",
    "                              stride = 2, padding = 1)\n",
    "        self.enc3 = nn.Conv2d(init_channels*2, init_channels*4, kernel_size, \\\n",
    "                              stride = 2, padding = 1)\n",
    "        self.enc4 = nn.Conv2d(init_channels*4, 64, kernel_size, \\\n",
    "                              stride = 2, padding = 0)\n",
    "        \n",
    "        # FC layers for learning representations\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        self.fc_log_var = nn.Linear(128, latent_dim)\n",
    "        self.fc2 = nn.Linear(latent_dim, 64)\n",
    "        \n",
    "        # Decoder, simply mirroring the encoder with ConvTranspose\n",
    "        self.dec1 = nn.ConvTranspose2d(64, init_channels*8, kernel_size, \\\n",
    "                                       stride = 1, padding = 0)\n",
    "        self.dec2 = nn.ConvTranspose2d(init_channels*8, init_channels*4, kernel_size, \\\n",
    "                                       stride = 2, padding = 1)\n",
    "        self.dec3 = nn.ConvTranspose2d(init_channels*4, init_channels*2, kernel_size, \\\n",
    "                                       stride = 2, padding = 1)\n",
    "        self.dec4 = nn.ConvTranspose2d(init_channels*2, image_channels, kernel_size, \\\n",
    "                                       stride = 2, padding = 1)\n",
    "        \n",
    "        \n",
    "    def sample(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        mu: mean from the encoder's latent space\n",
    "        log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        \n",
    "        # Standard deviation\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        \n",
    "        # randn_like is used to produce a vector with same dimensionality as std\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        # Sampling\n",
    "        sample = mu + (eps * std)\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Encoder\n",
    "        x = F.relu(self.enc1(x))\n",
    "        x = F.relu(self.enc2(x))\n",
    "        x = F.relu(self.enc3(x))\n",
    "        x = F.relu(self.enc4(x))\n",
    "        \n",
    "        # Pooling\n",
    "        batch, _, _, _ = x.shape\n",
    "        x = F.adaptive_avg_pool2d(x, 1).reshape(batch, -1)\n",
    "        \n",
    "        # FC layers to get mu and log_var\n",
    "        hidden = self.fc1(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        log_var = self.fc_log_var(hidden)\n",
    "        \n",
    "        # Get the latent vector through reparameterization\n",
    "        z = self.sample(mu, log_var)\n",
    "        z = self.fc2(z)\n",
    "        z = z.view(-1, 64, 1, 1)\n",
    " \n",
    "        # Decoding\n",
    "        x = F.relu(self.dec1(z))\n",
    "        x = F.relu(self.dec2(x))\n",
    "        x = F.relu(self.dec3(x))\n",
    "        x = torch.sigmoid(self.dec4(x))\n",
    "        return x, mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271d102",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b5dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Parameters (part 1 - model parameters)\n",
    "# - 4x4 kernel\n",
    "# - 8 filters\n",
    "# - image channels is set to 1, because MNIST is greayscale\n",
    "# - latent dimension for sampling set to 16 (arbitrarily)\n",
    "kernel_size = 4\n",
    "init_channels = 8\n",
    "image_channels = 1\n",
    "latent_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a140954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MNIST Autoencoder\n",
    "torch.manual_seed(10)\n",
    "model = MNIST_VAE(image_channels, init_channels, kernel_size, latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b7c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Parameters\n",
    "# - BCE Loss, which will be our reconstruction loss for now\n",
    "# - Adam as optimizer, default parameters\n",
    "# - 100 Epochs\n",
    "# - 64 as batch size\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "criterion = nn.MSELoss(reduction = 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0299517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_loss(mse_loss, mu, logvar, alpha = 1):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (MSELoss) and the KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    - param mse_loss: recontruction loss\n",
    "    - param mu: the mean from the latent vector\n",
    "    - param logvar: log variance from the latent vector\n",
    "    - alpha: scaling parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    MSE = bce_loss \n",
    "    KLD = -0.5*torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + alpha*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da48f039",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (3 x 3). Kernel size: (4 x 4). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b5c80f90d0bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# Gets output, mu and logvar values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m# Twofold loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-449d6a3335d2>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menc4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# Pooling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    393\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 395\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    396\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (3 x 3). Kernel size: (4 x 4). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        \n",
    "        # Send data to device\n",
    "        img, _ = data\n",
    "        img = Variable(img).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        # Gets output, mu and logvar values\n",
    "        output, mu, logvar = model(img)\n",
    "        \n",
    "        # Twofold loss\n",
    "        reco_loss = criterion(output, img)\n",
    "        loss = final_loss(reco_loss, mu, logvar)\n",
    "        \n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Display\n",
    "    print('epoch {}/{}, loss {:.4f}'.format(epoch + 1, num_epochs, loss.item()))\n",
    "    loss_list.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260d7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display loss\n",
    "plt.figure()\n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb4d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
