{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "through-glossary",
   "metadata": {},
   "source": [
    "# Homework 9 - Part 2 Notebook\n",
    "\n",
    "In this homework notebook, we will create and train our own SkipGram embedding, by using the short synopsis of the Lion King movie explained to kids in the text.text file.\n",
    "\n",
    "Get familiar with the code and write a small report (2 or 3 pages, idk), with answers to the questions listed at the end of the notebook.\n",
    "\n",
    "**The report must be submitted in PDF format, before April 8th, 11.59pm!**\n",
    "\n",
    "Do not forget to write your name and student ID on the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-saturn",
   "metadata": {},
   "source": [
    "### Imports needed\n",
    "\n",
    "Note, we strongly advise to use a CUDA/GPU machine for this notebook.\n",
    "\n",
    "Technically, this can be done on CPU only, but it will be very slow!\n",
    "\n",
    "If you decide to use it on CPU, you might also have to change some of the .cuda() methods used on torch tensors and models in this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "diverse-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923718d7",
   "metadata": {},
   "source": [
    "### Testing for CUDA\n",
    "\n",
    "We advise running on GPU and setting up CUDA on your machine as it might drastically speed up the running time for this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f80f5c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "# Define device for torch\n",
    "use_cuda = True\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-tattoo",
   "metadata": {},
   "source": [
    "### Step 1. Produce some data based on a given text for training our SkipGram model    \n",
    "\n",
    "The functions below will be used to produce our dataset for training the SkipGram model.\n",
    "\n",
    "The dataset text consists of a short description of the story behing the movie The Lion King, explained in simple terms to kids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "protected-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_train(text, context_window):\n",
    "    \"\"\"\n",
    "    This function receives the text as a list of words, in lowercase format.\n",
    "    It then returns data, a list of all the possible (x,y) pairs with\n",
    "    - x being the middle word of the sentence of length 2*context_window+1,\n",
    "    - y being a list of 2k words, containing the k preceding words and the k\n",
    "    posterior words.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get data from list of words in text, using a context window of size k = context_window\n",
    "    data = []\n",
    "    for i in range(context_window, len(text) - context_window):\n",
    "        target = [text[i+e] for e in range(-context_window, context_window+1) if i+e != i]\n",
    "        input_word = text[i]\n",
    "        data.append((input_word, target))\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sensitive-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text():\n",
    "    \"\"\"\n",
    "    This function loads the string of text from the text.txt file,\n",
    "    and produces a list of words in string format, as variable text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load corpus from file\n",
    "    with open(\"./text.txt\", 'r', encoding=\"utf8\",) as f:\n",
    "        corpus = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    # Join corpus into a single string\n",
    "    text = \"\"\n",
    "    for s in corpus:\n",
    "        l = s.split()\n",
    "        for s2 in l:\n",
    "            # Removes all special characters from string\n",
    "            s2 = ''.join(filter(str.isalnum, s2))\n",
    "            s2 += ' '\n",
    "            text += s2.lower()\n",
    "    text = text.split()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "identical-heather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'lion', 'king', 'is', 'an', 'animated', 'movie', 'made', 'by', 'walt', 'disney', 'in', '1994', 'it', 'was', 'the', 'most', 'successful', 'animated', 'movie', 'of', 'the', '1990s', 'the', 'movie', 'is', 'about', 'a', 'young', 'lion', 'prince', 'who', 'learns', 'about', 'his', 'role', 'as', 'prince', 'and', 'in', 'the', 'circle', 'of', 'life', 'it', 'is', 'dedicated', 'to', 'frank', 'wells', 'who', 'was', 'the', 'president', 'of', 'the', 'walt', 'disney', 'company', 'and', 'died', 'shortly', 'before', 'the', 'movie', 'was', 'released', 'into', 'theaters', 'on', 'june', '15', '1994', 'it', 'was', 'the', 'first', 'fulllength', 'disney', 'movie', 'to', 'feature', 'no', 'human', 'characters', 'since', 'bambi', 'much', 'of', 'the', 'voice', 'acting', 'work', 'was', 'done', 'by', 'wellknown', 'actors', 'including', 'james', 'earl', 'jones', 'jeremy', 'irons', 'matthew', 'broderick', 'whoopi', 'goldberg', 'rowan', 'atkinson', 'jonathan', 'taylor', 'thomas', 'and', 'nathan', 'lane', 'the', 'lion', 'king', 'is', 'a', 'musical', 'the', 'songs', 'have', 'music', 'by', 'elton', 'john', 'and', 'lyrics', 'by', 'tim', 'rice', 'computer', 'animation', 'was', 'used', 'a', 'lot', 'when', 'making', 'the', 'movie', 'like', 'during', 'the', 'song', 'circle', 'of', 'life', 'and', 'others', 'when', 'they', 'were', 'making', 'it', 'this', 'movie', 'was', 'thought', 'of', 'as', 'just', 'alright', 'compared', 'to', 'the', 'movie', 'they', 'were', 'going', 'to', 'make', 'after', 'that', 'which', 'would', 'be', 'pocahontas', 'the', 'studio', 'released', 'the', 'trailer', 'and', 'found', 'that', 'many', 'people', 'liked', 'it', 'especially', 'the', 'song', 'circle', 'of', 'life', 'when', 'it', 'was', 'released', 'the', 'movie', 'became', 'the', 'most', 'successeful', 'movie', 'worldwide', 'in', 'the', 'united', 'states', 'forrest', 'gump', 'was', 'most', 'successful', 'of', 'that', 'year', 'and', 'the', 'most', 'successful', 'animated', 'feature', 'movie', 'of', 'all', 'time', 'until', 'finding', 'nemo', 'since', 'then', 'shrek', '2', 'has', 'become', 'more', 'successful', 'than', 'finding', 'nemo', 'making', 'the', 'lion', 'king', 'the', 'third', 'most', 'successful', 'the', 'movie', 'was', 'also', 'made', 'into', 'an', 'awardwinning', 'stage', 'musical', 'the', 'stage', 'show', 'first', 'opened', 'on', 'november', '13', '1997', 'in', 'new', 'york', 'city', 'and', 'it', 'was', 'a', 'big', 'success', 'a', 'version', 'opened', 'later', 'in', 'london', 'england', 'many', 'other', 'shows', 'of', 'the', 'lion', 'king', 'have', 'been', 'shown', 'across', 'the', 'world', 'and', 'is', 'one', 'of', 'the', 'uks', 'biggest', 'and', 'most', 'popular', 'shows', 'in', 'the', 'movies', 'opening', 'scene', 'lots', 'of', 'animals', 'and', 'birds', 'gather', 'at', 'pride', 'rock', 'to', 'see', 'simba', 'the', 'new', 'prince', 'who', 'has', 'just', 'been', 'born', 'simba', 'is', 'the', 'son', 'of', 'mufasa', 'and', 'sarabi', 'rafiki', 'picks', 'up', 'simba', 'and', 'lifts', 'him', 'high', 'up', 'so', 'that', 'all', 'of', 'the', 'animals', 'can', 'see', 'the', 'animals', 'celebrate', 'and', 'rejoice', 'but', 'scar', 'mufasas', 'brother', 'is', 'jealous', 'because', 'simba', 'will', 'be', 'king', 'instead', 'of', 'him', 'scar', 'lies', 'to', 'simba', 'about', 'a', 'dangerous', 'place', 'called', 'the', 'elephant', 'graveyard', 'scar', 'says', 'that', 'only', 'brave', 'lions', 'go', 'there', 'causing', 'simba', 'to', 'be', 'interested', 'even', 'though', 'mufasa', 'has', 'forbidden', 'simba', 'from', 'going', 'there', 'simba', 'lies', 'to', 'his', 'mother', 'sarabi', 'about', 'going', 'to', 'the', 'water', 'hole', 'when', 'he', 'is', 'actually', 'going', 'to', 'the', 'elephant', 'graveyard', 'simbas', 'friend', 'nala', 'and', 'zazu', 'the', 'kings', 'messenger', 'go', 'with', 'simba', 'simba', 'and', 'nala', 'trick', 'zazu', 'with', 'the', 'song', 'i', 'just', 'cant', 'wait', 'to', 'be', 'king', 'and', 'run', 'away', 'from', 'him', 'simba', 'and', 'nala', 'find', 'the', 'elephant', 'graveyard', 'but', 'are', 'chased', 'by', 'the', 'three', 'hyenas', 'shenzi', 'banzai', 'and', 'ed', 'mufasa', 'saves', 'his', 'son', 'and', 'nala', 'and', 'takes', 'them', 'both', 'home', 'mufasa', 'speaks', 'to', 'simba', 'alone', 'and', 'explains', 'to', 'simba', 'that', 'being', 'brave', 'is', 'not', 'about', 'looking', 'for', 'danger', 'he', 'also', 'explains', 'that', 'the', 'great', 'kings', 'of', 'the', 'past', 'look', 'down', 'from', 'the', 'stars', 'and', 'watch', 'over', 'simba', 'scar', 'in', 'the', 'elephant', 'graveyard', 'is', 'angry', 'with', 'the', 'hyenas', 'because', 'they', 'did', 'not', 'kill', 'simba', 'it', 'is', 'revealed', 'that', 'the', 'hyenas', 'are', 'working', 'for', 'scar', 'during', 'scars', 'song', 'be', 'prepared', 'the', 'next', 'day', 'scar', 'takes', 'simba', 'into', 'a', 'gorge', 'long', 'deep', 'hole', 'in', 'the', 'ground', 'also', 'known', 'as', 'a', 'valley', 'where', 'he', 'explains', 'that', 'mufasa', 'has', 'a', 'wonderful', 'surprise', 'waiting', 'scar', 'has', 'actually', 'planned', 'a', 'wildebeest', 'stampede', 'with', 'the', 'hyenas', 'simba', 'is', 'trapped', 'in', 'the', 'gorge', 'as', 'the', 'wildebeest', 'run', 'towards', 'him', 'scar', 'tells', 'mufasa', 'that', 'simba', 'is', 'in', 'trouble', 'and', 'mufasa', 'rescues', 'his', 'son', 'scar', 'then', 'throws', 'mufasa', 'into', 'the', 'stampede', 'and', 'mufasa', 'dies', 'scar', 'blames', 'simba', 'for', 'the', 'death', 'of', 'mufasa', 'and', 'simba', 'runs', 'away', 'scar', 'becomes', 'king', 'and', 'tells', 'everyone', 'that', 'simba', 'and', 'mufasa', 'are', 'dead', 'simba', 'runs', 'to', 'a', 'desert', 'and', 'collapses', 'he', 'is', 'rescued', 'by', 'timon', 'the', 'meerkat', 'and', 'pumbaa', 'the', 'warthog', 'timon', 'and', 'pumbaa', 'live', 'in', 'the', 'jungle', 'and', 'are', 'very', 'relaxed', 'which', 'they', 'show', 'in', 'their', 'song', 'hakuna', 'matata', 'timon', 'and', 'pumbaa', 'look', 'after', 'simba', 'until', 'simba', 'is', 'an', 'adult', 'lion', 'one', 'day', 'a', 'lioness', 'female', 'lion', 'comes', 'to', 'the', 'jungle', 'and', 'tries', 'to', 'kill', 'and', 'eat', 'pumbaa', 'simba', 'fights', 'the', 'lioness', 'because', 'he', 'wants', 'to', 'save', 'pumbaas', 'life', 'while', 'the', 'two', 'lions', 'are', 'fighting', 'simba', 'finds', 'out', 'that', 'the', 'lioness', 'is', 'his', 'friend', 'nala', 'they', 'are', 'very', 'happy', 'to', 'see', 'each', 'other', 'and', 'they', 'fall', 'in', 'love', 'nala', 'wants', 'simba', 'to', 'go', 'home', 'and', 'fight', 'scar', 'because', 'scar', 'is', 'a', 'bad', 'king', 'simba', 'will', 'not', 'go', 'home', 'because', 'he', 'thinks', 'that', 'he', 'killed', 'mufasa', 'and', 'he', 'does', 'not', 'want', 'his', 'family', 'to', 'know', 'rafiki', 'comes', 'to', 'the', 'jungle', 'and', 'takes', 'simba', 'to', 'a', 'field', 'in', 'the', 'sky', 'above', 'the', 'field', 'mufasas', 'ghost', 'appears', 'and', 'tells', 'simba', 'that', 'he', 'must', 'go', 'home', 'because', 'simba', 'is', 'the', 'right', 'king', 'after', 'this', 'simba', 'goes', 'home', 'to', 'pride', 'rock', 'nala', 'timon', 'and', 'pumbaa', 'follow', 'him', 'when', 'they', 'get', 'to', 'pride', 'rock', 'they', 'find', 'that', 'the', 'land', 'is', 'dry', 'and', 'the', 'animals', 'have', 'gone', 'at', 'pride', 'rock', 'simba', 'sees', 'scar', 'hitting', 'sarabi', 'this', 'makes', 'simba', 'angry', 'and', 'he', 'tries', 'to', 'make', 'scar', 'leave', 'pride', 'rock', 'scar', 'does', 'not', 'leave', 'and', 'makes', 'simba', 'fall', 'over', 'the', 'edge', 'of', 'pride', 'rock', 'simba', 'does', 'not', 'fall', 'and', 'holds', 'on', 'to', 'the', 'edge', 'scar', 'thinks', 'that', 'he', 'was', 'won', 'so', 'he', 'tells', 'simba', 'the', 'truth', 'about', 'the', 'death', 'of', 'mufasa', 'that', 'scar', 'actually', 'killed', 'mufasa', 'simba', 'is', 'upset', 'and', 'a', 'big', 'fight', 'happens', 'the', 'lionesses', 'fight', 'the', 'hyenas', 'and', 'simba', 'fights', 'scar', 'while', 'the', 'fighting', 'is', 'going', 'on', 'lightning', 'hits', 'a', 'dead', 'tree', 'and', 'starts', 'a', 'fire', 'scar', 'and', 'simba', 'fight', 'on', 'top', 'of', 'pride', 'rock', 'scar', 'does', 'not', 'want', 'to', 'die', 'and', 'lies', 'to', 'simba', 'that', 'the', 'hyenas', 'are', 'to', 'blame', 'for', 'everything', 'another', 'fight', 'happens', 'and', 'simba', 'throws', 'scar', 'over', 'the', 'edge', 'scar', 'does', 'not', 'die', 'after', 'the', 'fall', 'but', 'the', 'hyenas', 'attack', 'and', 'kill', 'him', 'the', 'hyenas', 'are', 'angry', 'that', 'scar', 'blamed', 'them', 'rain', 'falls', 'and', 'puts', 'out', 'the', 'fire', 'simba', 'walks', 'to', 'the', 'top', 'of', 'pride', 'rock', 'and', 'roars', 'much', 'later', 'the', 'animals', 'come', 'back', 'at', 'the', 'end', 'of', 'the', 'movie', 'rafiki', 'picks', 'up', 'simba', 'and', 'nalas', 'daughter', 'and', 'lifts', 'her', 'up', 'high', 'above', 'pride', 'rock', 'so', 'the', 'animals', 'below', 'can', 'see']\n"
     ]
    }
   ],
   "source": [
    "text = create_text()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "separate-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(text, context_window):\n",
    "    \"\"\"\n",
    "    This function receives the text and context window size.\n",
    "    It produces four outputs:\n",
    "    - vocab, a set containing the words found in text.txt,\n",
    "    without any doublons,\n",
    "    - word2index, a dictionary to convert words to their integer index,\n",
    "    - word2index, a dictionary to convert integer index to their respective words,\n",
    "    - data, containing our (x,y) pairs for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create vocabulary set V\n",
    "    vocab = set(text)\n",
    "    \n",
    "    # Word to index and index 2 word converters\n",
    "    word2index = {w:i for i,w in enumerate(vocab)}\n",
    "    index2word = {i:w for i,w in enumerate(vocab)}\n",
    "    \n",
    "    # Generate data\n",
    "    data = text_to_train(text, context_window)\n",
    "    \n",
    "    return vocab, data, word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indie-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, data, word2index, index2word = generate_data(text, context_window = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "presidential-sauce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 389 different words.\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset contains\", len(vocab), \"different words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "588551ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains the following words:\n",
      "{'rowan', 'deep', 'bambi', 'going', 'cant', 'alone', 'mufasa', 'away', 'water', 'june', 'explains', 'across', 'waiting', 'happens', 'wonderful', 'leave', 'scars', 'one', 'because', 'interested', 'gorge', 'actors', 'know', 'much', 'sky', 'uks', 'rice', 'only', 'working', 'each', 'finds', 'whoopi', 'broderick', 'feature', 'president', 'land', 'music', 'than', '1990s', 'blames', 'fire', 'get', 'an', 'about', 'this', 'forbidden', 'won', 'dies', 'time', 'circle', 'killed', 'movies', 'when', 'life', 'role', 'jones', 'gump', 'become', 'relaxed', 'voice', 'is', 'london', 'comes', 'lane', 'they', 'field', 'songs', 'king', 'his', 'hits', 'up', 'walt', 'pocahontas', 'in', 'goldberg', 'wells', 'it', 'picks', 'scar', 'angry', 'on', 'shortly', 'appears', 'says', 'for', 'matata', 'were', 'jeremy', 'york', 'rescued', 'starts', 'ghost', 'collapses', 'worldwide', 'lyrics', 'hakuna', 'at', 'compared', 'song', 'live', 'who', 'a', 'two', 'learns', 'walks', 'lion', 'nathan', 'will', 'third', 'did', 'world', 'version', 'earl', 'dangerous', 'prince', 'into', 'them', 'valley', 'happy', 'which', 'shows', 'nemo', 'matthew', 'wait', 'follow', 'jonathan', 'and', 'above', 'danger', 'female', 'young', 'november', 'come', 'hyenas', 'eat', 'born', 'was', 'make', 'runs', 'has', 'acting', 'sarabi', 'dead', 'hole', 'found', 'stage', 'made', 'musical', 'mother', 'irons', 'elton', 'rain', 'death', 'family', 'more', 'work', 'trailer', 'dedicated', 'kings', 'die', 'watch', 'great', 'jealous', 'fall', 'prepared', 'england', 'forrest', 'high', 'very', 'became', 'looking', 'blame', 'warthog', 'states', 'including', 'rejoice', 'gone', 'go', 'rescues', 'desert', 'another', 'down', 'also', 'lots', 'wellknown', 'be', 'simbas', 'right', 'stampede', 'during', 'run', 'he', 'united', 'where', 'adult', 'fight', 'nala', 'lions', 'awardwinning', 'pumbaa', 'zazu', 'then', 'frank', 'successeful', 'people', 'brother', 'saves', 'year', 'save', 'want', 'takes', 'animation', 'towards', 'many', 'their', 'her', 'out', 'becomes', 'fights', 'upset', 'lioness', 'bad', 'being', 'show', 'shrek', 'biggest', 'love', 'have', 'since', 'everything', 'others', 'tries', 'opened', 'as', 'blamed', 'after', 'find', 'lot', 'timon', 'below', 'instead', 'known', 'tells', 'even', 'everyone', 'see', 'makes', 'no', 'john', 'lionesses', 'long', 'simba', 'company', 'liked', 'both', 'human', 'pumbaas', '1994', 'celebrate', 'puts', 'to', 'elephant', 'would', 'later', 'home', 'sees', 'thinks', 'finding', 'most', 'day', 'daughter', 'like', 'planned', 'studio', 'does', 'mufasas', 'theaters', 'other', 'while', 'lightning', 'tim', 'first', 'son', 'rafiki', 'back', 'jungle', 'scene', 'edge', 'before', 'thomas', 'until', 'attack', 'making', 'trapped', 'ground', 'not', 'alright', 'by', 'big', 'chased', 'with', 'meerkat', 'holds', 'just', 'top', 'shenzi', 'actually', 'friend', 'especially', 'gather', 'tree', 'pride', 'called', 'the', 'been', 'revealed', 'wants', 'opening', 'dry', 'next', 'from', 'animals', 'past', 'fighting', 'end', 'so', 'though', 'james', 'there', 'rock', 'taylor', 'shown', 'new', 'thought', 'computer', 'banzai', 'lies', '15', 'over', 'popular', 'lifts', 'movie', 'causing', 'falls', 'animated', 'throws', 'can', 'him', 'look', 'birds', 'trick', 'released', '13', 'messenger', 'nalas', '1997', 'truth', 'atkinson', 'stars', 'graveyard', 'done', 'all', 'ed', 'surprise', 'goes', 'of', 'that', 'trouble', 'must', 'kill', 'success', 'fulllength', 'but', 'three', 'speaks', 'died', 'are', 'successful', '2', 'wildebeest', 'place', 'hitting', 'disney', 'characters', 'brave', 'used', 'roars', 'city', 'i'}\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset contains the following words:\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "statewide-passion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rowan': 0, 'deep': 1, 'bambi': 2, 'going': 3, 'cant': 4, 'alone': 5, 'mufasa': 6, 'away': 7, 'water': 8, 'june': 9, 'explains': 10, 'across': 11, 'waiting': 12, 'happens': 13, 'wonderful': 14, 'leave': 15, 'scars': 16, 'one': 17, 'because': 18, 'interested': 19, 'gorge': 20, 'actors': 21, 'know': 22, 'much': 23, 'sky': 24, 'uks': 25, 'rice': 26, 'only': 27, 'working': 28, 'each': 29, 'finds': 30, 'whoopi': 31, 'broderick': 32, 'feature': 33, 'president': 34, 'land': 35, 'music': 36, 'than': 37, '1990s': 38, 'blames': 39, 'fire': 40, 'get': 41, 'an': 42, 'about': 43, 'this': 44, 'forbidden': 45, 'won': 46, 'dies': 47, 'time': 48, 'circle': 49, 'killed': 50, 'movies': 51, 'when': 52, 'life': 53, 'role': 54, 'jones': 55, 'gump': 56, 'become': 57, 'relaxed': 58, 'voice': 59, 'is': 60, 'london': 61, 'comes': 62, 'lane': 63, 'they': 64, 'field': 65, 'songs': 66, 'king': 67, 'his': 68, 'hits': 69, 'up': 70, 'walt': 71, 'pocahontas': 72, 'in': 73, 'goldberg': 74, 'wells': 75, 'it': 76, 'picks': 77, 'scar': 78, 'angry': 79, 'on': 80, 'shortly': 81, 'appears': 82, 'says': 83, 'for': 84, 'matata': 85, 'were': 86, 'jeremy': 87, 'york': 88, 'rescued': 89, 'starts': 90, 'ghost': 91, 'collapses': 92, 'worldwide': 93, 'lyrics': 94, 'hakuna': 95, 'at': 96, 'compared': 97, 'song': 98, 'live': 99, 'who': 100, 'a': 101, 'two': 102, 'learns': 103, 'walks': 104, 'lion': 105, 'nathan': 106, 'will': 107, 'third': 108, 'did': 109, 'world': 110, 'version': 111, 'earl': 112, 'dangerous': 113, 'prince': 114, 'into': 115, 'them': 116, 'valley': 117, 'happy': 118, 'which': 119, 'shows': 120, 'nemo': 121, 'matthew': 122, 'wait': 123, 'follow': 124, 'jonathan': 125, 'and': 126, 'above': 127, 'danger': 128, 'female': 129, 'young': 130, 'november': 131, 'come': 132, 'hyenas': 133, 'eat': 134, 'born': 135, 'was': 136, 'make': 137, 'runs': 138, 'has': 139, 'acting': 140, 'sarabi': 141, 'dead': 142, 'hole': 143, 'found': 144, 'stage': 145, 'made': 146, 'musical': 147, 'mother': 148, 'irons': 149, 'elton': 150, 'rain': 151, 'death': 152, 'family': 153, 'more': 154, 'work': 155, 'trailer': 156, 'dedicated': 157, 'kings': 158, 'die': 159, 'watch': 160, 'great': 161, 'jealous': 162, 'fall': 163, 'prepared': 164, 'england': 165, 'forrest': 166, 'high': 167, 'very': 168, 'became': 169, 'looking': 170, 'blame': 171, 'warthog': 172, 'states': 173, 'including': 174, 'rejoice': 175, 'gone': 176, 'go': 177, 'rescues': 178, 'desert': 179, 'another': 180, 'down': 181, 'also': 182, 'lots': 183, 'wellknown': 184, 'be': 185, 'simbas': 186, 'right': 187, 'stampede': 188, 'during': 189, 'run': 190, 'he': 191, 'united': 192, 'where': 193, 'adult': 194, 'fight': 195, 'nala': 196, 'lions': 197, 'awardwinning': 198, 'pumbaa': 199, 'zazu': 200, 'then': 201, 'frank': 202, 'successeful': 203, 'people': 204, 'brother': 205, 'saves': 206, 'year': 207, 'save': 208, 'want': 209, 'takes': 210, 'animation': 211, 'towards': 212, 'many': 213, 'their': 214, 'her': 215, 'out': 216, 'becomes': 217, 'fights': 218, 'upset': 219, 'lioness': 220, 'bad': 221, 'being': 222, 'show': 223, 'shrek': 224, 'biggest': 225, 'love': 226, 'have': 227, 'since': 228, 'everything': 229, 'others': 230, 'tries': 231, 'opened': 232, 'as': 233, 'blamed': 234, 'after': 235, 'find': 236, 'lot': 237, 'timon': 238, 'below': 239, 'instead': 240, 'known': 241, 'tells': 242, 'even': 243, 'everyone': 244, 'see': 245, 'makes': 246, 'no': 247, 'john': 248, 'lionesses': 249, 'long': 250, 'simba': 251, 'company': 252, 'liked': 253, 'both': 254, 'human': 255, 'pumbaas': 256, '1994': 257, 'celebrate': 258, 'puts': 259, 'to': 260, 'elephant': 261, 'would': 262, 'later': 263, 'home': 264, 'sees': 265, 'thinks': 266, 'finding': 267, 'most': 268, 'day': 269, 'daughter': 270, 'like': 271, 'planned': 272, 'studio': 273, 'does': 274, 'mufasas': 275, 'theaters': 276, 'other': 277, 'while': 278, 'lightning': 279, 'tim': 280, 'first': 281, 'son': 282, 'rafiki': 283, 'back': 284, 'jungle': 285, 'scene': 286, 'edge': 287, 'before': 288, 'thomas': 289, 'until': 290, 'attack': 291, 'making': 292, 'trapped': 293, 'ground': 294, 'not': 295, 'alright': 296, 'by': 297, 'big': 298, 'chased': 299, 'with': 300, 'meerkat': 301, 'holds': 302, 'just': 303, 'top': 304, 'shenzi': 305, 'actually': 306, 'friend': 307, 'especially': 308, 'gather': 309, 'tree': 310, 'pride': 311, 'called': 312, 'the': 313, 'been': 314, 'revealed': 315, 'wants': 316, 'opening': 317, 'dry': 318, 'next': 319, 'from': 320, 'animals': 321, 'past': 322, 'fighting': 323, 'end': 324, 'so': 325, 'though': 326, 'james': 327, 'there': 328, 'rock': 329, 'taylor': 330, 'shown': 331, 'new': 332, 'thought': 333, 'computer': 334, 'banzai': 335, 'lies': 336, '15': 337, 'over': 338, 'popular': 339, 'lifts': 340, 'movie': 341, 'causing': 342, 'falls': 343, 'animated': 344, 'throws': 345, 'can': 346, 'him': 347, 'look': 348, 'birds': 349, 'trick': 350, 'released': 351, '13': 352, 'messenger': 353, 'nalas': 354, '1997': 355, 'truth': 356, 'atkinson': 357, 'stars': 358, 'graveyard': 359, 'done': 360, 'all': 361, 'ed': 362, 'surprise': 363, 'goes': 364, 'of': 365, 'that': 366, 'trouble': 367, 'must': 368, 'kill': 369, 'success': 370, 'fulllength': 371, 'but': 372, 'three': 373, 'speaks': 374, 'died': 375, 'are': 376, 'successful': 377, '2': 378, 'wildebeest': 379, 'place': 380, 'hitting': 381, 'disney': 382, 'characters': 383, 'brave': 384, 'used': 385, 'roars': 386, 'city': 387, 'i': 388}\n"
     ]
    }
   ],
   "source": [
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unique-profession",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'rowan', 1: 'deep', 2: 'bambi', 3: 'going', 4: 'cant', 5: 'alone', 6: 'mufasa', 7: 'away', 8: 'water', 9: 'june', 10: 'explains', 11: 'across', 12: 'waiting', 13: 'happens', 14: 'wonderful', 15: 'leave', 16: 'scars', 17: 'one', 18: 'because', 19: 'interested', 20: 'gorge', 21: 'actors', 22: 'know', 23: 'much', 24: 'sky', 25: 'uks', 26: 'rice', 27: 'only', 28: 'working', 29: 'each', 30: 'finds', 31: 'whoopi', 32: 'broderick', 33: 'feature', 34: 'president', 35: 'land', 36: 'music', 37: 'than', 38: '1990s', 39: 'blames', 40: 'fire', 41: 'get', 42: 'an', 43: 'about', 44: 'this', 45: 'forbidden', 46: 'won', 47: 'dies', 48: 'time', 49: 'circle', 50: 'killed', 51: 'movies', 52: 'when', 53: 'life', 54: 'role', 55: 'jones', 56: 'gump', 57: 'become', 58: 'relaxed', 59: 'voice', 60: 'is', 61: 'london', 62: 'comes', 63: 'lane', 64: 'they', 65: 'field', 66: 'songs', 67: 'king', 68: 'his', 69: 'hits', 70: 'up', 71: 'walt', 72: 'pocahontas', 73: 'in', 74: 'goldberg', 75: 'wells', 76: 'it', 77: 'picks', 78: 'scar', 79: 'angry', 80: 'on', 81: 'shortly', 82: 'appears', 83: 'says', 84: 'for', 85: 'matata', 86: 'were', 87: 'jeremy', 88: 'york', 89: 'rescued', 90: 'starts', 91: 'ghost', 92: 'collapses', 93: 'worldwide', 94: 'lyrics', 95: 'hakuna', 96: 'at', 97: 'compared', 98: 'song', 99: 'live', 100: 'who', 101: 'a', 102: 'two', 103: 'learns', 104: 'walks', 105: 'lion', 106: 'nathan', 107: 'will', 108: 'third', 109: 'did', 110: 'world', 111: 'version', 112: 'earl', 113: 'dangerous', 114: 'prince', 115: 'into', 116: 'them', 117: 'valley', 118: 'happy', 119: 'which', 120: 'shows', 121: 'nemo', 122: 'matthew', 123: 'wait', 124: 'follow', 125: 'jonathan', 126: 'and', 127: 'above', 128: 'danger', 129: 'female', 130: 'young', 131: 'november', 132: 'come', 133: 'hyenas', 134: 'eat', 135: 'born', 136: 'was', 137: 'make', 138: 'runs', 139: 'has', 140: 'acting', 141: 'sarabi', 142: 'dead', 143: 'hole', 144: 'found', 145: 'stage', 146: 'made', 147: 'musical', 148: 'mother', 149: 'irons', 150: 'elton', 151: 'rain', 152: 'death', 153: 'family', 154: 'more', 155: 'work', 156: 'trailer', 157: 'dedicated', 158: 'kings', 159: 'die', 160: 'watch', 161: 'great', 162: 'jealous', 163: 'fall', 164: 'prepared', 165: 'england', 166: 'forrest', 167: 'high', 168: 'very', 169: 'became', 170: 'looking', 171: 'blame', 172: 'warthog', 173: 'states', 174: 'including', 175: 'rejoice', 176: 'gone', 177: 'go', 178: 'rescues', 179: 'desert', 180: 'another', 181: 'down', 182: 'also', 183: 'lots', 184: 'wellknown', 185: 'be', 186: 'simbas', 187: 'right', 188: 'stampede', 189: 'during', 190: 'run', 191: 'he', 192: 'united', 193: 'where', 194: 'adult', 195: 'fight', 196: 'nala', 197: 'lions', 198: 'awardwinning', 199: 'pumbaa', 200: 'zazu', 201: 'then', 202: 'frank', 203: 'successeful', 204: 'people', 205: 'brother', 206: 'saves', 207: 'year', 208: 'save', 209: 'want', 210: 'takes', 211: 'animation', 212: 'towards', 213: 'many', 214: 'their', 215: 'her', 216: 'out', 217: 'becomes', 218: 'fights', 219: 'upset', 220: 'lioness', 221: 'bad', 222: 'being', 223: 'show', 224: 'shrek', 225: 'biggest', 226: 'love', 227: 'have', 228: 'since', 229: 'everything', 230: 'others', 231: 'tries', 232: 'opened', 233: 'as', 234: 'blamed', 235: 'after', 236: 'find', 237: 'lot', 238: 'timon', 239: 'below', 240: 'instead', 241: 'known', 242: 'tells', 243: 'even', 244: 'everyone', 245: 'see', 246: 'makes', 247: 'no', 248: 'john', 249: 'lionesses', 250: 'long', 251: 'simba', 252: 'company', 253: 'liked', 254: 'both', 255: 'human', 256: 'pumbaas', 257: '1994', 258: 'celebrate', 259: 'puts', 260: 'to', 261: 'elephant', 262: 'would', 263: 'later', 264: 'home', 265: 'sees', 266: 'thinks', 267: 'finding', 268: 'most', 269: 'day', 270: 'daughter', 271: 'like', 272: 'planned', 273: 'studio', 274: 'does', 275: 'mufasas', 276: 'theaters', 277: 'other', 278: 'while', 279: 'lightning', 280: 'tim', 281: 'first', 282: 'son', 283: 'rafiki', 284: 'back', 285: 'jungle', 286: 'scene', 287: 'edge', 288: 'before', 289: 'thomas', 290: 'until', 291: 'attack', 292: 'making', 293: 'trapped', 294: 'ground', 295: 'not', 296: 'alright', 297: 'by', 298: 'big', 299: 'chased', 300: 'with', 301: 'meerkat', 302: 'holds', 303: 'just', 304: 'top', 305: 'shenzi', 306: 'actually', 307: 'friend', 308: 'especially', 309: 'gather', 310: 'tree', 311: 'pride', 312: 'called', 313: 'the', 314: 'been', 315: 'revealed', 316: 'wants', 317: 'opening', 318: 'dry', 319: 'next', 320: 'from', 321: 'animals', 322: 'past', 323: 'fighting', 324: 'end', 325: 'so', 326: 'though', 327: 'james', 328: 'there', 329: 'rock', 330: 'taylor', 331: 'shown', 332: 'new', 333: 'thought', 334: 'computer', 335: 'banzai', 336: 'lies', 337: '15', 338: 'over', 339: 'popular', 340: 'lifts', 341: 'movie', 342: 'causing', 343: 'falls', 344: 'animated', 345: 'throws', 346: 'can', 347: 'him', 348: 'look', 349: 'birds', 350: 'trick', 351: 'released', 352: '13', 353: 'messenger', 354: 'nalas', 355: '1997', 356: 'truth', 357: 'atkinson', 358: 'stars', 359: 'graveyard', 360: 'done', 361: 'all', 362: 'ed', 363: 'surprise', 364: 'goes', 365: 'of', 366: 'that', 367: 'trouble', 368: 'must', 369: 'kill', 370: 'success', 371: 'fulllength', 372: 'but', 373: 'three', 374: 'speaks', 375: 'died', 376: 'are', 377: 'successful', 378: '2', 379: 'wildebeest', 380: 'place', 381: 'hitting', 382: 'disney', 383: 'characters', 384: 'brave', 385: 'used', 386: 'roars', 387: 'city', 388: 'i'}\n"
     ]
    }
   ],
   "source": [
    "print(index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unavailable-banks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('king', ['the', 'lion', 'is', 'an']), ('is', ['lion', 'king', 'an', 'animated']), ('an', ['king', 'is', 'animated', 'movie']), ('animated', ['is', 'an', 'movie', 'made']), ('movie', ['an', 'animated', 'made', 'by']), ('made', ['animated', 'movie', 'by', 'walt']), ('by', ['movie', 'made', 'walt', 'disney']), ('walt', ['made', 'by', 'disney', 'in']), ('disney', ['by', 'walt', 'in', '1994']), ('in', ['walt', 'disney', '1994', 'it']), ('1994', ['disney', 'in', 'it', 'was']), ('it', ['in', '1994', 'was', 'the']), ('was', ['1994', 'it', 'the', 'most']), ('the', ['it', 'was', 'most', 'successful']), ('most', ['was', 'the', 'successful', 'animated']), ('successful', ['the', 'most', 'animated', 'movie']), ('animated', ['most', 'successful', 'movie', 'of']), ('movie', ['successful', 'animated', 'of', 'the']), ('of', ['animated', 'movie', 'the', '1990s']), ('the', ['movie', 'of', '1990s', 'the']), ('1990s', ['of', 'the', 'the', 'movie']), ('the', ['the', '1990s', 'movie', 'is']), ('movie', ['1990s', 'the', 'is', 'about']), ('is', ['the', 'movie', 'about', 'a']), ('about', ['movie', 'is', 'a', 'young']), ('a', ['is', 'about', 'young', 'lion']), ('young', ['about', 'a', 'lion', 'prince']), ('lion', ['a', 'young', 'prince', 'who']), ('prince', ['young', 'lion', 'who', 'learns']), ('who', ['lion', 'prince', 'learns', 'about']), ('learns', ['prince', 'who', 'about', 'his']), ('about', ['who', 'learns', 'his', 'role']), ('his', ['learns', 'about', 'role', 'as']), ('role', ['about', 'his', 'as', 'prince']), ('as', ['his', 'role', 'prince', 'and']), ('prince', ['role', 'as', 'and', 'in']), ('and', ['as', 'prince', 'in', 'the']), ('in', ['prince', 'and', 'the', 'circle']), ('the', ['and', 'in', 'circle', 'of']), ('circle', ['in', 'the', 'of', 'life']), ('of', ['the', 'circle', 'life', 'it']), ('life', ['circle', 'of', 'it', 'is']), ('it', ['of', 'life', 'is', 'dedicated']), ('is', ['life', 'it', 'dedicated', 'to']), ('dedicated', ['it', 'is', 'to', 'frank']), ('to', ['is', 'dedicated', 'frank', 'wells']), ('frank', ['dedicated', 'to', 'wells', 'who']), ('wells', ['to', 'frank', 'who', 'was']), ('who', ['frank', 'wells', 'was', 'the']), ('was', ['wells', 'who', 'the', 'president']), ('the', ['who', 'was', 'president', 'of']), ('president', ['was', 'the', 'of', 'the']), ('of', ['the', 'president', 'the', 'walt']), ('the', ['president', 'of', 'walt', 'disney']), ('walt', ['of', 'the', 'disney', 'company']), ('disney', ['the', 'walt', 'company', 'and']), ('company', ['walt', 'disney', 'and', 'died']), ('and', ['disney', 'company', 'died', 'shortly']), ('died', ['company', 'and', 'shortly', 'before']), ('shortly', ['and', 'died', 'before', 'the']), ('before', ['died', 'shortly', 'the', 'movie']), ('the', ['shortly', 'before', 'movie', 'was']), ('movie', ['before', 'the', 'was', 'released']), ('was', ['the', 'movie', 'released', 'into']), ('released', ['movie', 'was', 'into', 'theaters']), ('into', ['was', 'released', 'theaters', 'on']), ('theaters', ['released', 'into', 'on', 'june']), ('on', ['into', 'theaters', 'june', '15']), ('june', ['theaters', 'on', '15', '1994']), ('15', ['on', 'june', '1994', 'it']), ('1994', ['june', '15', 'it', 'was']), ('it', ['15', '1994', 'was', 'the']), ('was', ['1994', 'it', 'the', 'first']), ('the', ['it', 'was', 'first', 'fulllength']), ('first', ['was', 'the', 'fulllength', 'disney']), ('fulllength', ['the', 'first', 'disney', 'movie']), ('disney', ['first', 'fulllength', 'movie', 'to']), ('movie', ['fulllength', 'disney', 'to', 'feature']), ('to', ['disney', 'movie', 'feature', 'no']), ('feature', ['movie', 'to', 'no', 'human']), ('no', ['to', 'feature', 'human', 'characters']), ('human', ['feature', 'no', 'characters', 'since']), ('characters', ['no', 'human', 'since', 'bambi']), ('since', ['human', 'characters', 'bambi', 'much']), ('bambi', ['characters', 'since', 'much', 'of']), ('much', ['since', 'bambi', 'of', 'the']), ('of', ['bambi', 'much', 'the', 'voice']), ('the', ['much', 'of', 'voice', 'acting']), ('voice', ['of', 'the', 'acting', 'work']), ('acting', ['the', 'voice', 'work', 'was']), ('work', ['voice', 'acting', 'was', 'done']), ('was', ['acting', 'work', 'done', 'by']), ('done', ['work', 'was', 'by', 'wellknown']), ('by', ['was', 'done', 'wellknown', 'actors']), ('wellknown', ['done', 'by', 'actors', 'including']), ('actors', ['by', 'wellknown', 'including', 'james']), ('including', ['wellknown', 'actors', 'james', 'earl']), ('james', ['actors', 'including', 'earl', 'jones']), ('earl', ['including', 'james', 'jones', 'jeremy']), ('jones', ['james', 'earl', 'jeremy', 'irons']), ('jeremy', ['earl', 'jones', 'irons', 'matthew']), ('irons', ['jones', 'jeremy', 'matthew', 'broderick']), ('matthew', ['jeremy', 'irons', 'broderick', 'whoopi']), ('broderick', ['irons', 'matthew', 'whoopi', 'goldberg']), ('whoopi', ['matthew', 'broderick', 'goldberg', 'rowan']), ('goldberg', ['broderick', 'whoopi', 'rowan', 'atkinson']), ('rowan', ['whoopi', 'goldberg', 'atkinson', 'jonathan']), ('atkinson', ['goldberg', 'rowan', 'jonathan', 'taylor']), ('jonathan', ['rowan', 'atkinson', 'taylor', 'thomas']), ('taylor', ['atkinson', 'jonathan', 'thomas', 'and']), ('thomas', ['jonathan', 'taylor', 'and', 'nathan']), ('and', ['taylor', 'thomas', 'nathan', 'lane']), ('nathan', ['thomas', 'and', 'lane', 'the']), ('lane', ['and', 'nathan', 'the', 'lion']), ('the', ['nathan', 'lane', 'lion', 'king']), ('lion', ['lane', 'the', 'king', 'is']), ('king', ['the', 'lion', 'is', 'a']), ('is', ['lion', 'king', 'a', 'musical']), ('a', ['king', 'is', 'musical', 'the']), ('musical', ['is', 'a', 'the', 'songs']), ('the', ['a', 'musical', 'songs', 'have']), ('songs', ['musical', 'the', 'have', 'music']), ('have', ['the', 'songs', 'music', 'by']), ('music', ['songs', 'have', 'by', 'elton']), ('by', ['have', 'music', 'elton', 'john']), ('elton', ['music', 'by', 'john', 'and']), ('john', ['by', 'elton', 'and', 'lyrics']), ('and', ['elton', 'john', 'lyrics', 'by']), ('lyrics', ['john', 'and', 'by', 'tim']), ('by', ['and', 'lyrics', 'tim', 'rice']), ('tim', ['lyrics', 'by', 'rice', 'computer']), ('rice', ['by', 'tim', 'computer', 'animation']), ('computer', ['tim', 'rice', 'animation', 'was']), ('animation', ['rice', 'computer', 'was', 'used']), ('was', ['computer', 'animation', 'used', 'a']), ('used', ['animation', 'was', 'a', 'lot']), ('a', ['was', 'used', 'lot', 'when']), ('lot', ['used', 'a', 'when', 'making']), ('when', ['a', 'lot', 'making', 'the']), ('making', ['lot', 'when', 'the', 'movie']), ('the', ['when', 'making', 'movie', 'like']), ('movie', ['making', 'the', 'like', 'during']), ('like', ['the', 'movie', 'during', 'the']), ('during', ['movie', 'like', 'the', 'song']), ('the', ['like', 'during', 'song', 'circle']), ('song', ['during', 'the', 'circle', 'of']), ('circle', ['the', 'song', 'of', 'life']), ('of', ['song', 'circle', 'life', 'and']), ('life', ['circle', 'of', 'and', 'others']), ('and', ['of', 'life', 'others', 'when']), ('others', ['life', 'and', 'when', 'they']), ('when', ['and', 'others', 'they', 'were']), ('they', ['others', 'when', 'were', 'making']), ('were', ['when', 'they', 'making', 'it']), ('making', ['they', 'were', 'it', 'this']), ('it', ['were', 'making', 'this', 'movie']), ('this', ['making', 'it', 'movie', 'was']), ('movie', ['it', 'this', 'was', 'thought']), ('was', ['this', 'movie', 'thought', 'of']), ('thought', ['movie', 'was', 'of', 'as']), ('of', ['was', 'thought', 'as', 'just']), ('as', ['thought', 'of', 'just', 'alright']), ('just', ['of', 'as', 'alright', 'compared']), ('alright', ['as', 'just', 'compared', 'to']), ('compared', ['just', 'alright', 'to', 'the']), ('to', ['alright', 'compared', 'the', 'movie']), ('the', ['compared', 'to', 'movie', 'they']), ('movie', ['to', 'the', 'they', 'were']), ('they', ['the', 'movie', 'were', 'going']), ('were', ['movie', 'they', 'going', 'to']), ('going', ['they', 'were', 'to', 'make']), ('to', ['were', 'going', 'make', 'after']), ('make', ['going', 'to', 'after', 'that']), ('after', ['to', 'make', 'that', 'which']), ('that', ['make', 'after', 'which', 'would']), ('which', ['after', 'that', 'would', 'be']), ('would', ['that', 'which', 'be', 'pocahontas']), ('be', ['which', 'would', 'pocahontas', 'the']), ('pocahontas', ['would', 'be', 'the', 'studio']), ('the', ['be', 'pocahontas', 'studio', 'released']), ('studio', ['pocahontas', 'the', 'released', 'the']), ('released', ['the', 'studio', 'the', 'trailer']), ('the', ['studio', 'released', 'trailer', 'and']), ('trailer', ['released', 'the', 'and', 'found']), ('and', ['the', 'trailer', 'found', 'that']), ('found', ['trailer', 'and', 'that', 'many']), ('that', ['and', 'found', 'many', 'people']), ('many', ['found', 'that', 'people', 'liked']), ('people', ['that', 'many', 'liked', 'it']), ('liked', ['many', 'people', 'it', 'especially']), ('it', ['people', 'liked', 'especially', 'the']), ('especially', ['liked', 'it', 'the', 'song']), ('the', ['it', 'especially', 'song', 'circle']), ('song', ['especially', 'the', 'circle', 'of']), ('circle', ['the', 'song', 'of', 'life']), ('of', ['song', 'circle', 'life', 'when']), ('life', ['circle', 'of', 'when', 'it']), ('when', ['of', 'life', 'it', 'was']), ('it', ['life', 'when', 'was', 'released']), ('was', ['when', 'it', 'released', 'the']), ('released', ['it', 'was', 'the', 'movie']), ('the', ['was', 'released', 'movie', 'became']), ('movie', ['released', 'the', 'became', 'the']), ('became', ['the', 'movie', 'the', 'most']), ('the', ['movie', 'became', 'most', 'successeful']), ('most', ['became', 'the', 'successeful', 'movie']), ('successeful', ['the', 'most', 'movie', 'worldwide']), ('movie', ['most', 'successeful', 'worldwide', 'in']), ('worldwide', ['successeful', 'movie', 'in', 'the']), ('in', ['movie', 'worldwide', 'the', 'united']), ('the', ['worldwide', 'in', 'united', 'states']), ('united', ['in', 'the', 'states', 'forrest']), ('states', ['the', 'united', 'forrest', 'gump']), ('forrest', ['united', 'states', 'gump', 'was']), ('gump', ['states', 'forrest', 'was', 'most']), ('was', ['forrest', 'gump', 'most', 'successful']), ('most', ['gump', 'was', 'successful', 'of']), ('successful', ['was', 'most', 'of', 'that']), ('of', ['most', 'successful', 'that', 'year']), ('that', ['successful', 'of', 'year', 'and']), ('year', ['of', 'that', 'and', 'the']), ('and', ['that', 'year', 'the', 'most']), ('the', ['year', 'and', 'most', 'successful']), ('most', ['and', 'the', 'successful', 'animated']), ('successful', ['the', 'most', 'animated', 'feature']), ('animated', ['most', 'successful', 'feature', 'movie']), ('feature', ['successful', 'animated', 'movie', 'of']), ('movie', ['animated', 'feature', 'of', 'all']), ('of', ['feature', 'movie', 'all', 'time']), ('all', ['movie', 'of', 'time', 'until']), ('time', ['of', 'all', 'until', 'finding']), ('until', ['all', 'time', 'finding', 'nemo']), ('finding', ['time', 'until', 'nemo', 'since']), ('nemo', ['until', 'finding', 'since', 'then']), ('since', ['finding', 'nemo', 'then', 'shrek']), ('then', ['nemo', 'since', 'shrek', '2']), ('shrek', ['since', 'then', '2', 'has']), ('2', ['then', 'shrek', 'has', 'become']), ('has', ['shrek', '2', 'become', 'more']), ('become', ['2', 'has', 'more', 'successful']), ('more', ['has', 'become', 'successful', 'than']), ('successful', ['become', 'more', 'than', 'finding']), ('than', ['more', 'successful', 'finding', 'nemo']), ('finding', ['successful', 'than', 'nemo', 'making']), ('nemo', ['than', 'finding', 'making', 'the']), ('making', ['finding', 'nemo', 'the', 'lion']), ('the', ['nemo', 'making', 'lion', 'king']), ('lion', ['making', 'the', 'king', 'the']), ('king', ['the', 'lion', 'the', 'third']), ('the', ['lion', 'king', 'third', 'most']), ('third', ['king', 'the', 'most', 'successful']), ('most', ['the', 'third', 'successful', 'the']), ('successful', ['third', 'most', 'the', 'movie']), ('the', ['most', 'successful', 'movie', 'was']), ('movie', ['successful', 'the', 'was', 'also']), ('was', ['the', 'movie', 'also', 'made']), ('also', ['movie', 'was', 'made', 'into']), ('made', ['was', 'also', 'into', 'an']), ('into', ['also', 'made', 'an', 'awardwinning']), ('an', ['made', 'into', 'awardwinning', 'stage']), ('awardwinning', ['into', 'an', 'stage', 'musical']), ('stage', ['an', 'awardwinning', 'musical', 'the']), ('musical', ['awardwinning', 'stage', 'the', 'stage']), ('the', ['stage', 'musical', 'stage', 'show']), ('stage', ['musical', 'the', 'show', 'first']), ('show', ['the', 'stage', 'first', 'opened']), ('first', ['stage', 'show', 'opened', 'on']), ('opened', ['show', 'first', 'on', 'november']), ('on', ['first', 'opened', 'november', '13']), ('november', ['opened', 'on', '13', '1997']), ('13', ['on', 'november', '1997', 'in']), ('1997', ['november', '13', 'in', 'new']), ('in', ['13', '1997', 'new', 'york']), ('new', ['1997', 'in', 'york', 'city']), ('york', ['in', 'new', 'city', 'and']), ('city', ['new', 'york', 'and', 'it']), ('and', ['york', 'city', 'it', 'was']), ('it', ['city', 'and', 'was', 'a']), ('was', ['and', 'it', 'a', 'big']), ('a', ['it', 'was', 'big', 'success']), ('big', ['was', 'a', 'success', 'a']), ('success', ['a', 'big', 'a', 'version']), ('a', ['big', 'success', 'version', 'opened']), ('version', ['success', 'a', 'opened', 'later']), ('opened', ['a', 'version', 'later', 'in']), ('later', ['version', 'opened', 'in', 'london']), ('in', ['opened', 'later', 'london', 'england']), ('london', ['later', 'in', 'england', 'many']), ('england', ['in', 'london', 'many', 'other']), ('many', ['london', 'england', 'other', 'shows']), ('other', ['england', 'many', 'shows', 'of']), ('shows', ['many', 'other', 'of', 'the']), ('of', ['other', 'shows', 'the', 'lion']), ('the', ['shows', 'of', 'lion', 'king']), ('lion', ['of', 'the', 'king', 'have']), ('king', ['the', 'lion', 'have', 'been']), ('have', ['lion', 'king', 'been', 'shown']), ('been', ['king', 'have', 'shown', 'across']), ('shown', ['have', 'been', 'across', 'the']), ('across', ['been', 'shown', 'the', 'world']), ('the', ['shown', 'across', 'world', 'and']), ('world', ['across', 'the', 'and', 'is']), ('and', ['the', 'world', 'is', 'one']), ('is', ['world', 'and', 'one', 'of']), ('one', ['and', 'is', 'of', 'the']), ('of', ['is', 'one', 'the', 'uks']), ('the', ['one', 'of', 'uks', 'biggest']), ('uks', ['of', 'the', 'biggest', 'and']), ('biggest', ['the', 'uks', 'and', 'most']), ('and', ['uks', 'biggest', 'most', 'popular']), ('most', ['biggest', 'and', 'popular', 'shows']), ('popular', ['and', 'most', 'shows', 'in']), ('shows', ['most', 'popular', 'in', 'the']), ('in', ['popular', 'shows', 'the', 'movies']), ('the', ['shows', 'in', 'movies', 'opening']), ('movies', ['in', 'the', 'opening', 'scene']), ('opening', ['the', 'movies', 'scene', 'lots']), ('scene', ['movies', 'opening', 'lots', 'of']), ('lots', ['opening', 'scene', 'of', 'animals']), ('of', ['scene', 'lots', 'animals', 'and']), ('animals', ['lots', 'of', 'and', 'birds']), ('and', ['of', 'animals', 'birds', 'gather']), ('birds', ['animals', 'and', 'gather', 'at']), ('gather', ['and', 'birds', 'at', 'pride']), ('at', ['birds', 'gather', 'pride', 'rock']), ('pride', ['gather', 'at', 'rock', 'to']), ('rock', ['at', 'pride', 'to', 'see']), ('to', ['pride', 'rock', 'see', 'simba']), ('see', ['rock', 'to', 'simba', 'the']), ('simba', ['to', 'see', 'the', 'new']), ('the', ['see', 'simba', 'new', 'prince']), ('new', ['simba', 'the', 'prince', 'who']), ('prince', ['the', 'new', 'who', 'has']), ('who', ['new', 'prince', 'has', 'just']), ('has', ['prince', 'who', 'just', 'been']), ('just', ['who', 'has', 'been', 'born']), ('been', ['has', 'just', 'born', 'simba']), ('born', ['just', 'been', 'simba', 'is']), ('simba', ['been', 'born', 'is', 'the']), ('is', ['born', 'simba', 'the', 'son']), ('the', ['simba', 'is', 'son', 'of']), ('son', ['is', 'the', 'of', 'mufasa']), ('of', ['the', 'son', 'mufasa', 'and']), ('mufasa', ['son', 'of', 'and', 'sarabi']), ('and', ['of', 'mufasa', 'sarabi', 'rafiki']), ('sarabi', ['mufasa', 'and', 'rafiki', 'picks']), ('rafiki', ['and', 'sarabi', 'picks', 'up']), ('picks', ['sarabi', 'rafiki', 'up', 'simba']), ('up', ['rafiki', 'picks', 'simba', 'and']), ('simba', ['picks', 'up', 'and', 'lifts']), ('and', ['up', 'simba', 'lifts', 'him']), ('lifts', ['simba', 'and', 'him', 'high']), ('him', ['and', 'lifts', 'high', 'up']), ('high', ['lifts', 'him', 'up', 'so']), ('up', ['him', 'high', 'so', 'that']), ('so', ['high', 'up', 'that', 'all']), ('that', ['up', 'so', 'all', 'of']), ('all', ['so', 'that', 'of', 'the']), ('of', ['that', 'all', 'the', 'animals']), ('the', ['all', 'of', 'animals', 'can']), ('animals', ['of', 'the', 'can', 'see']), ('can', ['the', 'animals', 'see', 'the']), ('see', ['animals', 'can', 'the', 'animals']), ('the', ['can', 'see', 'animals', 'celebrate']), ('animals', ['see', 'the', 'celebrate', 'and']), ('celebrate', ['the', 'animals', 'and', 'rejoice']), ('and', ['animals', 'celebrate', 'rejoice', 'but']), ('rejoice', ['celebrate', 'and', 'but', 'scar']), ('but', ['and', 'rejoice', 'scar', 'mufasas']), ('scar', ['rejoice', 'but', 'mufasas', 'brother']), ('mufasas', ['but', 'scar', 'brother', 'is']), ('brother', ['scar', 'mufasas', 'is', 'jealous']), ('is', ['mufasas', 'brother', 'jealous', 'because']), ('jealous', ['brother', 'is', 'because', 'simba']), ('because', ['is', 'jealous', 'simba', 'will']), ('simba', ['jealous', 'because', 'will', 'be']), ('will', ['because', 'simba', 'be', 'king']), ('be', ['simba', 'will', 'king', 'instead']), ('king', ['will', 'be', 'instead', 'of']), ('instead', ['be', 'king', 'of', 'him']), ('of', ['king', 'instead', 'him', 'scar']), ('him', ['instead', 'of', 'scar', 'lies']), ('scar', ['of', 'him', 'lies', 'to']), ('lies', ['him', 'scar', 'to', 'simba']), ('to', ['scar', 'lies', 'simba', 'about']), ('simba', ['lies', 'to', 'about', 'a']), ('about', ['to', 'simba', 'a', 'dangerous']), ('a', ['simba', 'about', 'dangerous', 'place']), ('dangerous', ['about', 'a', 'place', 'called']), ('place', ['a', 'dangerous', 'called', 'the']), ('called', ['dangerous', 'place', 'the', 'elephant']), ('the', ['place', 'called', 'elephant', 'graveyard']), ('elephant', ['called', 'the', 'graveyard', 'scar']), ('graveyard', ['the', 'elephant', 'scar', 'says']), ('scar', ['elephant', 'graveyard', 'says', 'that']), ('says', ['graveyard', 'scar', 'that', 'only']), ('that', ['scar', 'says', 'only', 'brave']), ('only', ['says', 'that', 'brave', 'lions']), ('brave', ['that', 'only', 'lions', 'go']), ('lions', ['only', 'brave', 'go', 'there']), ('go', ['brave', 'lions', 'there', 'causing']), ('there', ['lions', 'go', 'causing', 'simba']), ('causing', ['go', 'there', 'simba', 'to']), ('simba', ['there', 'causing', 'to', 'be']), ('to', ['causing', 'simba', 'be', 'interested']), ('be', ['simba', 'to', 'interested', 'even']), ('interested', ['to', 'be', 'even', 'though']), ('even', ['be', 'interested', 'though', 'mufasa']), ('though', ['interested', 'even', 'mufasa', 'has']), ('mufasa', ['even', 'though', 'has', 'forbidden']), ('has', ['though', 'mufasa', 'forbidden', 'simba']), ('forbidden', ['mufasa', 'has', 'simba', 'from']), ('simba', ['has', 'forbidden', 'from', 'going']), ('from', ['forbidden', 'simba', 'going', 'there']), ('going', ['simba', 'from', 'there', 'simba']), ('there', ['from', 'going', 'simba', 'lies']), ('simba', ['going', 'there', 'lies', 'to']), ('lies', ['there', 'simba', 'to', 'his']), ('to', ['simba', 'lies', 'his', 'mother']), ('his', ['lies', 'to', 'mother', 'sarabi']), ('mother', ['to', 'his', 'sarabi', 'about']), ('sarabi', ['his', 'mother', 'about', 'going']), ('about', ['mother', 'sarabi', 'going', 'to']), ('going', ['sarabi', 'about', 'to', 'the']), ('to', ['about', 'going', 'the', 'water']), ('the', ['going', 'to', 'water', 'hole']), ('water', ['to', 'the', 'hole', 'when']), ('hole', ['the', 'water', 'when', 'he']), ('when', ['water', 'hole', 'he', 'is']), ('he', ['hole', 'when', 'is', 'actually']), ('is', ['when', 'he', 'actually', 'going']), ('actually', ['he', 'is', 'going', 'to']), ('going', ['is', 'actually', 'to', 'the']), ('to', ['actually', 'going', 'the', 'elephant']), ('the', ['going', 'to', 'elephant', 'graveyard']), ('elephant', ['to', 'the', 'graveyard', 'simbas']), ('graveyard', ['the', 'elephant', 'simbas', 'friend']), ('simbas', ['elephant', 'graveyard', 'friend', 'nala']), ('friend', ['graveyard', 'simbas', 'nala', 'and']), ('nala', ['simbas', 'friend', 'and', 'zazu']), ('and', ['friend', 'nala', 'zazu', 'the']), ('zazu', ['nala', 'and', 'the', 'kings']), ('the', ['and', 'zazu', 'kings', 'messenger']), ('kings', ['zazu', 'the', 'messenger', 'go']), ('messenger', ['the', 'kings', 'go', 'with']), ('go', ['kings', 'messenger', 'with', 'simba']), ('with', ['messenger', 'go', 'simba', 'simba']), ('simba', ['go', 'with', 'simba', 'and']), ('simba', ['with', 'simba', 'and', 'nala']), ('and', ['simba', 'simba', 'nala', 'trick']), ('nala', ['simba', 'and', 'trick', 'zazu']), ('trick', ['and', 'nala', 'zazu', 'with']), ('zazu', ['nala', 'trick', 'with', 'the']), ('with', ['trick', 'zazu', 'the', 'song']), ('the', ['zazu', 'with', 'song', 'i']), ('song', ['with', 'the', 'i', 'just']), ('i', ['the', 'song', 'just', 'cant']), ('just', ['song', 'i', 'cant', 'wait']), ('cant', ['i', 'just', 'wait', 'to']), ('wait', ['just', 'cant', 'to', 'be']), ('to', ['cant', 'wait', 'be', 'king']), ('be', ['wait', 'to', 'king', 'and']), ('king', ['to', 'be', 'and', 'run']), ('and', ['be', 'king', 'run', 'away']), ('run', ['king', 'and', 'away', 'from']), ('away', ['and', 'run', 'from', 'him']), ('from', ['run', 'away', 'him', 'simba']), ('him', ['away', 'from', 'simba', 'and']), ('simba', ['from', 'him', 'and', 'nala']), ('and', ['him', 'simba', 'nala', 'find']), ('nala', ['simba', 'and', 'find', 'the']), ('find', ['and', 'nala', 'the', 'elephant']), ('the', ['nala', 'find', 'elephant', 'graveyard']), ('elephant', ['find', 'the', 'graveyard', 'but']), ('graveyard', ['the', 'elephant', 'but', 'are']), ('but', ['elephant', 'graveyard', 'are', 'chased']), ('are', ['graveyard', 'but', 'chased', 'by']), ('chased', ['but', 'are', 'by', 'the']), ('by', ['are', 'chased', 'the', 'three']), ('the', ['chased', 'by', 'three', 'hyenas']), ('three', ['by', 'the', 'hyenas', 'shenzi']), ('hyenas', ['the', 'three', 'shenzi', 'banzai']), ('shenzi', ['three', 'hyenas', 'banzai', 'and']), ('banzai', ['hyenas', 'shenzi', 'and', 'ed']), ('and', ['shenzi', 'banzai', 'ed', 'mufasa']), ('ed', ['banzai', 'and', 'mufasa', 'saves']), ('mufasa', ['and', 'ed', 'saves', 'his']), ('saves', ['ed', 'mufasa', 'his', 'son']), ('his', ['mufasa', 'saves', 'son', 'and']), ('son', ['saves', 'his', 'and', 'nala']), ('and', ['his', 'son', 'nala', 'and']), ('nala', ['son', 'and', 'and', 'takes']), ('and', ['and', 'nala', 'takes', 'them']), ('takes', ['nala', 'and', 'them', 'both']), ('them', ['and', 'takes', 'both', 'home']), ('both', ['takes', 'them', 'home', 'mufasa']), ('home', ['them', 'both', 'mufasa', 'speaks']), ('mufasa', ['both', 'home', 'speaks', 'to']), ('speaks', ['home', 'mufasa', 'to', 'simba']), ('to', ['mufasa', 'speaks', 'simba', 'alone']), ('simba', ['speaks', 'to', 'alone', 'and']), ('alone', ['to', 'simba', 'and', 'explains']), ('and', ['simba', 'alone', 'explains', 'to']), ('explains', ['alone', 'and', 'to', 'simba']), ('to', ['and', 'explains', 'simba', 'that']), ('simba', ['explains', 'to', 'that', 'being']), ('that', ['to', 'simba', 'being', 'brave']), ('being', ['simba', 'that', 'brave', 'is']), ('brave', ['that', 'being', 'is', 'not']), ('is', ['being', 'brave', 'not', 'about']), ('not', ['brave', 'is', 'about', 'looking']), ('about', ['is', 'not', 'looking', 'for']), ('looking', ['not', 'about', 'for', 'danger']), ('for', ['about', 'looking', 'danger', 'he']), ('danger', ['looking', 'for', 'he', 'also']), ('he', ['for', 'danger', 'also', 'explains']), ('also', ['danger', 'he', 'explains', 'that']), ('explains', ['he', 'also', 'that', 'the']), ('that', ['also', 'explains', 'the', 'great']), ('the', ['explains', 'that', 'great', 'kings']), ('great', ['that', 'the', 'kings', 'of']), ('kings', ['the', 'great', 'of', 'the']), ('of', ['great', 'kings', 'the', 'past']), ('the', ['kings', 'of', 'past', 'look']), ('past', ['of', 'the', 'look', 'down']), ('look', ['the', 'past', 'down', 'from']), ('down', ['past', 'look', 'from', 'the']), ('from', ['look', 'down', 'the', 'stars']), ('the', ['down', 'from', 'stars', 'and']), ('stars', ['from', 'the', 'and', 'watch']), ('and', ['the', 'stars', 'watch', 'over']), ('watch', ['stars', 'and', 'over', 'simba']), ('over', ['and', 'watch', 'simba', 'scar']), ('simba', ['watch', 'over', 'scar', 'in']), ('scar', ['over', 'simba', 'in', 'the']), ('in', ['simba', 'scar', 'the', 'elephant']), ('the', ['scar', 'in', 'elephant', 'graveyard']), ('elephant', ['in', 'the', 'graveyard', 'is']), ('graveyard', ['the', 'elephant', 'is', 'angry']), ('is', ['elephant', 'graveyard', 'angry', 'with']), ('angry', ['graveyard', 'is', 'with', 'the']), ('with', ['is', 'angry', 'the', 'hyenas']), ('the', ['angry', 'with', 'hyenas', 'because']), ('hyenas', ['with', 'the', 'because', 'they']), ('because', ['the', 'hyenas', 'they', 'did']), ('they', ['hyenas', 'because', 'did', 'not']), ('did', ['because', 'they', 'not', 'kill']), ('not', ['they', 'did', 'kill', 'simba']), ('kill', ['did', 'not', 'simba', 'it']), ('simba', ['not', 'kill', 'it', 'is']), ('it', ['kill', 'simba', 'is', 'revealed']), ('is', ['simba', 'it', 'revealed', 'that']), ('revealed', ['it', 'is', 'that', 'the']), ('that', ['is', 'revealed', 'the', 'hyenas']), ('the', ['revealed', 'that', 'hyenas', 'are']), ('hyenas', ['that', 'the', 'are', 'working']), ('are', ['the', 'hyenas', 'working', 'for']), ('working', ['hyenas', 'are', 'for', 'scar']), ('for', ['are', 'working', 'scar', 'during']), ('scar', ['working', 'for', 'during', 'scars']), ('during', ['for', 'scar', 'scars', 'song']), ('scars', ['scar', 'during', 'song', 'be']), ('song', ['during', 'scars', 'be', 'prepared']), ('be', ['scars', 'song', 'prepared', 'the']), ('prepared', ['song', 'be', 'the', 'next']), ('the', ['be', 'prepared', 'next', 'day']), ('next', ['prepared', 'the', 'day', 'scar']), ('day', ['the', 'next', 'scar', 'takes']), ('scar', ['next', 'day', 'takes', 'simba']), ('takes', ['day', 'scar', 'simba', 'into']), ('simba', ['scar', 'takes', 'into', 'a']), ('into', ['takes', 'simba', 'a', 'gorge']), ('a', ['simba', 'into', 'gorge', 'long']), ('gorge', ['into', 'a', 'long', 'deep']), ('long', ['a', 'gorge', 'deep', 'hole']), ('deep', ['gorge', 'long', 'hole', 'in']), ('hole', ['long', 'deep', 'in', 'the']), ('in', ['deep', 'hole', 'the', 'ground']), ('the', ['hole', 'in', 'ground', 'also']), ('ground', ['in', 'the', 'also', 'known']), ('also', ['the', 'ground', 'known', 'as']), ('known', ['ground', 'also', 'as', 'a']), ('as', ['also', 'known', 'a', 'valley']), ('a', ['known', 'as', 'valley', 'where']), ('valley', ['as', 'a', 'where', 'he']), ('where', ['a', 'valley', 'he', 'explains']), ('he', ['valley', 'where', 'explains', 'that']), ('explains', ['where', 'he', 'that', 'mufasa']), ('that', ['he', 'explains', 'mufasa', 'has']), ('mufasa', ['explains', 'that', 'has', 'a']), ('has', ['that', 'mufasa', 'a', 'wonderful']), ('a', ['mufasa', 'has', 'wonderful', 'surprise']), ('wonderful', ['has', 'a', 'surprise', 'waiting']), ('surprise', ['a', 'wonderful', 'waiting', 'scar']), ('waiting', ['wonderful', 'surprise', 'scar', 'has']), ('scar', ['surprise', 'waiting', 'has', 'actually']), ('has', ['waiting', 'scar', 'actually', 'planned']), ('actually', ['scar', 'has', 'planned', 'a']), ('planned', ['has', 'actually', 'a', 'wildebeest']), ('a', ['actually', 'planned', 'wildebeest', 'stampede']), ('wildebeest', ['planned', 'a', 'stampede', 'with']), ('stampede', ['a', 'wildebeest', 'with', 'the']), ('with', ['wildebeest', 'stampede', 'the', 'hyenas']), ('the', ['stampede', 'with', 'hyenas', 'simba']), ('hyenas', ['with', 'the', 'simba', 'is']), ('simba', ['the', 'hyenas', 'is', 'trapped']), ('is', ['hyenas', 'simba', 'trapped', 'in']), ('trapped', ['simba', 'is', 'in', 'the']), ('in', ['is', 'trapped', 'the', 'gorge']), ('the', ['trapped', 'in', 'gorge', 'as']), ('gorge', ['in', 'the', 'as', 'the']), ('as', ['the', 'gorge', 'the', 'wildebeest']), ('the', ['gorge', 'as', 'wildebeest', 'run']), ('wildebeest', ['as', 'the', 'run', 'towards']), ('run', ['the', 'wildebeest', 'towards', 'him']), ('towards', ['wildebeest', 'run', 'him', 'scar']), ('him', ['run', 'towards', 'scar', 'tells']), ('scar', ['towards', 'him', 'tells', 'mufasa']), ('tells', ['him', 'scar', 'mufasa', 'that']), ('mufasa', ['scar', 'tells', 'that', 'simba']), ('that', ['tells', 'mufasa', 'simba', 'is']), ('simba', ['mufasa', 'that', 'is', 'in']), ('is', ['that', 'simba', 'in', 'trouble']), ('in', ['simba', 'is', 'trouble', 'and']), ('trouble', ['is', 'in', 'and', 'mufasa']), ('and', ['in', 'trouble', 'mufasa', 'rescues']), ('mufasa', ['trouble', 'and', 'rescues', 'his']), ('rescues', ['and', 'mufasa', 'his', 'son']), ('his', ['mufasa', 'rescues', 'son', 'scar']), ('son', ['rescues', 'his', 'scar', 'then']), ('scar', ['his', 'son', 'then', 'throws']), ('then', ['son', 'scar', 'throws', 'mufasa']), ('throws', ['scar', 'then', 'mufasa', 'into']), ('mufasa', ['then', 'throws', 'into', 'the']), ('into', ['throws', 'mufasa', 'the', 'stampede']), ('the', ['mufasa', 'into', 'stampede', 'and']), ('stampede', ['into', 'the', 'and', 'mufasa']), ('and', ['the', 'stampede', 'mufasa', 'dies']), ('mufasa', ['stampede', 'and', 'dies', 'scar']), ('dies', ['and', 'mufasa', 'scar', 'blames']), ('scar', ['mufasa', 'dies', 'blames', 'simba']), ('blames', ['dies', 'scar', 'simba', 'for']), ('simba', ['scar', 'blames', 'for', 'the']), ('for', ['blames', 'simba', 'the', 'death']), ('the', ['simba', 'for', 'death', 'of']), ('death', ['for', 'the', 'of', 'mufasa']), ('of', ['the', 'death', 'mufasa', 'and']), ('mufasa', ['death', 'of', 'and', 'simba']), ('and', ['of', 'mufasa', 'simba', 'runs']), ('simba', ['mufasa', 'and', 'runs', 'away']), ('runs', ['and', 'simba', 'away', 'scar']), ('away', ['simba', 'runs', 'scar', 'becomes']), ('scar', ['runs', 'away', 'becomes', 'king']), ('becomes', ['away', 'scar', 'king', 'and']), ('king', ['scar', 'becomes', 'and', 'tells']), ('and', ['becomes', 'king', 'tells', 'everyone']), ('tells', ['king', 'and', 'everyone', 'that']), ('everyone', ['and', 'tells', 'that', 'simba']), ('that', ['tells', 'everyone', 'simba', 'and']), ('simba', ['everyone', 'that', 'and', 'mufasa']), ('and', ['that', 'simba', 'mufasa', 'are']), ('mufasa', ['simba', 'and', 'are', 'dead']), ('are', ['and', 'mufasa', 'dead', 'simba']), ('dead', ['mufasa', 'are', 'simba', 'runs']), ('simba', ['are', 'dead', 'runs', 'to']), ('runs', ['dead', 'simba', 'to', 'a']), ('to', ['simba', 'runs', 'a', 'desert']), ('a', ['runs', 'to', 'desert', 'and']), ('desert', ['to', 'a', 'and', 'collapses']), ('and', ['a', 'desert', 'collapses', 'he']), ('collapses', ['desert', 'and', 'he', 'is']), ('he', ['and', 'collapses', 'is', 'rescued']), ('is', ['collapses', 'he', 'rescued', 'by']), ('rescued', ['he', 'is', 'by', 'timon']), ('by', ['is', 'rescued', 'timon', 'the']), ('timon', ['rescued', 'by', 'the', 'meerkat']), ('the', ['by', 'timon', 'meerkat', 'and']), ('meerkat', ['timon', 'the', 'and', 'pumbaa']), ('and', ['the', 'meerkat', 'pumbaa', 'the']), ('pumbaa', ['meerkat', 'and', 'the', 'warthog']), ('the', ['and', 'pumbaa', 'warthog', 'timon']), ('warthog', ['pumbaa', 'the', 'timon', 'and']), ('timon', ['the', 'warthog', 'and', 'pumbaa']), ('and', ['warthog', 'timon', 'pumbaa', 'live']), ('pumbaa', ['timon', 'and', 'live', 'in']), ('live', ['and', 'pumbaa', 'in', 'the']), ('in', ['pumbaa', 'live', 'the', 'jungle']), ('the', ['live', 'in', 'jungle', 'and']), ('jungle', ['in', 'the', 'and', 'are']), ('and', ['the', 'jungle', 'are', 'very']), ('are', ['jungle', 'and', 'very', 'relaxed']), ('very', ['and', 'are', 'relaxed', 'which']), ('relaxed', ['are', 'very', 'which', 'they']), ('which', ['very', 'relaxed', 'they', 'show']), ('they', ['relaxed', 'which', 'show', 'in']), ('show', ['which', 'they', 'in', 'their']), ('in', ['they', 'show', 'their', 'song']), ('their', ['show', 'in', 'song', 'hakuna']), ('song', ['in', 'their', 'hakuna', 'matata']), ('hakuna', ['their', 'song', 'matata', 'timon']), ('matata', ['song', 'hakuna', 'timon', 'and']), ('timon', ['hakuna', 'matata', 'and', 'pumbaa']), ('and', ['matata', 'timon', 'pumbaa', 'look']), ('pumbaa', ['timon', 'and', 'look', 'after']), ('look', ['and', 'pumbaa', 'after', 'simba']), ('after', ['pumbaa', 'look', 'simba', 'until']), ('simba', ['look', 'after', 'until', 'simba']), ('until', ['after', 'simba', 'simba', 'is']), ('simba', ['simba', 'until', 'is', 'an']), ('is', ['until', 'simba', 'an', 'adult']), ('an', ['simba', 'is', 'adult', 'lion']), ('adult', ['is', 'an', 'lion', 'one']), ('lion', ['an', 'adult', 'one', 'day']), ('one', ['adult', 'lion', 'day', 'a']), ('day', ['lion', 'one', 'a', 'lioness']), ('a', ['one', 'day', 'lioness', 'female']), ('lioness', ['day', 'a', 'female', 'lion']), ('female', ['a', 'lioness', 'lion', 'comes']), ('lion', ['lioness', 'female', 'comes', 'to']), ('comes', ['female', 'lion', 'to', 'the']), ('to', ['lion', 'comes', 'the', 'jungle']), ('the', ['comes', 'to', 'jungle', 'and']), ('jungle', ['to', 'the', 'and', 'tries']), ('and', ['the', 'jungle', 'tries', 'to']), ('tries', ['jungle', 'and', 'to', 'kill']), ('to', ['and', 'tries', 'kill', 'and']), ('kill', ['tries', 'to', 'and', 'eat']), ('and', ['to', 'kill', 'eat', 'pumbaa']), ('eat', ['kill', 'and', 'pumbaa', 'simba']), ('pumbaa', ['and', 'eat', 'simba', 'fights']), ('simba', ['eat', 'pumbaa', 'fights', 'the']), ('fights', ['pumbaa', 'simba', 'the', 'lioness']), ('the', ['simba', 'fights', 'lioness', 'because']), ('lioness', ['fights', 'the', 'because', 'he']), ('because', ['the', 'lioness', 'he', 'wants']), ('he', ['lioness', 'because', 'wants', 'to']), ('wants', ['because', 'he', 'to', 'save']), ('to', ['he', 'wants', 'save', 'pumbaas']), ('save', ['wants', 'to', 'pumbaas', 'life']), ('pumbaas', ['to', 'save', 'life', 'while']), ('life', ['save', 'pumbaas', 'while', 'the']), ('while', ['pumbaas', 'life', 'the', 'two']), ('the', ['life', 'while', 'two', 'lions']), ('two', ['while', 'the', 'lions', 'are']), ('lions', ['the', 'two', 'are', 'fighting']), ('are', ['two', 'lions', 'fighting', 'simba']), ('fighting', ['lions', 'are', 'simba', 'finds']), ('simba', ['are', 'fighting', 'finds', 'out']), ('finds', ['fighting', 'simba', 'out', 'that']), ('out', ['simba', 'finds', 'that', 'the']), ('that', ['finds', 'out', 'the', 'lioness']), ('the', ['out', 'that', 'lioness', 'is']), ('lioness', ['that', 'the', 'is', 'his']), ('is', ['the', 'lioness', 'his', 'friend']), ('his', ['lioness', 'is', 'friend', 'nala']), ('friend', ['is', 'his', 'nala', 'they']), ('nala', ['his', 'friend', 'they', 'are']), ('they', ['friend', 'nala', 'are', 'very']), ('are', ['nala', 'they', 'very', 'happy']), ('very', ['they', 'are', 'happy', 'to']), ('happy', ['are', 'very', 'to', 'see']), ('to', ['very', 'happy', 'see', 'each']), ('see', ['happy', 'to', 'each', 'other']), ('each', ['to', 'see', 'other', 'and']), ('other', ['see', 'each', 'and', 'they']), ('and', ['each', 'other', 'they', 'fall']), ('they', ['other', 'and', 'fall', 'in']), ('fall', ['and', 'they', 'in', 'love']), ('in', ['they', 'fall', 'love', 'nala']), ('love', ['fall', 'in', 'nala', 'wants']), ('nala', ['in', 'love', 'wants', 'simba']), ('wants', ['love', 'nala', 'simba', 'to']), ('simba', ['nala', 'wants', 'to', 'go']), ('to', ['wants', 'simba', 'go', 'home']), ('go', ['simba', 'to', 'home', 'and']), ('home', ['to', 'go', 'and', 'fight']), ('and', ['go', 'home', 'fight', 'scar']), ('fight', ['home', 'and', 'scar', 'because']), ('scar', ['and', 'fight', 'because', 'scar']), ('because', ['fight', 'scar', 'scar', 'is']), ('scar', ['scar', 'because', 'is', 'a']), ('is', ['because', 'scar', 'a', 'bad']), ('a', ['scar', 'is', 'bad', 'king']), ('bad', ['is', 'a', 'king', 'simba']), ('king', ['a', 'bad', 'simba', 'will']), ('simba', ['bad', 'king', 'will', 'not']), ('will', ['king', 'simba', 'not', 'go']), ('not', ['simba', 'will', 'go', 'home']), ('go', ['will', 'not', 'home', 'because']), ('home', ['not', 'go', 'because', 'he']), ('because', ['go', 'home', 'he', 'thinks']), ('he', ['home', 'because', 'thinks', 'that']), ('thinks', ['because', 'he', 'that', 'he']), ('that', ['he', 'thinks', 'he', 'killed']), ('he', ['thinks', 'that', 'killed', 'mufasa']), ('killed', ['that', 'he', 'mufasa', 'and']), ('mufasa', ['he', 'killed', 'and', 'he']), ('and', ['killed', 'mufasa', 'he', 'does']), ('he', ['mufasa', 'and', 'does', 'not']), ('does', ['and', 'he', 'not', 'want']), ('not', ['he', 'does', 'want', 'his']), ('want', ['does', 'not', 'his', 'family']), ('his', ['not', 'want', 'family', 'to']), ('family', ['want', 'his', 'to', 'know']), ('to', ['his', 'family', 'know', 'rafiki']), ('know', ['family', 'to', 'rafiki', 'comes']), ('rafiki', ['to', 'know', 'comes', 'to']), ('comes', ['know', 'rafiki', 'to', 'the']), ('to', ['rafiki', 'comes', 'the', 'jungle']), ('the', ['comes', 'to', 'jungle', 'and']), ('jungle', ['to', 'the', 'and', 'takes']), ('and', ['the', 'jungle', 'takes', 'simba']), ('takes', ['jungle', 'and', 'simba', 'to']), ('simba', ['and', 'takes', 'to', 'a']), ('to', ['takes', 'simba', 'a', 'field']), ('a', ['simba', 'to', 'field', 'in']), ('field', ['to', 'a', 'in', 'the']), ('in', ['a', 'field', 'the', 'sky']), ('the', ['field', 'in', 'sky', 'above']), ('sky', ['in', 'the', 'above', 'the']), ('above', ['the', 'sky', 'the', 'field']), ('the', ['sky', 'above', 'field', 'mufasas']), ('field', ['above', 'the', 'mufasas', 'ghost']), ('mufasas', ['the', 'field', 'ghost', 'appears']), ('ghost', ['field', 'mufasas', 'appears', 'and']), ('appears', ['mufasas', 'ghost', 'and', 'tells']), ('and', ['ghost', 'appears', 'tells', 'simba']), ('tells', ['appears', 'and', 'simba', 'that']), ('simba', ['and', 'tells', 'that', 'he']), ('that', ['tells', 'simba', 'he', 'must']), ('he', ['simba', 'that', 'must', 'go']), ('must', ['that', 'he', 'go', 'home']), ('go', ['he', 'must', 'home', 'because']), ('home', ['must', 'go', 'because', 'simba']), ('because', ['go', 'home', 'simba', 'is']), ('simba', ['home', 'because', 'is', 'the']), ('is', ['because', 'simba', 'the', 'right']), ('the', ['simba', 'is', 'right', 'king']), ('right', ['is', 'the', 'king', 'after']), ('king', ['the', 'right', 'after', 'this']), ('after', ['right', 'king', 'this', 'simba']), ('this', ['king', 'after', 'simba', 'goes']), ('simba', ['after', 'this', 'goes', 'home']), ('goes', ['this', 'simba', 'home', 'to']), ('home', ['simba', 'goes', 'to', 'pride']), ('to', ['goes', 'home', 'pride', 'rock']), ('pride', ['home', 'to', 'rock', 'nala']), ('rock', ['to', 'pride', 'nala', 'timon']), ('nala', ['pride', 'rock', 'timon', 'and']), ('timon', ['rock', 'nala', 'and', 'pumbaa']), ('and', ['nala', 'timon', 'pumbaa', 'follow']), ('pumbaa', ['timon', 'and', 'follow', 'him']), ('follow', ['and', 'pumbaa', 'him', 'when']), ('him', ['pumbaa', 'follow', 'when', 'they']), ('when', ['follow', 'him', 'they', 'get']), ('they', ['him', 'when', 'get', 'to']), ('get', ['when', 'they', 'to', 'pride']), ('to', ['they', 'get', 'pride', 'rock']), ('pride', ['get', 'to', 'rock', 'they']), ('rock', ['to', 'pride', 'they', 'find']), ('they', ['pride', 'rock', 'find', 'that']), ('find', ['rock', 'they', 'that', 'the']), ('that', ['they', 'find', 'the', 'land']), ('the', ['find', 'that', 'land', 'is']), ('land', ['that', 'the', 'is', 'dry']), ('is', ['the', 'land', 'dry', 'and']), ('dry', ['land', 'is', 'and', 'the']), ('and', ['is', 'dry', 'the', 'animals']), ('the', ['dry', 'and', 'animals', 'have']), ('animals', ['and', 'the', 'have', 'gone']), ('have', ['the', 'animals', 'gone', 'at']), ('gone', ['animals', 'have', 'at', 'pride']), ('at', ['have', 'gone', 'pride', 'rock']), ('pride', ['gone', 'at', 'rock', 'simba']), ('rock', ['at', 'pride', 'simba', 'sees']), ('simba', ['pride', 'rock', 'sees', 'scar']), ('sees', ['rock', 'simba', 'scar', 'hitting']), ('scar', ['simba', 'sees', 'hitting', 'sarabi']), ('hitting', ['sees', 'scar', 'sarabi', 'this']), ('sarabi', ['scar', 'hitting', 'this', 'makes']), ('this', ['hitting', 'sarabi', 'makes', 'simba']), ('makes', ['sarabi', 'this', 'simba', 'angry']), ('simba', ['this', 'makes', 'angry', 'and']), ('angry', ['makes', 'simba', 'and', 'he']), ('and', ['simba', 'angry', 'he', 'tries']), ('he', ['angry', 'and', 'tries', 'to']), ('tries', ['and', 'he', 'to', 'make']), ('to', ['he', 'tries', 'make', 'scar']), ('make', ['tries', 'to', 'scar', 'leave']), ('scar', ['to', 'make', 'leave', 'pride']), ('leave', ['make', 'scar', 'pride', 'rock']), ('pride', ['scar', 'leave', 'rock', 'scar']), ('rock', ['leave', 'pride', 'scar', 'does']), ('scar', ['pride', 'rock', 'does', 'not']), ('does', ['rock', 'scar', 'not', 'leave']), ('not', ['scar', 'does', 'leave', 'and']), ('leave', ['does', 'not', 'and', 'makes']), ('and', ['not', 'leave', 'makes', 'simba']), ('makes', ['leave', 'and', 'simba', 'fall']), ('simba', ['and', 'makes', 'fall', 'over']), ('fall', ['makes', 'simba', 'over', 'the']), ('over', ['simba', 'fall', 'the', 'edge']), ('the', ['fall', 'over', 'edge', 'of']), ('edge', ['over', 'the', 'of', 'pride']), ('of', ['the', 'edge', 'pride', 'rock']), ('pride', ['edge', 'of', 'rock', 'simba']), ('rock', ['of', 'pride', 'simba', 'does']), ('simba', ['pride', 'rock', 'does', 'not']), ('does', ['rock', 'simba', 'not', 'fall']), ('not', ['simba', 'does', 'fall', 'and']), ('fall', ['does', 'not', 'and', 'holds']), ('and', ['not', 'fall', 'holds', 'on']), ('holds', ['fall', 'and', 'on', 'to']), ('on', ['and', 'holds', 'to', 'the']), ('to', ['holds', 'on', 'the', 'edge']), ('the', ['on', 'to', 'edge', 'scar']), ('edge', ['to', 'the', 'scar', 'thinks']), ('scar', ['the', 'edge', 'thinks', 'that']), ('thinks', ['edge', 'scar', 'that', 'he']), ('that', ['scar', 'thinks', 'he', 'was']), ('he', ['thinks', 'that', 'was', 'won']), ('was', ['that', 'he', 'won', 'so']), ('won', ['he', 'was', 'so', 'he']), ('so', ['was', 'won', 'he', 'tells']), ('he', ['won', 'so', 'tells', 'simba']), ('tells', ['so', 'he', 'simba', 'the']), ('simba', ['he', 'tells', 'the', 'truth']), ('the', ['tells', 'simba', 'truth', 'about']), ('truth', ['simba', 'the', 'about', 'the']), ('about', ['the', 'truth', 'the', 'death']), ('the', ['truth', 'about', 'death', 'of']), ('death', ['about', 'the', 'of', 'mufasa']), ('of', ['the', 'death', 'mufasa', 'that']), ('mufasa', ['death', 'of', 'that', 'scar']), ('that', ['of', 'mufasa', 'scar', 'actually']), ('scar', ['mufasa', 'that', 'actually', 'killed']), ('actually', ['that', 'scar', 'killed', 'mufasa']), ('killed', ['scar', 'actually', 'mufasa', 'simba']), ('mufasa', ['actually', 'killed', 'simba', 'is']), ('simba', ['killed', 'mufasa', 'is', 'upset']), ('is', ['mufasa', 'simba', 'upset', 'and']), ('upset', ['simba', 'is', 'and', 'a']), ('and', ['is', 'upset', 'a', 'big']), ('a', ['upset', 'and', 'big', 'fight']), ('big', ['and', 'a', 'fight', 'happens']), ('fight', ['a', 'big', 'happens', 'the']), ('happens', ['big', 'fight', 'the', 'lionesses']), ('the', ['fight', 'happens', 'lionesses', 'fight']), ('lionesses', ['happens', 'the', 'fight', 'the']), ('fight', ['the', 'lionesses', 'the', 'hyenas']), ('the', ['lionesses', 'fight', 'hyenas', 'and']), ('hyenas', ['fight', 'the', 'and', 'simba']), ('and', ['the', 'hyenas', 'simba', 'fights']), ('simba', ['hyenas', 'and', 'fights', 'scar']), ('fights', ['and', 'simba', 'scar', 'while']), ('scar', ['simba', 'fights', 'while', 'the']), ('while', ['fights', 'scar', 'the', 'fighting']), ('the', ['scar', 'while', 'fighting', 'is']), ('fighting', ['while', 'the', 'is', 'going']), ('is', ['the', 'fighting', 'going', 'on']), ('going', ['fighting', 'is', 'on', 'lightning']), ('on', ['is', 'going', 'lightning', 'hits']), ('lightning', ['going', 'on', 'hits', 'a']), ('hits', ['on', 'lightning', 'a', 'dead']), ('a', ['lightning', 'hits', 'dead', 'tree']), ('dead', ['hits', 'a', 'tree', 'and']), ('tree', ['a', 'dead', 'and', 'starts']), ('and', ['dead', 'tree', 'starts', 'a']), ('starts', ['tree', 'and', 'a', 'fire']), ('a', ['and', 'starts', 'fire', 'scar']), ('fire', ['starts', 'a', 'scar', 'and']), ('scar', ['a', 'fire', 'and', 'simba']), ('and', ['fire', 'scar', 'simba', 'fight']), ('simba', ['scar', 'and', 'fight', 'on']), ('fight', ['and', 'simba', 'on', 'top']), ('on', ['simba', 'fight', 'top', 'of']), ('top', ['fight', 'on', 'of', 'pride']), ('of', ['on', 'top', 'pride', 'rock']), ('pride', ['top', 'of', 'rock', 'scar']), ('rock', ['of', 'pride', 'scar', 'does']), ('scar', ['pride', 'rock', 'does', 'not']), ('does', ['rock', 'scar', 'not', 'want']), ('not', ['scar', 'does', 'want', 'to']), ('want', ['does', 'not', 'to', 'die']), ('to', ['not', 'want', 'die', 'and']), ('die', ['want', 'to', 'and', 'lies']), ('and', ['to', 'die', 'lies', 'to']), ('lies', ['die', 'and', 'to', 'simba']), ('to', ['and', 'lies', 'simba', 'that']), ('simba', ['lies', 'to', 'that', 'the']), ('that', ['to', 'simba', 'the', 'hyenas']), ('the', ['simba', 'that', 'hyenas', 'are']), ('hyenas', ['that', 'the', 'are', 'to']), ('are', ['the', 'hyenas', 'to', 'blame']), ('to', ['hyenas', 'are', 'blame', 'for']), ('blame', ['are', 'to', 'for', 'everything']), ('for', ['to', 'blame', 'everything', 'another']), ('everything', ['blame', 'for', 'another', 'fight']), ('another', ['for', 'everything', 'fight', 'happens']), ('fight', ['everything', 'another', 'happens', 'and']), ('happens', ['another', 'fight', 'and', 'simba']), ('and', ['fight', 'happens', 'simba', 'throws']), ('simba', ['happens', 'and', 'throws', 'scar']), ('throws', ['and', 'simba', 'scar', 'over']), ('scar', ['simba', 'throws', 'over', 'the']), ('over', ['throws', 'scar', 'the', 'edge']), ('the', ['scar', 'over', 'edge', 'scar']), ('edge', ['over', 'the', 'scar', 'does']), ('scar', ['the', 'edge', 'does', 'not']), ('does', ['edge', 'scar', 'not', 'die']), ('not', ['scar', 'does', 'die', 'after']), ('die', ['does', 'not', 'after', 'the']), ('after', ['not', 'die', 'the', 'fall']), ('the', ['die', 'after', 'fall', 'but']), ('fall', ['after', 'the', 'but', 'the']), ('but', ['the', 'fall', 'the', 'hyenas']), ('the', ['fall', 'but', 'hyenas', 'attack']), ('hyenas', ['but', 'the', 'attack', 'and']), ('attack', ['the', 'hyenas', 'and', 'kill']), ('and', ['hyenas', 'attack', 'kill', 'him']), ('kill', ['attack', 'and', 'him', 'the']), ('him', ['and', 'kill', 'the', 'hyenas']), ('the', ['kill', 'him', 'hyenas', 'are']), ('hyenas', ['him', 'the', 'are', 'angry']), ('are', ['the', 'hyenas', 'angry', 'that']), ('angry', ['hyenas', 'are', 'that', 'scar']), ('that', ['are', 'angry', 'scar', 'blamed']), ('scar', ['angry', 'that', 'blamed', 'them']), ('blamed', ['that', 'scar', 'them', 'rain']), ('them', ['scar', 'blamed', 'rain', 'falls']), ('rain', ['blamed', 'them', 'falls', 'and']), ('falls', ['them', 'rain', 'and', 'puts']), ('and', ['rain', 'falls', 'puts', 'out']), ('puts', ['falls', 'and', 'out', 'the']), ('out', ['and', 'puts', 'the', 'fire']), ('the', ['puts', 'out', 'fire', 'simba']), ('fire', ['out', 'the', 'simba', 'walks']), ('simba', ['the', 'fire', 'walks', 'to']), ('walks', ['fire', 'simba', 'to', 'the']), ('to', ['simba', 'walks', 'the', 'top']), ('the', ['walks', 'to', 'top', 'of']), ('top', ['to', 'the', 'of', 'pride']), ('of', ['the', 'top', 'pride', 'rock']), ('pride', ['top', 'of', 'rock', 'and']), ('rock', ['of', 'pride', 'and', 'roars']), ('and', ['pride', 'rock', 'roars', 'much']), ('roars', ['rock', 'and', 'much', 'later']), ('much', ['and', 'roars', 'later', 'the']), ('later', ['roars', 'much', 'the', 'animals']), ('the', ['much', 'later', 'animals', 'come']), ('animals', ['later', 'the', 'come', 'back']), ('come', ['the', 'animals', 'back', 'at']), ('back', ['animals', 'come', 'at', 'the']), ('at', ['come', 'back', 'the', 'end']), ('the', ['back', 'at', 'end', 'of']), ('end', ['at', 'the', 'of', 'the']), ('of', ['the', 'end', 'the', 'movie']), ('the', ['end', 'of', 'movie', 'rafiki']), ('movie', ['of', 'the', 'rafiki', 'picks']), ('rafiki', ['the', 'movie', 'picks', 'up']), ('picks', ['movie', 'rafiki', 'up', 'simba']), ('up', ['rafiki', 'picks', 'simba', 'and']), ('simba', ['picks', 'up', 'and', 'nalas']), ('and', ['up', 'simba', 'nalas', 'daughter']), ('nalas', ['simba', 'and', 'daughter', 'and']), ('daughter', ['and', 'nalas', 'and', 'lifts']), ('and', ['nalas', 'daughter', 'lifts', 'her']), ('lifts', ['daughter', 'and', 'her', 'up']), ('her', ['and', 'lifts', 'up', 'high']), ('up', ['lifts', 'her', 'high', 'above']), ('high', ['her', 'up', 'above', 'pride']), ('above', ['up', 'high', 'pride', 'rock']), ('pride', ['high', 'above', 'rock', 'so']), ('rock', ['above', 'pride', 'so', 'the']), ('so', ['pride', 'rock', 'the', 'animals']), ('the', ['rock', 'so', 'animals', 'below']), ('animals', ['so', 'the', 'below', 'can']), ('below', ['the', 'animals', 'can', 'see'])]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "golden-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_tensor(words: list, word2index: dict, dtype = torch.FloatTensor):\n",
    "    \"\"\"\n",
    "    This function converts a word or a list of words into a torch tensor,\n",
    "    with appropriate format.\n",
    "    It reuses the word2index dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    tensor = dtype([word2index[word] for word in words])\n",
    "    tensor = tensor.to(device)\n",
    "    \n",
    "    return Variable(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-excuse",
   "metadata": {},
   "source": [
    "### Step 2. Create a SkipGram model and train\n",
    "\n",
    "Task #1: Write your own model for the SkipGram model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "higher-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \"\"\"\n",
    "    Your skipgram model here!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, context_size, embedding_dim, vocab_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.context_size = context_size\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 512) # This additional linear layer helps to marginally improve the model!\n",
    "        self.linear2 = nn.Linear(512, 2 * context_size * vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embeddings(inputs).view((1, -1))\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        log_probs = F.log_softmax(x, dim=1).view(2 * self.context_size, -1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "civilian-tongue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkipGram(\n",
       "  (embeddings): Embedding(389, 20)\n",
       "  (linear1): Linear(in_features=20, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=1556, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model and pass to CUDA if available.\n",
    "model = SkipGram(context_size = 2, embedding_dim = 20, vocab_size = len(vocab))\n",
    "model = model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "swiss-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters\n",
    "learning_rate = 0.001\n",
    "epochs = 825 # This is a custom number of epochs\n",
    "torch.manual_seed(28)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-arcade",
   "metadata": {},
   "source": [
    "Task #2: Write your own training function for the SkipGram model in the cell below. It should return a list of losses and accuracies for display later on, along with your trained model. You may also write a helper function for computing the accuracy of your model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "canadian-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(context, model, word2index):\n",
    "    \"\"\"\n",
    "    This is a helper function to get predictions from our model.\n",
    "    \"\"\"\n",
    "    context_idxs = torch.tensor([word2index[context]], dtype=torch.long).cuda()\n",
    "    log_probs = model(context_idxs)\n",
    "    output_ids = torch.argmax(log_probs, dim=1)\n",
    "\n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d50da5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model, data, word2index):\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "\n",
    "    for context, targets in data:\n",
    "        output_ids = get_prediction(context, model, word2index).tolist()\n",
    "\n",
    "        target_list = [word2index[w] for w in targets]\n",
    "\n",
    "        for target in target_list:\n",
    "            if target in output_ids:\n",
    "                correct_count += 1\n",
    "                output_ids.remove(target)\n",
    "\n",
    "        total_count += len(target_list)\n",
    "\n",
    "    return correct_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "periodic-native",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss - 7.3277794208951255; accuracy - 0.0939239332096475\n",
      "Epoch 2: loss - 7.20163759052864; accuracy - 0.09485157699443414\n",
      "Epoch 3: loss - 7.084025919990327; accuracy - 0.08441558441558442\n",
      "Epoch 4: loss - 6.978375266788178; accuracy - 0.08024118738404452\n",
      "Epoch 5: loss - 6.887717306282171; accuracy - 0.08024118738404452\n",
      "Epoch 6: loss - 6.812297085884109; accuracy - 0.08024118738404452\n",
      "Epoch 7: loss - 6.749457310214777; accuracy - 0.0839517625231911\n",
      "Epoch 8: loss - 6.695868478856414; accuracy - 0.08905380333951762\n",
      "Epoch 9: loss - 6.649014066013201; accuracy - 0.10041743970315399\n",
      "Epoch 10: loss - 6.6071904868938045; accuracy - 0.11062152133580705\n",
      "Epoch 11: loss - 6.56918681486197; accuracy - 0.11665120593692022\n",
      "Epoch 12: loss - 6.5341241585301555; accuracy - 0.12036178107606679\n",
      "Epoch 13: loss - 6.501382026247722; accuracy - 0.12708719851576994\n",
      "Epoch 14: loss - 6.470530901415229; accuracy - 0.13218923933209648\n",
      "Epoch 15: loss - 6.441264029557718; accuracy - 0.13868274582560297\n",
      "Epoch 16: loss - 6.413346416211526; accuracy - 0.14401669758812616\n",
      "Epoch 17: loss - 6.386590350757945; accuracy - 0.14679962894248608\n",
      "Epoch 18: loss - 6.360839046662283; accuracy - 0.14888682745825602\n",
      "Epoch 19: loss - 6.335958973153845; accuracy - 0.14865491651205937\n",
      "Epoch 20: loss - 6.311835566345526; accuracy - 0.15213358070500926\n",
      "Epoch 21: loss - 6.2883728731541115; accuracy - 0.15282931354359927\n",
      "Epoch 22: loss - 6.265493492027382; accuracy - 0.1537569573283859\n",
      "Epoch 23: loss - 6.243119573327738; accuracy - 0.15769944341372913\n",
      "Epoch 24: loss - 6.2211908984493896; accuracy - 0.1602504638218924\n",
      "Epoch 25: loss - 6.199658665011234; accuracy - 0.16396103896103897\n",
      "Epoch 26: loss - 6.178478788798727; accuracy - 0.16535250463821893\n",
      "Epoch 27: loss - 6.157616568407898; accuracy - 0.16581632653061223\n",
      "Epoch 28: loss - 6.137039581565999; accuracy - 0.16651205936920221\n",
      "Epoch 29: loss - 6.1167210532915615; accuracy - 0.16743970315398887\n",
      "Epoch 30: loss - 6.096641062142013; accuracy - 0.16859925788497218\n",
      "Epoch 31: loss - 6.07677737259909; accuracy - 0.17045454545454544\n",
      "Epoch 32: loss - 6.057111432025959; accuracy - 0.1713821892393321\n",
      "Epoch 33: loss - 6.037624998304972; accuracy - 0.1730055658627087\n",
      "Epoch 34: loss - 6.018307924712965; accuracy - 0.17555658627087198\n",
      "Epoch 35: loss - 5.999148921975399; accuracy - 0.17717996289424862\n",
      "Epoch 36: loss - 5.980135030339512; accuracy - 0.17694805194805194\n",
      "Epoch 37: loss - 5.961255188775637; accuracy - 0.17810760667903525\n",
      "Epoch 38: loss - 5.942500452384878; accuracy - 0.17857142857142858\n",
      "Epoch 39: loss - 5.923860771094271; accuracy - 0.1794990723562152\n",
      "Epoch 40: loss - 5.905326110091413; accuracy - 0.18112244897959184\n",
      "Epoch 41: loss - 5.886890847961624; accuracy - 0.1813543599257885\n",
      "Epoch 42: loss - 5.868546665710067; accuracy - 0.18205009276437847\n",
      "Epoch 43: loss - 5.850286674632212; accuracy - 0.18344155844155843\n",
      "Epoch 44: loss - 5.832110508695825; accuracy - 0.1843692022263451\n",
      "Epoch 45: loss - 5.814008275299214; accuracy - 0.1841372912801484\n",
      "Epoch 46: loss - 5.795976986026941; accuracy - 0.18668831168831168\n",
      "Epoch 47: loss - 5.778008961500618; accuracy - 0.18761595547309834\n",
      "Epoch 48: loss - 5.760098302740334; accuracy - 0.18807977736549164\n",
      "Epoch 49: loss - 5.742244394236902; accuracy - 0.1890074211502783\n",
      "Epoch 50: loss - 5.724440900425743; accuracy - 0.1906307977736549\n",
      "Epoch 51: loss - 5.706686462011319; accuracy - 0.1908627087198516\n",
      "Epoch 52: loss - 5.688976644584995; accuracy - 0.1924860853432282\n",
      "Epoch 53: loss - 5.671309294859864; accuracy - 0.1924860853432282\n",
      "Epoch 54: loss - 5.653680258204189; accuracy - 0.1936456400742115\n",
      "Epoch 55: loss - 5.636085309876139; accuracy - 0.19480519480519481\n",
      "Epoch 56: loss - 5.618523515664138; accuracy - 0.19642857142857142\n",
      "Epoch 57: loss - 5.6009911181533045; accuracy - 0.19666048237476808\n",
      "Epoch 58: loss - 5.583487812574807; accuracy - 0.19782003710575138\n",
      "Epoch 59: loss - 5.5660113549630585; accuracy - 0.20060296846011133\n",
      "Epoch 60: loss - 5.548559993892521; accuracy - 0.2017625231910946\n",
      "Epoch 61: loss - 5.531134720635989; accuracy - 0.20292207792207792\n",
      "Epoch 62: loss - 5.513730417156043; accuracy - 0.20500927643784786\n",
      "Epoch 63: loss - 5.496349636168117; accuracy - 0.2070964749536178\n",
      "Epoch 64: loss - 5.478989690486929; accuracy - 0.20941558441558442\n",
      "Epoch 65: loss - 5.461653282337154; accuracy - 0.21080705009276438\n",
      "Epoch 66: loss - 5.444337427063201; accuracy - 0.21173469387755103\n",
      "Epoch 67: loss - 5.4270387983056745; accuracy - 0.21150278293135436\n",
      "Epoch 68: loss - 5.409762401085396; accuracy - 0.212430426716141\n",
      "Epoch 69: loss - 5.392504642093779; accuracy - 0.2147495361781076\n",
      "Epoch 70: loss - 5.375267878091844; accuracy - 0.2166048237476809\n",
      "Epoch 71: loss - 5.358050266975375; accuracy - 0.21846011131725418\n",
      "Epoch 72: loss - 5.340851260691273; accuracy - 0.2196196660482375\n",
      "Epoch 73: loss - 5.323670581453143; accuracy - 0.2196196660482375\n",
      "Epoch 74: loss - 5.306510346497587; accuracy - 0.22077922077922077\n",
      "Epoch 75: loss - 5.2893697178208985; accuracy - 0.2212430426716141\n",
      "Epoch 76: loss - 5.272250463215009; accuracy - 0.2224025974025974\n",
      "Epoch 77: loss - 5.255153094019208; accuracy - 0.2230983302411874\n",
      "Epoch 78: loss - 5.238077299493142; accuracy - 0.22495361781076068\n",
      "Epoch 79: loss - 5.221027166148948; accuracy - 0.22541743970315398\n",
      "Epoch 80: loss - 5.204001753361194; accuracy - 0.22564935064935066\n",
      "Epoch 81: loss - 5.1870007740544475; accuracy - 0.22657699443413729\n",
      "Epoch 82: loss - 5.1700283693692235; accuracy - 0.22518552875695733\n",
      "Epoch 83: loss - 5.153080917023993; accuracy - 0.22541743970315398\n",
      "Epoch 84: loss - 5.136163229623841; accuracy - 0.22518552875695733\n",
      "Epoch 85: loss - 5.119274650964755; accuracy - 0.22564935064935066\n",
      "Epoch 86: loss - 5.102418454104761; accuracy - 0.22680890538033396\n",
      "Epoch 87: loss - 5.085592299533908; accuracy - 0.22843228200371057\n",
      "Epoch 88: loss - 5.068800475690272; accuracy - 0.22959183673469388\n",
      "Epoch 89: loss - 5.052045045845584; accuracy - 0.23098330241187384\n",
      "Epoch 90: loss - 5.035325167130451; accuracy - 0.2323747680890538\n",
      "Epoch 91: loss - 5.01864053477604; accuracy - 0.2335343228200371\n",
      "Epoch 92: loss - 5.001994330498195; accuracy - 0.2353896103896104\n",
      "Epoch 93: loss - 4.985387660797983; accuracy - 0.2358534322820037\n",
      "Epoch 94: loss - 4.968822100166929; accuracy - 0.237012987012987\n",
      "Epoch 95: loss - 4.952298293529504; accuracy - 0.237708719851577\n",
      "Epoch 96: loss - 4.93581802703454; accuracy - 0.2400278293135436\n",
      "Epoch 97: loss - 4.919381622480771; accuracy - 0.2423469387755102\n",
      "Epoch 98: loss - 4.9029893507984; accuracy - 0.24281076066790352\n",
      "Epoch 99: loss - 4.886647687108707; accuracy - 0.24373840445269015\n",
      "Epoch 100: loss - 4.870353629947372; accuracy - 0.2465213358070501\n",
      "Epoch 101: loss - 4.854110917930037; accuracy - 0.24953617810760667\n",
      "Epoch 102: loss - 4.8379163848225835; accuracy - 0.2511595547309833\n",
      "Epoch 103: loss - 4.821774342728015; accuracy - 0.25162337662337664\n",
      "Epoch 104: loss - 4.805685592936231; accuracy - 0.25208719851576994\n",
      "Epoch 105: loss - 4.789650277657942; accuracy - 0.2527829313543599\n",
      "Epoch 106: loss - 4.773673972747321; accuracy - 0.2532467532467532\n",
      "Epoch 107: loss - 4.757751567686643; accuracy - 0.2541743970315399\n",
      "Epoch 108: loss - 4.741890052917495; accuracy - 0.2553339517625232\n",
      "Epoch 109: loss - 4.726088090818756; accuracy - 0.2569573283858998\n",
      "Epoch 110: loss - 4.710348722443731; accuracy - 0.25834879406307976\n",
      "Epoch 111: loss - 4.69467188299034; accuracy - 0.2592764378478664\n",
      "Epoch 112: loss - 4.6790584825628985; accuracy - 0.2604359925788497\n",
      "Epoch 113: loss - 4.66351234227253; accuracy - 0.2606679035250464\n",
      "Epoch 114: loss - 4.648031661373344; accuracy - 0.2618274582560297\n",
      "Epoch 115: loss - 4.632620697782301; accuracy - 0.262987012987013\n",
      "Epoch 116: loss - 4.617279493078893; accuracy - 0.26553803339517623\n",
      "Epoch 117: loss - 4.602010820040234; accuracy - 0.2660018552875696\n",
      "Epoch 118: loss - 4.586811847722155; accuracy - 0.2664656771799629\n",
      "Epoch 119: loss - 4.571688420697355; accuracy - 0.26855287569573283\n",
      "Epoch 120: loss - 4.556640786673452; accuracy - 0.27040816326530615\n",
      "Epoch 121: loss - 4.541671721966235; accuracy - 0.2720315398886827\n",
      "Epoch 122: loss - 4.526777587043111; accuracy - 0.2724953617810761\n",
      "Epoch 123: loss - 4.511963309737437; accuracy - 0.2734230055658627\n",
      "Epoch 124: loss - 4.4972295668218045; accuracy - 0.274582560296846\n",
      "Epoch 125: loss - 4.482576585656416; accuracy - 0.27713358070500926\n",
      "Epoch 126: loss - 4.468006191094421; accuracy - 0.2782931354359926\n",
      "Epoch 127: loss - 4.453522415683088; accuracy - 0.28014842300556586\n",
      "Epoch 128: loss - 4.439121379701901; accuracy - 0.2815398886827458\n",
      "Epoch 129: loss - 4.424811093395849; accuracy - 0.28316326530612246\n",
      "Epoch 130: loss - 4.410585456964921; accuracy - 0.2854823747680891\n",
      "Epoch 131: loss - 4.3964514913718205; accuracy - 0.28780148423005564\n",
      "Epoch 132: loss - 4.382408524709642; accuracy - 0.288961038961039\n",
      "Epoch 133: loss - 4.368457225567777; accuracy - 0.2917439703153989\n",
      "Epoch 134: loss - 4.354600011528312; accuracy - 0.29313543599257885\n",
      "Epoch 135: loss - 4.340835594953987; accuracy - 0.2947588126159555\n",
      "Epoch 136: loss - 4.327167269913739; accuracy - 0.2952226345083488\n",
      "Epoch 137: loss - 4.313597770038031; accuracy - 0.29615027829313545\n",
      "Epoch 138: loss - 4.300124161539803; accuracy - 0.29730983302411873\n",
      "Epoch 139: loss - 4.286750031092613; accuracy - 0.30009276437847865\n",
      "Epoch 140: loss - 4.273475075475803; accuracy - 0.301252319109462\n",
      "Epoch 141: loss - 4.260302941299325; accuracy - 0.30264378478664195\n",
      "Epoch 142: loss - 4.247230053390333; accuracy - 0.3051948051948052\n",
      "Epoch 143: loss - 4.234260852571321; accuracy - 0.3068181818181818\n",
      "Epoch 144: loss - 4.221394828387669; accuracy - 0.3086734693877551\n",
      "Epoch 145: loss - 4.208634231661158; accuracy - 0.3105287569573284\n",
      "Epoch 146: loss - 4.195978358415593; accuracy - 0.31261595547309834\n",
      "Epoch 147: loss - 4.183429932550066; accuracy - 0.3151669758812616\n",
      "Epoch 148: loss - 4.17098386234609; accuracy - 0.31563079777365494\n",
      "Epoch 149: loss - 4.1586487880443155; accuracy - 0.3179499072356215\n",
      "Epoch 150: loss - 4.146417160312851; accuracy - 0.32258812615955473\n",
      "Epoch 151: loss - 4.134296735054044; accuracy - 0.325139146567718\n",
      "Epoch 152: loss - 4.122284737057942; accuracy - 0.3267625231910946\n",
      "Epoch 153: loss - 4.1103776004407315; accuracy - 0.32954545454545453\n",
      "Epoch 154: loss - 4.0985825700972205; accuracy - 0.33163265306122447\n",
      "Epoch 155: loss - 4.08689386680971; accuracy - 0.3344155844155844\n",
      "Epoch 156: loss - 4.075315500787547; accuracy - 0.336038961038961\n",
      "Epoch 157: loss - 4.063848558516139; accuracy - 0.33789424860853434\n",
      "Epoch 158: loss - 4.05248845574586; accuracy - 0.3404452690166976\n",
      "Epoch 159: loss - 4.041238848928618; accuracy - 0.3425324675324675\n",
      "Epoch 160: loss - 4.030100104760151; accuracy - 0.3439239332096475\n",
      "Epoch 161: loss - 4.019068998618117; accuracy - 0.3467068645640074\n",
      "Epoch 162: loss - 4.0081503907692015; accuracy - 0.3494897959183674\n",
      "Epoch 163: loss - 3.9973396939780144; accuracy - 0.3529684601113173\n",
      "Epoch 164: loss - 3.986636950376967; accuracy - 0.3557513914656772\n",
      "Epoch 165: loss - 3.9760446321765213; accuracy - 0.3580705009276438\n",
      "Epoch 166: loss - 3.965560548159541; accuracy - 0.36062152133580705\n",
      "Epoch 167: loss - 3.955185848728845; accuracy - 0.3622448979591837\n",
      "Epoch 168: loss - 3.944919818296946; accuracy - 0.36410018552875695\n",
      "Epoch 169: loss - 3.9347591657806635; accuracy - 0.3650278293135436\n",
      "Epoch 170: loss - 3.9247077519906917; accuracy - 0.3673469387755102\n",
      "Epoch 171: loss - 3.9147660038909136; accuracy - 0.3685064935064935\n",
      "Epoch 172: loss - 3.904928360334796; accuracy - 0.37059369202226344\n",
      "Epoch 173: loss - 3.8951994556664093; accuracy - 0.37337662337662336\n",
      "Epoch 174: loss - 3.8855746517818357; accuracy - 0.3754638218923933\n",
      "Epoch 175: loss - 3.876058085619408; accuracy - 0.37685528756957326\n",
      "Epoch 176: loss - 3.8666440954013748; accuracy - 0.3787105751391466\n",
      "Epoch 177: loss - 3.857335501345279; accuracy - 0.3812615955473098\n",
      "Epoch 178: loss - 3.848131224374824; accuracy - 0.38311688311688313\n",
      "Epoch 179: loss - 3.8390326708942264; accuracy - 0.3849721706864564\n",
      "Epoch 180: loss - 3.8300354818023865; accuracy - 0.38659554730983303\n",
      "Epoch 181: loss - 3.8211398205420966; accuracy - 0.38705936920222633\n",
      "Epoch 182: loss - 3.812347125033943; accuracy - 0.38868274582560297\n",
      "Epoch 183: loss - 3.803656036092974; accuracy - 0.3907699443413729\n",
      "Epoch 184: loss - 3.795062916177102; accuracy - 0.39239332096474955\n",
      "Epoch 185: loss - 3.786571765764304; accuracy - 0.3942486085343228\n",
      "Epoch 186: loss - 3.7781778927857887; accuracy - 0.39564007421150277\n",
      "Epoch 187: loss - 3.7698829781146572; accuracy - 0.3970315398886827\n",
      "Epoch 188: loss - 3.7616852103889764; accuracy - 0.3977272727272727\n",
      "Epoch 189: loss - 3.753585200584001; accuracy - 0.399582560296846\n",
      "Epoch 190: loss - 3.7455809048907436; accuracy - 0.40190166975881264\n",
      "Epoch 191: loss - 3.737671566031638; accuracy - 0.4039888682745826\n",
      "Epoch 192: loss - 3.729856544611405; accuracy - 0.40514842300556586\n",
      "Epoch 193: loss - 3.722135503791922; accuracy - 0.4060760667903525\n",
      "Epoch 194: loss - 3.714509667735374; accuracy - 0.4079313543599258\n",
      "Epoch 195: loss - 3.7069766480537867; accuracy - 0.40885899814471244\n",
      "Epoch 196: loss - 3.6995313547980144; accuracy - 0.4095547309833024\n",
      "Epoch 197: loss - 3.6921777349234954; accuracy - 0.41117810760667906\n",
      "Epoch 198: loss - 3.6849159962171085; accuracy - 0.4130333951762523\n",
      "Epoch 199: loss - 3.6777388846055916; accuracy - 0.41465677179962895\n",
      "Epoch 200: loss - 3.6706515146982692; accuracy - 0.4155844155844156\n",
      "Epoch 201: loss - 3.6636515412348323; accuracy - 0.4179035250463822\n",
      "Epoch 202: loss - 3.656737022032764; accuracy - 0.41836734693877553\n",
      "Epoch 203: loss - 3.6499107515435933; accuracy - 0.42045454545454547\n",
      "Epoch 204: loss - 3.643162714085906; accuracy - 0.42115027829313545\n",
      "Epoch 205: loss - 3.6365032468080964; accuracy - 0.42161410018552875\n",
      "Epoch 206: loss - 3.6299238036868746; accuracy - 0.42207792207792205\n",
      "Epoch 207: loss - 3.6234278900061554; accuracy - 0.42393320964749537\n",
      "Epoch 208: loss - 3.617008075864505; accuracy - 0.425556586270872\n",
      "Epoch 209: loss - 3.610674734350038; accuracy - 0.4264842300556586\n",
      "Epoch 210: loss - 3.6044165454459325; accuracy - 0.4283395176252319\n",
      "Epoch 211: loss - 3.5982364497954418; accuracy - 0.4290352504638219\n",
      "Epoch 212: loss - 3.592132748279147; accuracy - 0.4299628942486085\n",
      "Epoch 213: loss - 3.5861065477298673; accuracy - 0.4318181818181818\n",
      "Epoch 214: loss - 3.5801558919209495; accuracy - 0.4327458256029685\n",
      "Epoch 215: loss - 3.5742800204784837; accuracy - 0.43413729128014844\n",
      "Epoch 216: loss - 3.5684754312149005; accuracy - 0.4355287569573284\n",
      "Epoch 217: loss - 3.5627444115552036; accuracy - 0.4366883116883117\n",
      "Epoch 218: loss - 3.557084488514845; accuracy - 0.4362244897959184\n",
      "Epoch 219: loss - 3.551497064848778; accuracy - 0.43738404452690166\n",
      "Epoch 220: loss - 3.5459763538196047; accuracy - 0.438543599257885\n",
      "Epoch 221: loss - 3.540528283048428; accuracy - 0.4390074211502783\n",
      "Epoch 222: loss - 3.5351480590611533; accuracy - 0.439239332096475\n",
      "Epoch 223: loss - 3.529832861529653; accuracy - 0.4394712430426716\n",
      "Epoch 224: loss - 3.5245859123116743; accuracy - 0.4401669758812616\n",
      "Epoch 225: loss - 3.519402593097793; accuracy - 0.44063079777365494\n",
      "Epoch 226: loss - 3.5142852244881397; accuracy - 0.4422541743970315\n",
      "Epoch 227: loss - 3.5092321393220014; accuracy - 0.4429499072356215\n",
      "Epoch 228: loss - 3.50424252989562; accuracy - 0.4431818181818182\n",
      "Epoch 229: loss - 3.4993147476263524; accuracy - 0.44341372912801486\n",
      "Epoch 230: loss - 3.494448731471966; accuracy - 0.4436456400742115\n",
      "Epoch 231: loss - 3.4896436907586006; accuracy - 0.44434137291280146\n",
      "Epoch 232: loss - 3.4848961396650835; accuracy - 0.4448051948051948\n",
      "Epoch 233: loss - 3.4802102459826143; accuracy - 0.4455009276437848\n",
      "Epoch 234: loss - 3.47558384359657; accuracy - 0.4461966604823748\n",
      "Epoch 235: loss - 3.4710133948219952; accuracy - 0.4466604823747681\n",
      "Epoch 236: loss - 3.4664981501221876; accuracy - 0.4466604823747681\n",
      "Epoch 237: loss - 3.462040668073523; accuracy - 0.44735621521335805\n",
      "Epoch 238: loss - 3.457636899872923; accuracy - 0.4482838589981447\n",
      "Epoch 239: loss - 3.4532901609319926; accuracy - 0.448747680890538\n",
      "Epoch 240: loss - 3.4489944889063295; accuracy - 0.449443413729128\n",
      "Epoch 241: loss - 3.4447559068508182; accuracy - 0.4496753246753247\n",
      "Epoch 242: loss - 3.440565849832347; accuracy - 0.450139146567718\n",
      "Epoch 243: loss - 3.4364297287141237; accuracy - 0.450139146567718\n",
      "Epoch 244: loss - 3.4323432548369017; accuracy - 0.450139146567718\n",
      "Epoch 245: loss - 3.4283050008961355; accuracy - 0.45060296846011133\n",
      "Epoch 246: loss - 3.4243210948906935; accuracy - 0.45060296846011133\n",
      "Epoch 247: loss - 3.420380870464339; accuracy - 0.45083487940630795\n",
      "Epoch 248: loss - 3.4164904357108643; accuracy - 0.45106679035250463\n",
      "Epoch 249: loss - 3.4126490200826542; accuracy - 0.4512987012987013\n",
      "Epoch 250: loss - 3.408853589382596; accuracy - 0.45153061224489793\n",
      "Epoch 251: loss - 3.4051023604476165; accuracy - 0.45153061224489793\n",
      "Epoch 252: loss - 3.4014007323084603; accuracy - 0.4519944341372913\n",
      "Epoch 253: loss - 3.3977393868460504; accuracy - 0.4519944341372913\n",
      "Epoch 254: loss - 3.3941271629094634; accuracy - 0.4524582560296846\n",
      "Epoch 255: loss - 3.390556246761046; accuracy - 0.4536178107606679\n",
      "Epoch 256: loss - 3.3870280689122727; accuracy - 0.4536178107606679\n",
      "Epoch 257: loss - 3.383541242089917; accuracy - 0.45384972170686455\n",
      "Epoch 258: loss - 3.380099742111779; accuracy - 0.45384972170686455\n",
      "Epoch 259: loss - 3.3766957468358396; accuracy - 0.45408163265306123\n",
      "Epoch 260: loss - 3.3733335261221056; accuracy - 0.4543135435992579\n",
      "Epoch 261: loss - 3.3700116673736713; accuracy - 0.4550092764378479\n",
      "Epoch 262: loss - 3.366731666346428; accuracy - 0.4552411873840445\n",
      "Epoch 263: loss - 3.363486682660947; accuracy - 0.45570500927643787\n",
      "Epoch 264: loss - 3.360284348918688; accuracy - 0.45570500927643787\n",
      "Epoch 265: loss - 3.357119205599592; accuracy - 0.45570500927643787\n",
      "Epoch 266: loss - 3.3539885042549726; accuracy - 0.4559369202226345\n",
      "Epoch 267: loss - 3.350895579206258; accuracy - 0.45663265306122447\n",
      "Epoch 268: loss - 3.347839878751086; accuracy - 0.45732838589981445\n",
      "Epoch 269: loss - 3.344820719703009; accuracy - 0.45732838589981445\n",
      "Epoch 270: loss - 3.341836408250628; accuracy - 0.4575602968460111\n",
      "Epoch 271: loss - 3.3388860091871146; accuracy - 0.4577922077922078\n",
      "Epoch 272: loss - 3.3359703099793983; accuracy - 0.45802411873840443\n",
      "Epoch 273: loss - 3.3330919447325598; accuracy - 0.45802411873840443\n",
      "Epoch 274: loss - 3.330242997197804; accuracy - 0.4582560296846011\n",
      "Epoch 275: loss - 3.32742675259737; accuracy - 0.4587198515769944\n",
      "Epoch 276: loss - 3.3246445785205747; accuracy - 0.4587198515769944\n",
      "Epoch 277: loss - 3.3218959877353873; accuracy - 0.4587198515769944\n",
      "Epoch 278: loss - 3.319178733732793; accuracy - 0.45964749536178107\n",
      "Epoch 279: loss - 3.3164894577523554; accuracy - 0.4601113172541744\n",
      "Epoch 280: loss - 3.3138344994502518; accuracy - 0.46034322820037105\n",
      "Epoch 281: loss - 3.3112093929014756; accuracy - 0.4605751391465677\n",
      "Epoch 282: loss - 3.308613514303055; accuracy - 0.4612708719851577\n",
      "Epoch 283: loss - 3.306045678307705; accuracy - 0.461734693877551\n",
      "Epoch 284: loss - 3.303510634819519; accuracy - 0.4615027829313544\n",
      "Epoch 285: loss - 3.3009961500680953; accuracy - 0.4615027829313544\n",
      "Epoch 286: loss - 3.2985189764972964; accuracy - 0.4615027829313544\n",
      "Epoch 287: loss - 3.296066463878292; accuracy - 0.4619666048237477\n",
      "Epoch 288: loss - 3.2936401860391054; accuracy - 0.4619666048237477\n",
      "Epoch 289: loss - 3.2912427508764672; accuracy - 0.46266233766233766\n",
      "Epoch 290: loss - 3.288870287737731; accuracy - 0.46266233766233766\n",
      "Epoch 291: loss - 3.286525870077243; accuracy - 0.46266233766233766\n",
      "Epoch 292: loss - 3.284207069143957; accuracy - 0.46289424860853434\n",
      "Epoch 293: loss - 3.2819142465467577; accuracy - 0.46289424860853434\n",
      "Epoch 294: loss - 3.279645683592901; accuracy - 0.46289424860853434\n",
      "Epoch 295: loss - 3.2774020769600525; accuracy - 0.46312615955473097\n",
      "Epoch 296: loss - 3.2751828239667393; accuracy - 0.46335807050092764\n",
      "Epoch 297: loss - 3.2729898487703255; accuracy - 0.46382189239332094\n",
      "Epoch 298: loss - 3.2708211094417465; accuracy - 0.4640538033395176\n",
      "Epoch 299: loss - 3.2686725392633553; accuracy - 0.4640538033395176\n",
      "Epoch 300: loss - 3.2665519615935925; accuracy - 0.4640538033395176\n",
      "Epoch 301: loss - 3.2644530802356955; accuracy - 0.4640538033395176\n",
      "Epoch 302: loss - 3.2623755777036987; accuracy - 0.4645176252319109\n",
      "Epoch 303: loss - 3.260320195461691; accuracy - 0.4647495361781076\n",
      "Epoch 304: loss - 3.2582890508568574; accuracy - 0.4647495361781076\n",
      "Epoch 305: loss - 3.256277971895816; accuracy - 0.4647495361781076\n",
      "Epoch 306: loss - 3.254290886292431; accuracy - 0.4647495361781076\n",
      "Epoch 307: loss - 3.2523220909106265; accuracy - 0.4649814471243043\n",
      "Epoch 308: loss - 3.2503780763534094; accuracy - 0.4652133580705009\n",
      "Epoch 309: loss - 3.248454007898943; accuracy - 0.4652133580705009\n",
      "Epoch 310: loss - 3.246546230975241; accuracy - 0.4652133580705009\n",
      "Epoch 311: loss - 3.2446620113774443; accuracy - 0.4652133580705009\n",
      "Epoch 312: loss - 3.242796565583995; accuracy - 0.4652133580705009\n",
      "Epoch 313: loss - 3.240952847508199; accuracy - 0.4654452690166976\n",
      "Epoch 314: loss - 3.2391273076326375; accuracy - 0.4654452690166976\n",
      "Epoch 315: loss - 3.2373201497631747; accuracy - 0.46567717996289426\n",
      "Epoch 316: loss - 3.235535812731798; accuracy - 0.46567717996289426\n",
      "Epoch 317: loss - 3.2337656979879332; accuracy - 0.46567717996289426\n",
      "Epoch 318: loss - 3.232017478053778; accuracy - 0.4659090909090909\n",
      "Epoch 319: loss - 3.230285080775259; accuracy - 0.4659090909090909\n",
      "Epoch 320: loss - 3.2285727459336035; accuracy - 0.4659090909090909\n",
      "Epoch 321: loss - 3.2268766473308346; accuracy - 0.46614100185528756\n",
      "Epoch 322: loss - 3.225198289237792; accuracy - 0.46614100185528756\n",
      "Epoch 323: loss - 3.223537828130492; accuracy - 0.46660482374768086\n",
      "Epoch 324: loss - 3.221892877279717; accuracy - 0.4670686456400742\n",
      "Epoch 325: loss - 3.220267036186742; accuracy - 0.4670686456400742\n",
      "Epoch 326: loss - 3.2186600176655515; accuracy - 0.4670686456400742\n",
      "Epoch 327: loss - 3.217064930689357; accuracy - 0.4670686456400742\n",
      "Epoch 328: loss - 3.215488563442053; accuracy - 0.4670686456400742\n",
      "Epoch 329: loss - 3.2139286226376975; accuracy - 0.4673005565862709\n",
      "Epoch 330: loss - 3.2123800474106714; accuracy - 0.4673005565862709\n",
      "Epoch 331: loss - 3.2108517411909654; accuracy - 0.4673005565862709\n",
      "Epoch 332: loss - 3.209339403929206; accuracy - 0.4675324675324675\n",
      "Epoch 333: loss - 3.207841542634097; accuracy - 0.4675324675324675\n",
      "Epoch 334: loss - 3.2063550905969014; accuracy - 0.4677643784786642\n",
      "Epoch 335: loss - 3.2048869878330124; accuracy - 0.4679962894248609\n",
      "Epoch 336: loss - 3.203434194284379; accuracy - 0.4679962894248609\n",
      "Epoch 337: loss - 3.2019918179467792; accuracy - 0.4679962894248609\n",
      "Epoch 338: loss - 3.2005698752757126; accuracy - 0.4679962894248609\n",
      "Epoch 339: loss - 3.1991583204004006; accuracy - 0.4679962894248609\n",
      "Epoch 340: loss - 3.197762074961512; accuracy - 0.4679962894248609\n",
      "Epoch 341: loss - 3.1963823174501393; accuracy - 0.4679962894248609\n",
      "Epoch 342: loss - 3.1950105821268013; accuracy - 0.4679962894248609\n",
      "Epoch 343: loss - 3.1936549461396595; accuracy - 0.4679962894248609\n",
      "Epoch 344: loss - 3.1923142164005642; accuracy - 0.4682282003710575\n",
      "Epoch 345: loss - 3.1909826753975508; accuracy - 0.4684601113172542\n",
      "Epoch 346: loss - 3.1896719505702853; accuracy - 0.4684601113172542\n",
      "Epoch 347: loss - 3.1883647635385586; accuracy - 0.46869202226345086\n",
      "Epoch 348: loss - 3.1870788665339767; accuracy - 0.46869202226345086\n",
      "Epoch 349: loss - 3.185796841509046; accuracy - 0.46869202226345086\n",
      "Epoch 350: loss - 3.1845356677149135; accuracy - 0.46869202226345086\n",
      "Epoch 351: loss - 3.1832822330143986; accuracy - 0.46869202226345086\n",
      "Epoch 352: loss - 3.182042909907056; accuracy - 0.46869202226345086\n",
      "Epoch 353: loss - 3.18081291254907; accuracy - 0.4689239332096475\n",
      "Epoch 354: loss - 3.179596151182072; accuracy - 0.4689239332096475\n",
      "Epoch 355: loss - 3.1783943443882223; accuracy - 0.4689239332096475\n",
      "Epoch 356: loss - 3.1772001769414415; accuracy - 0.4689239332096475\n",
      "Epoch 357: loss - 3.1760201053849397; accuracy - 0.4689239332096475\n",
      "Epoch 358: loss - 3.174850195038075; accuracy - 0.4689239332096475\n",
      "Epoch 359: loss - 3.173691373366814; accuracy - 0.4689239332096475\n",
      "Epoch 360: loss - 3.1725470250081926; accuracy - 0.4689239332096475\n",
      "Epoch 361: loss - 3.1714070818640967; accuracy - 0.4689239332096475\n",
      "Epoch 362: loss - 3.1702846875880777; accuracy - 0.4689239332096475\n",
      "Epoch 363: loss - 3.1691673319503417; accuracy - 0.46869202226345086\n",
      "Epoch 364: loss - 3.1680618615230074; accuracy - 0.46869202226345086\n",
      "Epoch 365: loss - 3.1669693955242746; accuracy - 0.46869202226345086\n",
      "Epoch 366: loss - 3.165884806959439; accuracy - 0.4689239332096475\n",
      "Epoch 367: loss - 3.1648132538972407; accuracy - 0.4689239332096475\n",
      "Epoch 368: loss - 3.1637525414712795; accuracy - 0.46915584415584416\n",
      "Epoch 369: loss - 3.1626965827092612; accuracy - 0.46915584415584416\n",
      "Epoch 370: loss - 3.1616530951409705; accuracy - 0.46915584415584416\n",
      "Epoch 371: loss - 3.160619090461554; accuracy - 0.46915584415584416\n",
      "Epoch 372: loss - 3.15959446085185; accuracy - 0.46938775510204084\n",
      "Epoch 373: loss - 3.158581153249475; accuracy - 0.46938775510204084\n",
      "Epoch 374: loss - 3.157577072423995; accuracy - 0.46938775510204084\n",
      "Epoch 375: loss - 3.1565811895041387; accuracy - 0.46938775510204084\n",
      "Epoch 376: loss - 3.155593464900921; accuracy - 0.46938775510204084\n",
      "Epoch 377: loss - 3.1546190960508995; accuracy - 0.46938775510204084\n",
      "Epoch 378: loss - 3.1536495529654296; accuracy - 0.46938775510204084\n",
      "Epoch 379: loss - 3.1526914635925434; accuracy - 0.46938775510204084\n",
      "Epoch 380: loss - 3.1517376569073807; accuracy - 0.46938775510204084\n",
      "Epoch 381: loss - 3.150800218719277; accuracy - 0.46938775510204084\n",
      "Epoch 382: loss - 3.1498665016745813; accuracy - 0.46938775510204084\n",
      "Epoch 383: loss - 3.148940309854518; accuracy - 0.46961966604823746\n",
      "Epoch 384: loss - 3.1480232998037603; accuracy - 0.46938775510204084\n",
      "Epoch 385: loss - 3.1471166086550766; accuracy - 0.46938775510204084\n",
      "Epoch 386: loss - 3.146219180444176; accuracy - 0.46961966604823746\n",
      "Epoch 387: loss - 3.1453248394000064; accuracy - 0.46985157699443414\n",
      "Epoch 388: loss - 3.144443353606951; accuracy - 0.46985157699443414\n",
      "Epoch 389: loss - 3.143565853694817; accuracy - 0.46985157699443414\n",
      "Epoch 390: loss - 3.1427016205159544; accuracy - 0.46985157699443414\n",
      "Epoch 391: loss - 3.1418389630450387; accuracy - 0.46985157699443414\n",
      "Epoch 392: loss - 3.140986533465766; accuracy - 0.46985157699443414\n",
      "Epoch 393: loss - 3.14014354829222; accuracy - 0.4700834879406308\n",
      "Epoch 394: loss - 3.139304584496981; accuracy - 0.46985157699443414\n",
      "Epoch 395: loss - 3.1384780979775764; accuracy - 0.4700834879406308\n",
      "Epoch 396: loss - 3.1376538345234293; accuracy - 0.4700834879406308\n",
      "Epoch 397: loss - 3.1368409628549623; accuracy - 0.4700834879406308\n",
      "Epoch 398: loss - 3.1360366170835405; accuracy - 0.4700834879406308\n",
      "Epoch 399: loss - 3.1352332046390243; accuracy - 0.46961966604823746\n",
      "Epoch 400: loss - 3.1344435033638978; accuracy - 0.46961966604823746\n",
      "Epoch 401: loss - 3.1336576626782957; accuracy - 0.46961966604823746\n",
      "Epoch 402: loss - 3.132877890608085; accuracy - 0.46961966604823746\n",
      "Epoch 403: loss - 3.13210549900837; accuracy - 0.46961966604823746\n",
      "Epoch 404: loss - 3.131338660040238; accuracy - 0.46961966604823746\n",
      "Epoch 405: loss - 3.1305797891846834; accuracy - 0.46961966604823746\n",
      "Epoch 406: loss - 3.1298312674205686; accuracy - 0.46961966604823746\n",
      "Epoch 407: loss - 3.129082921374043; accuracy - 0.46961966604823746\n",
      "Epoch 408: loss - 3.128347613373581; accuracy - 0.46961966604823746\n",
      "Epoch 409: loss - 3.1276138454730966; accuracy - 0.46961966604823746\n",
      "Epoch 410: loss - 3.1268882191911036; accuracy - 0.46938775510204084\n",
      "Epoch 411: loss - 3.1261668961654125; accuracy - 0.46938775510204084\n",
      "Epoch 412: loss - 3.1254559488818465; accuracy - 0.46938775510204084\n",
      "Epoch 413: loss - 3.124746856198461; accuracy - 0.4710111317254174\n",
      "Epoch 414: loss - 3.1240459247068926; accuracy - 0.4710111317254174\n",
      "Epoch 415: loss - 3.123349290610687; accuracy - 0.4710111317254174\n",
      "Epoch 416: loss - 3.1226612440066788; accuracy - 0.4707792207792208\n",
      "Epoch 417: loss - 3.1219735038302603; accuracy - 0.4707792207792208\n",
      "Epoch 418: loss - 3.121298583849905; accuracy - 0.4707792207792208\n",
      "Epoch 419: loss - 3.1206253926254157; accuracy - 0.4707792207792208\n",
      "Epoch 420: loss - 3.1199607272979724; accuracy - 0.4707792207792208\n",
      "Epoch 421: loss - 3.119297794154491; accuracy - 0.4707792207792208\n",
      "Epoch 422: loss - 3.1186452580957997; accuracy - 0.4707792207792208\n",
      "Epoch 423: loss - 3.117993762519674; accuracy - 0.4707792207792208\n",
      "Epoch 424: loss - 3.117346920639774; accuracy - 0.4707792207792208\n",
      "Epoch 425: loss - 3.116711144212889; accuracy - 0.4707792207792208\n",
      "Epoch 426: loss - 3.1160775351170487; accuracy - 0.4710111317254174\n",
      "Epoch 427: loss - 3.115446723458497; accuracy - 0.4710111317254174\n",
      "Epoch 428: loss - 3.1148267362909547; accuracy - 0.4710111317254174\n",
      "Epoch 429: loss - 3.114209006801386; accuracy - 0.4710111317254174\n",
      "Epoch 430: loss - 3.113595714153297; accuracy - 0.4710111317254174\n",
      "Epoch 431: loss - 3.1129892540331894; accuracy - 0.4710111317254174\n",
      "Epoch 432: loss - 3.1123877136738267; accuracy - 0.4710111317254174\n",
      "Epoch 433: loss - 3.1117883635363466; accuracy - 0.4710111317254174\n",
      "Epoch 434: loss - 3.1111978818843893; accuracy - 0.4710111317254174\n",
      "Epoch 435: loss - 3.110607938222407; accuracy - 0.4710111317254174\n",
      "Epoch 436: loss - 3.1100249846702606; accuracy - 0.4710111317254174\n",
      "Epoch 437: loss - 3.109447559089519; accuracy - 0.4710111317254174\n",
      "Epoch 438: loss - 3.108873829439089; accuracy - 0.4710111317254174\n",
      "Epoch 439: loss - 3.108305601662298; accuracy - 0.4710111317254174\n",
      "Epoch 440: loss - 3.1077409705116046; accuracy - 0.4710111317254174\n",
      "Epoch 441: loss - 3.107179279119495; accuracy - 0.4710111317254174\n",
      "Epoch 442: loss - 3.1066281087765666; accuracy - 0.4710111317254174\n",
      "Epoch 443: loss - 3.106077281022116; accuracy - 0.4710111317254174\n",
      "Epoch 444: loss - 3.105530390159982; accuracy - 0.4710111317254174\n",
      "Epoch 445: loss - 3.104988348174405; accuracy - 0.4710111317254174\n",
      "Epoch 446: loss - 3.104451015287516; accuracy - 0.4710111317254174\n",
      "Epoch 447: loss - 3.1039198050914756; accuracy - 0.4710111317254174\n",
      "Epoch 448: loss - 3.1033891112109946; accuracy - 0.4712430426716141\n",
      "Epoch 449: loss - 3.1028622773671195; accuracy - 0.4714749536178108\n",
      "Epoch 450: loss - 3.1023456234215363; accuracy - 0.4714749536178108\n",
      "Epoch 451: loss - 3.1018279718998856; accuracy - 0.4714749536178108\n",
      "Epoch 452: loss - 3.101317883311928; accuracy - 0.4714749536178108\n",
      "Epoch 453: loss - 3.100806421052547; accuracy - 0.4714749536178108\n",
      "Epoch 454: loss - 3.1003046298955943; accuracy - 0.4714749536178108\n",
      "Epoch 455: loss - 3.0998052200492547; accuracy - 0.4714749536178108\n",
      "Epoch 456: loss - 3.0993101561445475; accuracy - 0.4717068645640074\n",
      "Epoch 457: loss - 3.098812606869912; accuracy - 0.4717068645640074\n",
      "Epoch 458: loss - 3.0983284050546906; accuracy - 0.4717068645640074\n",
      "Epoch 459: loss - 3.097843849150281; accuracy - 0.4717068645640074\n",
      "Epoch 460: loss - 3.0973645773800937; accuracy - 0.4717068645640074\n",
      "Epoch 461: loss - 3.096885940360669; accuracy - 0.4719387755102041\n",
      "Epoch 462: loss - 3.0964118121727497; accuracy - 0.4719387755102041\n",
      "Epoch 463: loss - 3.095944553447787; accuracy - 0.4719387755102041\n",
      "Epoch 464: loss - 3.095479883488565; accuracy - 0.47217068645640076\n",
      "Epoch 465: loss - 3.0950148748112962; accuracy - 0.47217068645640076\n",
      "Epoch 466: loss - 3.0945598051808982; accuracy - 0.47217068645640076\n",
      "Epoch 467: loss - 3.0941023620913333; accuracy - 0.47217068645640076\n",
      "Epoch 468: loss - 3.0936501195346713; accuracy - 0.47217068645640076\n",
      "Epoch 469: loss - 3.093200773498344; accuracy - 0.4724025974025974\n",
      "Epoch 470: loss - 3.0927572334409867; accuracy - 0.4724025974025974\n",
      "Epoch 471: loss - 3.092316043177872; accuracy - 0.4724025974025974\n",
      "Epoch 472: loss - 3.0918784646908293; accuracy - 0.4724025974025974\n",
      "Epoch 473: loss - 3.09144026074206; accuracy - 0.4724025974025974\n",
      "Epoch 474: loss - 3.0910151279038978; accuracy - 0.4724025974025974\n",
      "Epoch 475: loss - 3.0905818398454414; accuracy - 0.4724025974025974\n",
      "Epoch 476: loss - 3.0901575192449706; accuracy - 0.4724025974025974\n",
      "Epoch 477: loss - 3.0897340169642984; accuracy - 0.4724025974025974\n",
      "Epoch 478: loss - 3.089318054284147; accuracy - 0.4724025974025974\n",
      "Epoch 479: loss - 3.0889037614187194; accuracy - 0.4724025974025974\n",
      "Epoch 480: loss - 3.088488765793519; accuracy - 0.4724025974025974\n",
      "Epoch 481: loss - 3.088079351994014; accuracy - 0.4724025974025974\n",
      "Epoch 482: loss - 3.0876735772847685; accuracy - 0.4724025974025974\n",
      "Epoch 483: loss - 3.0872704207565436; accuracy - 0.4724025974025974\n",
      "Epoch 484: loss - 3.0868704222792376; accuracy - 0.4724025974025974\n",
      "Epoch 485: loss - 3.086470999611552; accuracy - 0.4724025974025974\n",
      "Epoch 486: loss - 3.086081460477691; accuracy - 0.4724025974025974\n",
      "Epoch 487: loss - 3.0856898540910853; accuracy - 0.4724025974025974\n",
      "Epoch 488: loss - 3.08530056211192; accuracy - 0.4724025974025974\n",
      "Epoch 489: loss - 3.0849151506273556; accuracy - 0.4724025974025974\n",
      "Epoch 490: loss - 3.084532902391147; accuracy - 0.4724025974025974\n",
      "Epoch 491: loss - 3.084152154113012; accuracy - 0.4724025974025974\n",
      "Epoch 492: loss - 3.083775400120164; accuracy - 0.4724025974025974\n",
      "Epoch 493: loss - 3.083401599377117; accuracy - 0.4724025974025974\n",
      "Epoch 494: loss - 3.0830275617636644; accuracy - 0.4724025974025974\n",
      "Epoch 495: loss - 3.082661685342028; accuracy - 0.4724025974025974\n",
      "Epoch 496: loss - 3.082293684717012; accuracy - 0.4724025974025974\n",
      "Epoch 497: loss - 3.0819323322768555; accuracy - 0.4724025974025974\n",
      "Epoch 498: loss - 3.081569887158601; accuracy - 0.4724025974025974\n",
      "Epoch 499: loss - 3.0812106585900727; accuracy - 0.4724025974025974\n",
      "Epoch 500: loss - 3.080852629634135; accuracy - 0.4724025974025974\n",
      "Epoch 501: loss - 3.080501490156814; accuracy - 0.4724025974025974\n",
      "Epoch 502: loss - 3.080156682705393; accuracy - 0.47263450834879406\n",
      "Epoch 503: loss - 3.0798067740252817; accuracy - 0.47263450834879406\n",
      "Epoch 504: loss - 3.079460987133529; accuracy - 0.47263450834879406\n",
      "Epoch 505: loss - 3.079114813419795; accuracy - 0.47263450834879406\n",
      "Epoch 506: loss - 3.078777622977524; accuracy - 0.47263450834879406\n",
      "Epoch 507: loss - 3.07843616134824; accuracy - 0.47263450834879406\n",
      "Epoch 508: loss - 3.0781041030981102; accuracy - 0.47263450834879406\n",
      "Epoch 509: loss - 3.077766227921219; accuracy - 0.47263450834879406\n",
      "Epoch 510: loss - 3.077436481203352; accuracy - 0.47263450834879406\n",
      "Epoch 511: loss - 3.0771115869007217; accuracy - 0.47263450834879406\n",
      "Epoch 512: loss - 3.0767850133837484; accuracy - 0.47263450834879406\n",
      "Epoch 513: loss - 3.0764575926846165; accuracy - 0.47263450834879406\n",
      "Epoch 514: loss - 3.076137290925396; accuracy - 0.47263450834879406\n",
      "Epoch 515: loss - 3.0758144729433785; accuracy - 0.47263450834879406\n",
      "Epoch 516: loss - 3.0755017489360745; accuracy - 0.47263450834879406\n",
      "Epoch 517: loss - 3.07518412971762; accuracy - 0.47263450834879406\n",
      "Epoch 518: loss - 3.0748712033420413; accuracy - 0.47263450834879406\n",
      "Epoch 519: loss - 3.07456023064969; accuracy - 0.47263450834879406\n",
      "Epoch 520: loss - 3.0742533066498328; accuracy - 0.47263450834879406\n",
      "Epoch 521: loss - 3.0739453865114967; accuracy - 0.4724025974025974\n",
      "Epoch 522: loss - 3.0736409175816184; accuracy - 0.47263450834879406\n",
      "Epoch 523: loss - 3.0733388007682416; accuracy - 0.47263450834879406\n",
      "Epoch 524: loss - 3.073041376576574; accuracy - 0.47263450834879406\n",
      "Epoch 525: loss - 3.0727391262886035; accuracy - 0.47263450834879406\n",
      "Epoch 526: loss - 3.072445398358998; accuracy - 0.47263450834879406\n",
      "Epoch 527: loss - 3.072150156648349; accuracy - 0.4724025974025974\n",
      "Epoch 528: loss - 3.0718557843471945; accuracy - 0.4724025974025974\n",
      "Epoch 529: loss - 3.0715685915858493; accuracy - 0.4724025974025974\n",
      "Epoch 530: loss - 3.071280766617168; accuracy - 0.4724025974025974\n",
      "Epoch 531: loss - 3.0709941460161794; accuracy - 0.4724025974025974\n",
      "Epoch 532: loss - 3.0707103896450687; accuracy - 0.4724025974025974\n",
      "Epoch 533: loss - 3.0704287788200024; accuracy - 0.4724025974025974\n",
      "Epoch 534: loss - 3.070147873408719; accuracy - 0.4724025974025974\n",
      "Epoch 535: loss - 3.0698667434018265; accuracy - 0.4724025974025974\n",
      "Epoch 536: loss - 3.0695903786701706; accuracy - 0.4724025974025974\n",
      "Epoch 537: loss - 3.069316388503519; accuracy - 0.4724025974025974\n",
      "Epoch 538: loss - 3.0690420350692267; accuracy - 0.4724025974025974\n",
      "Epoch 539: loss - 3.0687748371780694; accuracy - 0.4724025974025974\n",
      "Epoch 540: loss - 3.068505167408202; accuracy - 0.4724025974025974\n",
      "Epoch 541: loss - 3.0682355752681314; accuracy - 0.4724025974025974\n",
      "Epoch 542: loss - 3.0679679897809073; accuracy - 0.4724025974025974\n",
      "Epoch 543: loss - 3.0677082035448646; accuracy - 0.4724025974025974\n",
      "Epoch 544: loss - 3.0674461965658226; accuracy - 0.4724025974025974\n",
      "Epoch 545: loss - 3.067183286449241; accuracy - 0.4724025974025974\n",
      "Epoch 546: loss - 3.0669251553202828; accuracy - 0.4724025974025974\n",
      "Epoch 547: loss - 3.066666488081272; accuracy - 0.4724025974025974\n",
      "Epoch 548: loss - 3.0664098488377727; accuracy - 0.4724025974025974\n",
      "Epoch 549: loss - 3.066158919254791; accuracy - 0.4724025974025974\n",
      "Epoch 550: loss - 3.065904910467109; accuracy - 0.4724025974025974\n",
      "Epoch 551: loss - 3.065655512247988; accuracy - 0.4724025974025974\n",
      "Epoch 552: loss - 3.0654046463612503; accuracy - 0.4724025974025974\n",
      "Epoch 553: loss - 3.0651578835742153; accuracy - 0.4724025974025974\n",
      "Epoch 554: loss - 3.0649138800730733; accuracy - 0.4724025974025974\n",
      "Epoch 555: loss - 3.0646695911552557; accuracy - 0.4724025974025974\n",
      "Epoch 556: loss - 3.064427713295082; accuracy - 0.4724025974025974\n",
      "Epoch 557: loss - 3.064186167960264; accuracy - 0.4724025974025974\n",
      "Epoch 558: loss - 3.06394582290154; accuracy - 0.4724025974025974\n",
      "Epoch 559: loss - 3.0637099982856157; accuracy - 0.4724025974025974\n",
      "Epoch 560: loss - 3.063474161394894; accuracy - 0.4724025974025974\n",
      "Epoch 561: loss - 3.0632372944164805; accuracy - 0.4724025974025974\n",
      "Epoch 562: loss - 3.0630046040759678; accuracy - 0.4724025974025974\n",
      "Epoch 563: loss - 3.0627703541727365; accuracy - 0.4724025974025974\n",
      "Epoch 564: loss - 3.0625435415800517; accuracy - 0.4724025974025974\n",
      "Epoch 565: loss - 3.062311532877814; accuracy - 0.4724025974025974\n",
      "Epoch 566: loss - 3.062086412327188; accuracy - 0.4724025974025974\n",
      "Epoch 567: loss - 3.0618573817560977; accuracy - 0.4724025974025974\n",
      "Epoch 568: loss - 3.0616330438067165; accuracy - 0.4724025974025974\n",
      "Epoch 569: loss - 3.061410312639318; accuracy - 0.47217068645640076\n",
      "Epoch 570: loss - 3.061188092590042; accuracy - 0.47217068645640076\n",
      "Epoch 571: loss - 3.060970273553109; accuracy - 0.47217068645640076\n",
      "Epoch 572: loss - 3.060752179936708; accuracy - 0.47217068645640076\n",
      "Epoch 573: loss - 3.0605323684016494; accuracy - 0.47217068645640076\n",
      "Epoch 574: loss - 3.0603190177225668; accuracy - 0.4719387755102041\n",
      "Epoch 575: loss - 3.0601016743063707; accuracy - 0.4719387755102041\n",
      "Epoch 576: loss - 3.0598857378473086; accuracy - 0.4719387755102041\n",
      "Epoch 577: loss - 3.0596730511131005; accuracy - 0.47217068645640076\n",
      "Epoch 578: loss - 3.059462277035545; accuracy - 0.47217068645640076\n",
      "Epoch 579: loss - 3.0592552852542148; accuracy - 0.47217068645640076\n",
      "Epoch 580: loss - 3.059044705914655; accuracy - 0.47217068645640076\n",
      "Epoch 581: loss - 3.0588395699499267; accuracy - 0.47217068645640076\n",
      "Epoch 582: loss - 3.0586317059945087; accuracy - 0.47217068645640076\n",
      "Epoch 583: loss - 3.0584257347906676; accuracy - 0.47217068645640076\n",
      "Epoch 584: loss - 3.0582231378289895; accuracy - 0.47217068645640076\n",
      "Epoch 585: loss - 3.058022469337442; accuracy - 0.47217068645640076\n",
      "Epoch 586: loss - 3.0578215691729245; accuracy - 0.47217068645640076\n",
      "Epoch 587: loss - 3.0576212288939666; accuracy - 0.47217068645640076\n",
      "Epoch 588: loss - 3.0574225179560774; accuracy - 0.47217068645640076\n",
      "Epoch 589: loss - 3.0572234925179846; accuracy - 0.47217068645640076\n",
      "Epoch 590: loss - 3.0570289955953056; accuracy - 0.47217068645640076\n",
      "Epoch 591: loss - 3.05683296918869; accuracy - 0.4724025974025974\n",
      "Epoch 592: loss - 3.056639789318552; accuracy - 0.4724025974025974\n",
      "Epoch 593: loss - 3.05644682718783; accuracy - 0.4724025974025974\n",
      "Epoch 594: loss - 3.0562551848742427; accuracy - 0.4724025974025974\n",
      "Epoch 595: loss - 3.0560636772516707; accuracy - 0.4724025974025974\n",
      "Epoch 596: loss - 3.055873133808872; accuracy - 0.4724025974025974\n",
      "Epoch 597: loss - 3.0556871142369695; accuracy - 0.4724025974025974\n",
      "Epoch 598: loss - 3.055499017238617; accuracy - 0.4724025974025974\n",
      "Epoch 599: loss - 3.0553137637909797; accuracy - 0.4724025974025974\n",
      "Epoch 600: loss - 3.0551262585263084; accuracy - 0.4724025974025974\n",
      "Epoch 601: loss - 3.0549449648618254; accuracy - 0.4724025974025974\n",
      "Epoch 602: loss - 3.0547595845746196; accuracy - 0.4724025974025974\n",
      "Epoch 603: loss - 3.054580656407273; accuracy - 0.4724025974025974\n",
      "Epoch 604: loss - 3.0543984325122304; accuracy - 0.4724025974025974\n",
      "Epoch 605: loss - 3.0542188356449076; accuracy - 0.4724025974025974\n",
      "Epoch 606: loss - 3.0540388875635744; accuracy - 0.4724025974025974\n",
      "Epoch 607: loss - 3.053862869850116; accuracy - 0.4724025974025974\n",
      "Epoch 608: loss - 3.053684208189623; accuracy - 0.4724025974025974\n",
      "Epoch 609: loss - 3.0535105464630976; accuracy - 0.4724025974025974\n",
      "Epoch 610: loss - 3.053334907277838; accuracy - 0.4724025974025974\n",
      "Epoch 611: loss - 3.0531639442373075; accuracy - 0.4724025974025974\n",
      "Epoch 612: loss - 3.0529917185514446; accuracy - 0.4724025974025974\n",
      "Epoch 613: loss - 3.0528181135543866; accuracy - 0.4724025974025974\n",
      "Epoch 614: loss - 3.052645621029919; accuracy - 0.4724025974025974\n",
      "Epoch 615: loss - 3.0524819928993763; accuracy - 0.4724025974025974\n",
      "Epoch 616: loss - 3.052311889536969; accuracy - 0.4724025974025974\n",
      "Epoch 617: loss - 3.052142034988014; accuracy - 0.4724025974025974\n",
      "Epoch 618: loss - 3.0519781610297803; accuracy - 0.4724025974025974\n",
      "Epoch 619: loss - 3.051811266255069; accuracy - 0.4724025974025974\n",
      "Epoch 620: loss - 3.0516452828010956; accuracy - 0.4724025974025974\n",
      "Epoch 621: loss - 3.0514811937574553; accuracy - 0.4724025974025974\n",
      "Epoch 622: loss - 3.051319528046256; accuracy - 0.4724025974025974\n",
      "Epoch 623: loss - 3.051156027529368; accuracy - 0.4724025974025974\n",
      "Epoch 624: loss - 3.050995518856013; accuracy - 0.4724025974025974\n",
      "Epoch 625: loss - 3.050835302013192; accuracy - 0.4724025974025974\n",
      "Epoch 626: loss - 3.0506760564275046; accuracy - 0.4724025974025974\n",
      "Epoch 627: loss - 3.050516752232426; accuracy - 0.4724025974025974\n",
      "Epoch 628: loss - 3.050364719251091; accuracy - 0.4724025974025974\n",
      "Epoch 629: loss - 3.0502036640728116; accuracy - 0.4724025974025974\n",
      "Epoch 630: loss - 3.0500473144545404; accuracy - 0.4724025974025974\n",
      "Epoch 631: loss - 3.0498915642002298; accuracy - 0.4724025974025974\n",
      "Epoch 632: loss - 3.049738318557421; accuracy - 0.4724025974025974\n",
      "Epoch 633: loss - 3.0495869675461127; accuracy - 0.4724025974025974\n",
      "Epoch 634: loss - 3.04943239091721; accuracy - 0.4724025974025974\n",
      "Epoch 635: loss - 3.049284981663904; accuracy - 0.4724025974025974\n",
      "Epoch 636: loss - 3.0491326565202845; accuracy - 0.47217068645640076\n",
      "Epoch 637: loss - 3.0489814020485957; accuracy - 0.47217068645640076\n",
      "Epoch 638: loss - 3.0488317822259963; accuracy - 0.47217068645640076\n",
      "Epoch 639: loss - 3.0486823764935496; accuracy - 0.47217068645640076\n",
      "Epoch 640: loss - 3.0485338043414596; accuracy - 0.47217068645640076\n",
      "Epoch 641: loss - 3.0483844593194953; accuracy - 0.47217068645640076\n",
      "Epoch 642: loss - 3.048242161800335; accuracy - 0.47217068645640076\n",
      "Epoch 643: loss - 3.048100784116862; accuracy - 0.47217068645640076\n",
      "Epoch 644: loss - 3.0479478487720737; accuracy - 0.47217068645640076\n",
      "Epoch 645: loss - 3.047808499415863; accuracy - 0.47217068645640076\n",
      "Epoch 646: loss - 3.047665126137919; accuracy - 0.47217068645640076\n",
      "Epoch 647: loss - 3.0475232573519833; accuracy - 0.47217068645640076\n",
      "Epoch 648: loss - 3.0473831225193497; accuracy - 0.47217068645640076\n",
      "Epoch 649: loss - 3.047242944669635; accuracy - 0.47217068645640076\n",
      "Epoch 650: loss - 3.047098072207704; accuracy - 0.47217068645640076\n",
      "Epoch 651: loss - 3.0469614267349243; accuracy - 0.47217068645640076\n",
      "Epoch 652: loss - 3.0468238683932345; accuracy - 0.47217068645640076\n",
      "Epoch 653: loss - 3.0466857847787012; accuracy - 0.47217068645640076\n",
      "Epoch 654: loss - 3.0465489907919365; accuracy - 0.47217068645640076\n",
      "Epoch 655: loss - 3.046411355926287; accuracy - 0.47217068645640076\n",
      "Epoch 656: loss - 3.0462775293440454; accuracy - 0.47217068645640076\n",
      "Epoch 657: loss - 3.0461434087195953; accuracy - 0.47217068645640076\n",
      "Epoch 658: loss - 3.046008542318291; accuracy - 0.47217068645640076\n",
      "Epoch 659: loss - 3.045878410118188; accuracy - 0.47217068645640076\n",
      "Epoch 660: loss - 3.0457436673495235; accuracy - 0.47217068645640076\n",
      "Epoch 661: loss - 3.0456114111672083; accuracy - 0.47217068645640076\n",
      "Epoch 662: loss - 3.0454778980899166; accuracy - 0.47217068645640076\n",
      "Epoch 663: loss - 3.04535165478878; accuracy - 0.47217068645640076\n",
      "Epoch 664: loss - 3.045219142273317; accuracy - 0.47217068645640076\n",
      "Epoch 665: loss - 3.0450872547772465; accuracy - 0.47379406307977734\n",
      "Epoch 666: loss - 3.0449599550694835; accuracy - 0.47217068645640076\n",
      "Epoch 667: loss - 3.0448327086630913; accuracy - 0.47379406307977734\n",
      "Epoch 668: loss - 3.044704996035581; accuracy - 0.47217068645640076\n",
      "Epoch 669: loss - 3.0445762009036783; accuracy - 0.47217068645640076\n",
      "Epoch 670: loss - 3.0444523782809725; accuracy - 0.47379406307977734\n",
      "Epoch 671: loss - 3.044328126925042; accuracy - 0.47379406307977734\n",
      "Epoch 672: loss - 3.0442020675246924; accuracy - 0.47379406307977734\n",
      "Epoch 673: loss - 3.0440781307795493; accuracy - 0.47379406307977734\n",
      "Epoch 674: loss - 3.0439535545285423; accuracy - 0.47379406307977734\n",
      "Epoch 675: loss - 3.0438311980252806; accuracy - 0.47379406307977734\n",
      "Epoch 676: loss - 3.0437083655695303; accuracy - 0.47379406307977734\n",
      "Epoch 677: loss - 3.0435863797430205; accuracy - 0.47379406307977734\n",
      "Epoch 678: loss - 3.0434676551420745; accuracy - 0.47379406307977734\n",
      "Epoch 679: loss - 3.0433471697159735; accuracy - 0.47379406307977734\n",
      "Epoch 680: loss - 3.043226579456471; accuracy - 0.47379406307977734\n",
      "Epoch 681: loss - 3.0431061985320196; accuracy - 0.47379406307977734\n",
      "Epoch 682: loss - 3.0429869067469864; accuracy - 0.47379406307977734\n",
      "Epoch 683: loss - 3.042867711280405; accuracy - 0.47379406307977734\n",
      "Epoch 684: loss - 3.042749784873014; accuracy - 0.47379406307977734\n",
      "Epoch 685: loss - 3.042635389626137; accuracy - 0.47379406307977734\n",
      "Epoch 686: loss - 3.0425187901132404; accuracy - 0.47379406307977734\n",
      "Epoch 687: loss - 3.0424032434682897; accuracy - 0.47379406307977734\n",
      "Epoch 688: loss - 3.042287974609736; accuracy - 0.47379406307977734\n",
      "Epoch 689: loss - 3.0421722495931864; accuracy - 0.47379406307977734\n",
      "Epoch 690: loss - 3.042062781294998; accuracy - 0.47379406307977734\n",
      "Epoch 691: loss - 3.0419435922422746; accuracy - 0.47379406307977734\n",
      "Epoch 692: loss - 3.04183013585149; accuracy - 0.47379406307977734\n",
      "Epoch 693: loss - 3.041718216243171; accuracy - 0.47379406307977734\n",
      "Epoch 694: loss - 3.041607771048077; accuracy - 0.47379406307977734\n",
      "Epoch 695: loss - 3.0414947791090703; accuracy - 0.47379406307977734\n",
      "Epoch 696: loss - 3.0413827295648366; accuracy - 0.47379406307977734\n",
      "Epoch 697: loss - 3.041272230183702; accuracy - 0.47379406307977734\n",
      "Epoch 698: loss - 3.0411636650230536; accuracy - 0.47379406307977734\n",
      "Epoch 699: loss - 3.0410562772476606; accuracy - 0.47379406307977734\n",
      "Epoch 700: loss - 3.040944095998173; accuracy - 0.47379406307977734\n",
      "Epoch 701: loss - 3.0408382247684176; accuracy - 0.47379406307977734\n",
      "Epoch 702: loss - 3.040730128151145; accuracy - 0.47379406307977734\n",
      "Epoch 703: loss - 3.0406221157986955; accuracy - 0.47379406307977734\n",
      "Epoch 704: loss - 3.040515923765462; accuracy - 0.47379406307977734\n",
      "Epoch 705: loss - 3.0404114298563942; accuracy - 0.47379406307977734\n",
      "Epoch 706: loss - 3.0403047552356472; accuracy - 0.47379406307977734\n",
      "Epoch 707: loss - 3.040200173854828; accuracy - 0.47379406307977734\n",
      "Epoch 708: loss - 3.0400966090704826; accuracy - 0.47379406307977734\n",
      "Epoch 709: loss - 3.0399898404535426; accuracy - 0.47379406307977734\n",
      "Epoch 710: loss - 3.039886766661076; accuracy - 0.47379406307977734\n",
      "Epoch 711: loss - 3.03978207558117; accuracy - 0.47379406307977734\n",
      "Epoch 712: loss - 3.0396767788340298; accuracy - 0.47379406307977734\n",
      "Epoch 713: loss - 3.0395802602254838; accuracy - 0.47379406307977734\n",
      "Epoch 714: loss - 3.039476639706925; accuracy - 0.47379406307977734\n",
      "Epoch 715: loss - 3.039374814736821; accuracy - 0.47379406307977734\n",
      "Epoch 716: loss - 3.0392720813211573; accuracy - 0.47379406307977734\n",
      "Epoch 717: loss - 3.039167729291049; accuracy - 0.47379406307977734\n",
      "Epoch 718: loss - 3.0390725275138757; accuracy - 0.47379406307977734\n",
      "Epoch 719: loss - 3.038972414269739; accuracy - 0.47379406307977734\n",
      "Epoch 720: loss - 3.0388724457797403; accuracy - 0.47379406307977734\n",
      "Epoch 721: loss - 3.0387769665479216; accuracy - 0.47379406307977734\n",
      "Epoch 722: loss - 3.0386749927320817; accuracy - 0.47379406307977734\n",
      "Epoch 723: loss - 3.0385794311153647; accuracy - 0.47379406307977734\n",
      "Epoch 724: loss - 3.0384809015412055; accuracy - 0.47379406307977734\n",
      "Epoch 725: loss - 3.0383826654563366; accuracy - 0.47379406307977734\n",
      "Epoch 726: loss - 3.0382905985074937; accuracy - 0.47379406307977734\n",
      "Epoch 727: loss - 3.038191794906786; accuracy - 0.47379406307977734\n",
      "Epoch 728: loss - 3.038096459433851; accuracy - 0.47379406307977734\n",
      "Epoch 729: loss - 3.038001287182541; accuracy - 0.47379406307977734\n",
      "Epoch 730: loss - 3.0379081634953202; accuracy - 0.47379406307977734\n",
      "Epoch 731: loss - 3.0378128973584007; accuracy - 0.47379406307977734\n",
      "Epoch 732: loss - 3.037719004892905; accuracy - 0.47379406307977734\n",
      "Epoch 733: loss - 3.037624523237154; accuracy - 0.47379406307977734\n",
      "Epoch 734: loss - 3.0375304653383584; accuracy - 0.47379406307977734\n",
      "Epoch 735: loss - 3.037436501325172; accuracy - 0.47379406307977734\n",
      "Epoch 736: loss - 3.0373448726197565; accuracy - 0.47379406307977734\n",
      "Epoch 737: loss - 3.037255699647822; accuracy - 0.47379406307977734\n",
      "Epoch 738: loss - 3.037160863792299; accuracy - 0.47379406307977734\n",
      "Epoch 739: loss - 3.0370708715053127; accuracy - 0.47379406307977734\n",
      "Epoch 740: loss - 3.0369814717260937; accuracy - 0.47379406307977734\n",
      "Epoch 741: loss - 3.0368885185590258; accuracy - 0.47379406307977734\n",
      "Epoch 742: loss - 3.036799475301838; accuracy - 0.47379406307977734\n",
      "Epoch 743: loss - 3.0367106803051835; accuracy - 0.47379406307977734\n",
      "Epoch 744: loss - 3.036621932638377; accuracy - 0.47379406307977734\n",
      "Epoch 745: loss - 3.036532813631316; accuracy - 0.47379406307977734\n",
      "Epoch 746: loss - 3.0364444434974542; accuracy - 0.47379406307977734\n",
      "Epoch 747: loss - 3.0363545726314327; accuracy - 0.47379406307977734\n",
      "Epoch 748: loss - 3.036266605796531; accuracy - 0.47379406307977734\n",
      "Epoch 749: loss - 3.036182614337093; accuracy - 0.47379406307977734\n",
      "Epoch 750: loss - 3.036093341729194; accuracy - 0.47379406307977734\n",
      "Epoch 751: loss - 3.03600786877917; accuracy - 0.47379406307977734\n",
      "Epoch 752: loss - 3.035924120051078; accuracy - 0.47379406307977734\n",
      "Epoch 753: loss - 3.035834238790183; accuracy - 0.47379406307977734\n",
      "Epoch 754: loss - 3.0357525818644295; accuracy - 0.47379406307977734\n",
      "Epoch 755: loss - 3.0356655403944024; accuracy - 0.47379406307977734\n",
      "Epoch 756: loss - 3.035579703844987; accuracy - 0.47379406307977734\n",
      "Epoch 757: loss - 3.0354962735981137; accuracy - 0.47379406307977734\n",
      "Epoch 758: loss - 3.0354132926530433; accuracy - 0.47379406307977734\n",
      "Epoch 759: loss - 3.0353291256971837; accuracy - 0.47379406307977734\n",
      "Epoch 760: loss - 3.0352417210701; accuracy - 0.47379406307977734\n",
      "Epoch 761: loss - 3.0351640955857753; accuracy - 0.47379406307977734\n",
      "Epoch 762: loss - 3.0350794252526563; accuracy - 0.47379406307977734\n",
      "Epoch 763: loss - 3.0349978855456845; accuracy - 0.47379406307977734\n",
      "Epoch 764: loss - 3.0349177760184363; accuracy - 0.47379406307977734\n",
      "Epoch 765: loss - 3.0348341299120705; accuracy - 0.47379406307977734\n",
      "Epoch 766: loss - 3.0347522731614687; accuracy - 0.47379406307977734\n",
      "Epoch 767: loss - 3.034674277885062; accuracy - 0.47379406307977734\n",
      "Epoch 768: loss - 3.034589791010395; accuracy - 0.47379406307977734\n",
      "Epoch 769: loss - 3.0345113706500277; accuracy - 0.47379406307977734\n",
      "Epoch 770: loss - 3.034430122663006; accuracy - 0.4735621521335807\n",
      "Epoch 771: loss - 3.0343500870057074; accuracy - 0.4735621521335807\n",
      "Epoch 772: loss - 3.034272771045317; accuracy - 0.4735621521335807\n",
      "Epoch 773: loss - 3.034194588661194; accuracy - 0.4735621521335807\n",
      "Epoch 774: loss - 3.0341136946527767; accuracy - 0.4735621521335807\n",
      "Epoch 775: loss - 3.034035459520202; accuracy - 0.4735621521335807\n",
      "Epoch 776: loss - 3.0339584503191523; accuracy - 0.4735621521335807\n",
      "Epoch 777: loss - 3.03387880491193; accuracy - 0.47379406307977734\n",
      "Epoch 778: loss - 3.033803416537; accuracy - 0.47379406307977734\n",
      "Epoch 779: loss - 3.0337242383461493; accuracy - 0.47379406307977734\n",
      "Epoch 780: loss - 3.0336481264217; accuracy - 0.47379406307977734\n",
      "Epoch 781: loss - 3.03357231738616; accuracy - 0.4735621521335807\n",
      "Epoch 782: loss - 3.033496311622117; accuracy - 0.47379406307977734\n",
      "Epoch 783: loss - 3.033420091104419; accuracy - 0.47379406307977734\n",
      "Epoch 784: loss - 3.0333431617448414; accuracy - 0.4735621521335807\n",
      "Epoch 785: loss - 3.0332684059089985; accuracy - 0.47379406307977734\n",
      "Epoch 786: loss - 3.0331928386095504; accuracy - 0.4735621521335807\n",
      "Epoch 787: loss - 3.0331181141794943; accuracy - 0.4735621521335807\n",
      "Epoch 788: loss - 3.033044561384339; accuracy - 0.4735621521335807\n",
      "Epoch 789: loss - 3.032974897156399; accuracy - 0.4735621521335807\n",
      "Epoch 790: loss - 3.032895782440624; accuracy - 0.4735621521335807\n",
      "Epoch 791: loss - 3.0328242174990767; accuracy - 0.4735621521335807\n",
      "Epoch 792: loss - 3.0327480955115056; accuracy - 0.4735621521335807\n",
      "Epoch 793: loss - 3.032677858902041; accuracy - 0.4735621521335807\n",
      "Epoch 794: loss - 3.032604995596608; accuracy - 0.4735621521335807\n",
      "Epoch 795: loss - 3.0325323234904897; accuracy - 0.4735621521335807\n",
      "Epoch 796: loss - 3.0324565705209143; accuracy - 0.4735621521335807\n",
      "Epoch 797: loss - 3.0323882016710977; accuracy - 0.4735621521335807\n",
      "Epoch 798: loss - 3.032313220244392; accuracy - 0.47379406307977734\n",
      "Epoch 799: loss - 3.032246240658309; accuracy - 0.47379406307977734\n",
      "Epoch 800: loss - 3.0321703684794437; accuracy - 0.47379406307977734\n",
      "Epoch 801: loss - 3.0321045408443528; accuracy - 0.47379406307977734\n",
      "Epoch 802: loss - 3.032032451735799; accuracy - 0.47379406307977734\n",
      "Epoch 803: loss - 3.031959757844681; accuracy - 0.47379406307977734\n",
      "Epoch 804: loss - 3.031888871776814; accuracy - 0.47379406307977734\n",
      "Epoch 805: loss - 3.031820141539282; accuracy - 0.47379406307977734\n",
      "Epoch 806: loss - 3.0317490527713895; accuracy - 0.47379406307977734\n",
      "Epoch 807: loss - 3.03168045833071; accuracy - 0.47379406307977734\n",
      "Epoch 808: loss - 3.031612869206961; accuracy - 0.47379406307977734\n",
      "Epoch 809: loss - 3.031546050962579; accuracy - 0.47379406307977734\n",
      "Epoch 810: loss - 3.03147271767618; accuracy - 0.47379406307977734\n",
      "Epoch 811: loss - 3.0314094550313224; accuracy - 0.47379406307977734\n",
      "Epoch 812: loss - 3.031340352790033; accuracy - 0.47379406307977734\n",
      "Epoch 813: loss - 3.0312691709106176; accuracy - 0.47379406307977734\n",
      "Epoch 814: loss - 3.031202956232379; accuracy - 0.47379406307977734\n",
      "Epoch 815: loss - 3.031137939176223; accuracy - 0.47379406307977734\n",
      "Epoch 816: loss - 3.0310683836521157; accuracy - 0.47379406307977734\n",
      "Epoch 817: loss - 3.0310010947632655; accuracy - 0.47379406307977734\n",
      "Epoch 818: loss - 3.03093626669475; accuracy - 0.47379406307977734\n",
      "Epoch 819: loss - 3.030869640092018; accuracy - 0.47379406307977734\n",
      "Epoch 820: loss - 3.030801500482329; accuracy - 0.47379406307977734\n",
      "Epoch 821: loss - 3.0307345503115255; accuracy - 0.47379406307977734\n",
      "Epoch 822: loss - 3.030670996499637; accuracy - 0.47379406307977734\n",
      "Epoch 823: loss - 3.0306074071903617; accuracy - 0.47379406307977734\n",
      "Epoch 824: loss - 3.030538329498666; accuracy - 0.47379406307977734\n",
      "Epoch 825: loss - 3.0304769680097507; accuracy - 0.47379406307977734\n"
     ]
    }
   ],
   "source": [
    "def train(data, word2index, model, epochs, loss_func, optimizer):\n",
    "    \"\"\"\n",
    "    This is a trainer function to train our SkipGram model.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for epoch_no in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        e_loss = 0\n",
    "        for context, targets in data:\n",
    "            # Step 1. Prepare the inputs to be passed to the model (i.e, turn \n",
    "            # the words into integer indices and wrap them in tensors)\n",
    "            context_idxs = torch.tensor([word2index[context]], dtype=torch.long).cuda()\n",
    "\n",
    "            # Step 2. Recall that torch *accumulates* gradients. Before passing \n",
    "            # in a new instance, you need to zero out the gradients from the old\n",
    "            # instance\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step 3. Run the forward pass, getting log probabilities over next\n",
    "            # words\n",
    "            log_probs = model(context_idxs)\n",
    "\n",
    "            # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "            # word wrapped in a tensor)\n",
    "            target_list = torch.tensor([word2index[w] for w in targets], dtype=torch.long).cuda()\n",
    "            loss = loss_func(log_probs, target_list)\n",
    "\n",
    "            # Step 5. Do the backward pass and update the gradient\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Step 6. Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "            e_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            e_loss /= len(data)\n",
    "            accuracy = check_accuracy(model, data, word2index)\n",
    "\n",
    "            print(f\"Epoch {epoch_no+1}: loss - {e_loss}; accuracy - {accuracy}\")\n",
    "\n",
    "            losses.append(e_loss)\n",
    "            accuracies.append(accuracy)\n",
    "\n",
    "    return losses, accuracies, model\n",
    "\n",
    "losses, accuracies, model = train(data, word2index, model, epochs, loss_function, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-statistics",
   "metadata": {},
   "source": [
    "### 3. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "gross-affect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display losses over time\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "seventh-scout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display accuracies over time\n",
    "plt.figure()\n",
    "plt.plot(accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2385632e",
   "metadata": {},
   "source": [
    "### 4. Extract embedding and play with it? (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52b71f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78306048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SkipGram(\n",
       "  (embeddings): Embedding(389, 20)\n",
       "  (linear1): Linear(in_features=20, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=1556, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = SkipGram(context_size = 2, embedding_dim = 20, vocab_size = len(vocab))\n",
    "model = model.cuda()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12275636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embeddings.weight',\n",
       "              tensor([[-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
       "                      [ 0.7299, -1.3819,  0.1418,  ...,  1.6372, -0.9238,  0.7384],\n",
       "                      [ 2.4948,  1.0473, -0.4565,  ..., -0.0041,  0.3968,  1.8590],\n",
       "                      ...,\n",
       "                      [-0.4269, -0.3590,  0.2175,  ..., -1.4402,  0.1553, -0.2601],\n",
       "                      [-0.3633, -0.1142, -1.8354,  ..., -1.0230, -1.2687, -1.1595],\n",
       "                      [-1.6997, -0.2512,  0.5528,  ..., -1.0109,  1.5953,  0.5354]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear1.weight',\n",
       "              tensor([[ 0.0665,  0.1744,  0.0909,  ..., -0.0578, -0.1217, -0.1218],\n",
       "                      [ 0.0216, -0.6805,  0.2752,  ...,  0.3176, -0.3904, -0.6062],\n",
       "                      [-0.2093, -0.2896, -0.0120,  ..., -0.2630,  0.2270, -0.3779],\n",
       "                      ...,\n",
       "                      [ 0.4554, -0.6091,  0.0318,  ..., -0.3545,  0.0031,  0.3905],\n",
       "                      [-0.3193, -0.5676, -0.2340,  ..., -0.1876, -0.5300, -0.5404],\n",
       "                      [-0.4049, -0.1876,  0.1362,  ...,  0.1025, -0.1280,  0.3853]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear1.bias',\n",
       "              tensor([ 0.2393,  0.5866,  0.0427,  0.1932,  0.3957,  0.1192,  0.0325,  0.4513,\n",
       "                       0.2596,  0.0201, -0.0753, -0.0112,  0.3559,  0.1564,  0.0092,  0.2747,\n",
       "                       0.1861,  0.1876,  0.4835,  0.4722,  0.4204,  0.4284,  0.2743,  0.3086,\n",
       "                       0.2620,  0.2319,  0.3366,  0.0041,  0.3414,  0.5078, -0.0552,  0.3616,\n",
       "                       0.4226,  0.4502,  0.3905,  0.0771,  0.0041, -0.0971,  0.2045, -0.0199,\n",
       "                       0.0952,  0.1832,  0.1154,  0.3004,  0.1363, -0.0539,  0.2896,  0.1549,\n",
       "                       0.3186, -0.1108,  0.0983,  0.1618,  0.1777, -0.0616, -0.0078,  0.1489,\n",
       "                       0.1505,  0.3689,  0.3022,  0.3297,  0.2844,  0.2476,  0.0322,  0.0231,\n",
       "                       0.3499,  0.3655,  0.3683,  0.3650,  0.1685,  0.2854,  0.2547,  0.4058,\n",
       "                       0.2569,  0.0551,  0.0759,  0.5901,  0.4613, -0.0286,  0.2200,  0.3655,\n",
       "                       0.1798,  0.2129, -0.0247,  0.3550, -0.0428,  0.1818,  0.3312,  0.1473,\n",
       "                       0.5320, -0.1024,  0.3042,  0.0772,  0.1051,  0.0193,  0.3375,  0.2401,\n",
       "                       0.0332,  0.4885,  0.3995,  0.2752,  0.4293,  0.3147, -0.0892,  0.1429,\n",
       "                       0.1007, -0.1263,  0.2149,  0.4418,  0.0768, -0.0175, -0.0684,  0.2860,\n",
       "                       0.3255,  0.4751,  0.4537,  0.1001,  0.1138,  0.1001,  0.3494, -0.1069,\n",
       "                       0.0774,  0.1440, -0.0384,  0.5365,  0.1382,  0.2129,  0.5649,  0.0292,\n",
       "                       0.1535,  0.2940,  0.1826, -0.0508,  0.5659,  0.3114,  0.4401,  0.1576,\n",
       "                       0.4373, -0.0181, -0.0143,  0.0665,  0.0136,  0.3256,  0.2166,  0.3985,\n",
       "                       0.0585,  0.1962,  0.5955,  0.1107,  0.0951,  0.1842,  0.4807,  0.1515,\n",
       "                      -0.1468,  0.2557,  0.2807, -0.0598,  0.1871, -0.0114,  0.2125,  0.1689,\n",
       "                       0.3972,  0.5544,  0.2810,  0.1992,  0.4330,  0.0101,  0.2621,  0.1092,\n",
       "                       0.2264,  0.0050,  0.1131,  0.0796, -0.0374, -0.0496,  0.3204,  0.1243,\n",
       "                       0.3372,  0.1541,  0.3162, -0.0194,  0.0789, -0.0641,  0.1198,  0.2587,\n",
       "                       0.0491,  0.4205,  0.1208,  0.3335,  0.5268,  0.3450,  0.2644, -0.0637,\n",
       "                       0.3892,  0.2785, -0.0397,  0.5450,  0.3437,  0.1155,  0.3635,  0.3290,\n",
       "                       0.4022,  0.2219, -0.0675,  0.3194,  0.2019,  0.3714,  0.1523,  0.2091,\n",
       "                       0.4352,  0.4026,  0.4595, -0.0799,  0.0660,  0.1461, -0.0008,  0.2891,\n",
       "                       0.0391,  0.0285,  0.3754, -0.0831,  0.1395,  0.1708,  0.4535, -0.0018,\n",
       "                       0.2942,  0.2065,  0.0824,  0.5345,  0.2870,  0.3431,  0.0689,  0.4045,\n",
       "                      -0.0549,  0.2623,  0.2456,  0.1387,  0.1590,  0.2837,  0.1945,  0.1988,\n",
       "                      -0.0428,  0.2100,  0.2048,  0.0567,  0.2949,  0.3293,  0.1739,  0.1004,\n",
       "                       0.1503,  0.1863, -0.0172,  0.1326,  0.3267, -0.0186,  0.0221,  0.1988,\n",
       "                       0.1474, -0.0219,  0.0773,  0.4240, -0.0379,  0.3688,  0.2551,  0.3172,\n",
       "                       0.4038, -0.1110, -0.0168,  0.5783,  0.3507,  0.3059,  0.3790, -0.0392,\n",
       "                       0.0169,  0.1437,  0.2214,  0.0241,  0.1070,  0.5034,  0.1827,  0.2362,\n",
       "                       0.0652, -0.1520,  0.3889, -0.0771,  0.0859,  0.0221,  0.2786,  0.3543,\n",
       "                       0.0056,  0.1199,  0.1877,  0.2952,  0.5531,  0.4749,  0.0973,  0.3166,\n",
       "                       0.0952,  0.0341, -0.0401,  0.0109, -0.0007,  0.4349, -0.0384,  0.5459,\n",
       "                       0.2163, -0.0195,  0.3214,  0.1034,  0.5943,  0.2454,  0.3575,  0.1993,\n",
       "                       0.1096,  0.4058,  0.4889,  0.4222,  0.2282,  0.0882,  0.1115,  0.5016,\n",
       "                       0.1031,  0.1545,  0.0270,  0.0627,  0.2088,  0.0638, -0.0596, -0.0910,\n",
       "                       0.0844,  0.1804, -0.0399,  0.4996,  0.1623,  0.5265,  0.2126,  0.3436,\n",
       "                      -0.0670,  0.0751,  0.1288,  0.0450,  0.1493, -0.0593,  0.1424,  0.0404,\n",
       "                       0.0820,  0.5859,  0.0554,  0.0884,  0.2699,  0.2838,  0.2850,  0.4341,\n",
       "                       0.4713,  0.3115,  0.4740,  0.0698,  0.4725,  0.3329, -0.1162,  0.3375,\n",
       "                       0.1790, -0.0158,  0.4847,  0.0471,  0.0507,  0.1072, -0.0008,  0.3783,\n",
       "                       0.3076,  0.4098,  0.2352, -0.1583,  0.3892,  0.4428,  0.3323,  0.5478,\n",
       "                       0.0789,  0.0530, -0.0171, -0.0815,  0.0817, -0.1415,  0.3666, -0.0102,\n",
       "                      -0.0266,  0.4605,  0.0500,  0.1305,  0.1524,  0.3946,  0.5025,  0.1040,\n",
       "                       0.2502,  0.2490,  0.1647,  0.4533,  0.0320,  0.0752,  0.3736,  0.1218,\n",
       "                       0.2780,  0.2742,  0.2774,  0.2613,  0.4619, -0.0340,  0.4977,  0.3150,\n",
       "                       0.5090,  0.2712,  0.3247, -0.0589,  0.2535,  0.3792,  0.2377,  0.1357,\n",
       "                       0.4694,  0.0551,  0.5136,  0.0521,  0.4288,  0.3112,  0.5569,  0.0736,\n",
       "                       0.4872,  0.5636, -0.0313,  0.4224,  0.3268, -0.1450,  0.2030,  0.0718,\n",
       "                       0.2534, -0.1028,  0.1501,  0.3676, -0.0163,  0.0791,  0.4095,  0.0720,\n",
       "                       0.1916,  0.2618,  0.0209,  0.2745, -0.0492, -0.0306,  0.2342, -0.0383,\n",
       "                       0.1798,  0.0704,  0.0652,  0.1295,  0.0948,  0.1676,  0.1726,  0.3067,\n",
       "                       0.3372, -0.0211, -0.0212,  0.1204,  0.1763,  0.1928,  0.4993,  0.2133,\n",
       "                       0.2000, -0.0423,  0.1506,  0.4486,  0.2359,  0.1606,  0.2512,  0.2788,\n",
       "                       0.2616,  0.1946,  0.0638,  0.1574,  0.0847,  0.4813,  0.4649,  0.4864,\n",
       "                       0.4350,  0.2638,  0.5454,  0.5222,  0.3389,  0.5171,  0.3432,  0.4295,\n",
       "                       0.0442,  0.2753,  0.1658,  0.5994,  0.4942, -0.0815,  0.1175,  0.4713,\n",
       "                       0.4145,  0.3457,  0.4625,  0.4680,  0.3499,  0.2899,  0.5472,  0.0895,\n",
       "                       0.1151,  0.2807,  0.3386,  0.0132,  0.1320,  0.1785,  0.0157,  0.1210],\n",
       "                     device='cuda:0')),\n",
       "             ('linear2.weight',\n",
       "              tensor([[-0.0084,  0.0350,  0.0291,  ..., -0.0008,  0.0115,  0.0528],\n",
       "                      [ 0.0308,  0.0742, -0.0192,  ...,  0.0042, -0.0456, -0.0387],\n",
       "                      [-0.0444,  0.0224,  0.0658,  ..., -0.0435, -0.0313,  0.0288],\n",
       "                      ...,\n",
       "                      [-0.0498,  0.1208,  0.0349,  ...,  0.0150,  0.0157, -0.0016],\n",
       "                      [ 0.0039,  0.0601,  0.0037,  ...,  0.1019, -0.0418, -0.0019],\n",
       "                      [-0.0256, -0.0209, -0.0327,  ..., -0.0472, -0.0173,  0.0342]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear2.bias',\n",
       "              tensor([ 0.0416, -0.0500,  0.0275,  ..., -0.0326, -0.0082, -0.0044],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the model's state_dict\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33425afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
       "        [ 0.7299, -1.3819,  0.1418,  ...,  1.6372, -0.9238,  0.7384],\n",
       "        [ 2.4948,  1.0473, -0.4565,  ..., -0.0041,  0.3968,  1.8590],\n",
       "        ...,\n",
       "        [-0.4269, -0.3590,  0.2175,  ..., -1.4402,  0.1553, -0.2601],\n",
       "        [-0.3633, -0.1142, -1.8354,  ..., -1.0230, -1.2687, -1.1595],\n",
       "        [-1.6997, -0.2512,  0.5528,  ..., -1.0109,  1.5953,  0.5354]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the model's embedding weights\n",
    "model.state_dict()['embeddings.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "249df5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(389, 20)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the embedding weights\n",
    "embedding_layer = nn.Embedding(len(vocab), 20)\n",
    "embedding_layer.weight = torch.nn.Parameter(model.state_dict()['embeddings.weight'])\n",
    "embedding_layer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e31e641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68]\n"
     ]
    }
   ],
   "source": [
    "print([word2index['his']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c83c2cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        ...,\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "his_context_id = torch.zeros(len(vocab)).cuda().long()\n",
    "his_context_id[word2index['his']] = 1.\n",
    "his_vec = embedding_layer(his_context_id)\n",
    "print(his_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ccc56ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        ...,\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "her_context_id = torch.zeros(len(vocab)).cuda().long()\n",
    "her_context_id[word2index['her']] = 1.\n",
    "her_vec = embedding_layer(her_context_id)\n",
    "print(her_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d2eed2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        ...,\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lion_context_id = torch.zeros(len(vocab)).cuda().long()\n",
    "lion_context_id[word2index['lion']] = 1.\n",
    "lion_vec = embedding_layer(lion_context_id)\n",
    "print(lion_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb00522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        ...,\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749],\n",
      "        [-0.8982, -0.3106, -1.1535,  ..., -0.7977, -0.3934,  1.7749]],\n",
      "       device='cuda:0', grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lioness_context_id = torch.zeros(len(vocab)).cuda().long()\n",
    "lioness_context_id[word2index['lioness']] = 1.\n",
    "lioness_vec = embedding_layer(lioness_context_id)\n",
    "print(lioness_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6620521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity of 'his' and 'her': tensor([0.9915, 0.9708, 0.9967, 1.0000, 0.5326, 0.9839, 1.0000, 0.8660, 1.0000,\n",
      "        0.9633, 0.9975, 0.9795, 0.9995, 0.9999, 0.7005, 0.9991, 0.9985, 0.9762,\n",
      "        0.9954, 0.9991], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Cosine similarity of 'lion' and 'lioness': tensor([0.9915, 0.9708, 0.9967, 1.0000, 0.5326, 0.9839, 1.0000, 0.8660, 1.0000,\n",
      "        0.9633, 0.9975, 0.9795, 0.9995, 0.9999, 0.7005, 0.9991, 0.9985, 0.9762,\n",
      "        0.9954, 0.9991], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "his_her_sim = F.cosine_similarity(his_vec, her_vec, dim=0)\n",
    "print(f\"Cosine similarity of 'his' and 'her': {his_her_sim}\")\n",
    "lion_lioness_sim = F.cosine_similarity(lion_vec, lioness_vec, dim=0)\n",
    "print(f\"Cosine similarity of 'lion' and 'lioness': {lion_lioness_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b71436ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity of 'his' and 'lion': tensor([0.9915, 0.9708, 0.9967, 1.0000, 0.5326, 0.9839, 1.0000, 0.8660, 1.0000,\n",
      "        0.9633, 0.9975, 0.9795, 0.9995, 0.9999, 0.7005, 0.9991, 0.9985, 0.9762,\n",
      "        0.9954, 0.9991], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Cosine similarity of 'his' and 'lioness': tensor([0.9915, 0.9708, 0.9967, 1.0000, 0.5326, 0.9839, 1.0000, 0.8660, 1.0000,\n",
      "        0.9633, 0.9975, 0.9795, 0.9995, 0.9999, 0.7005, 0.9991, 0.9985, 0.9762,\n",
      "        0.9954, 0.9991], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Cosine similarity of 'her' and 'lion': tensor([0.9915, 0.9708, 0.9967, 1.0000, 0.5326, 0.9839, 1.0000, 0.8660, 1.0000,\n",
      "        0.9633, 0.9975, 0.9795, 0.9995, 0.9999, 0.7005, 0.9991, 0.9985, 0.9762,\n",
      "        0.9954, 0.9991], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Cosine similarity of 'her' and 'lioness': tensor([0.9915, 0.9708, 0.9967, 1.0000, 0.5326, 0.9839, 1.0000, 0.8660, 1.0000,\n",
      "        0.9633, 0.9975, 0.9795, 0.9995, 0.9999, 0.7005, 0.9991, 0.9985, 0.9762,\n",
      "        0.9954, 0.9991], device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "his_lion_sim = F.cosine_similarity(his_vec, lion_vec, dim=0)\n",
    "print(f\"Cosine similarity of 'his' and 'lion': {his_lion_sim}\")\n",
    "his_lioness_sim = F.cosine_similarity(his_vec, lioness_vec, dim=0)\n",
    "print(f\"Cosine similarity of 'his' and 'lioness': {his_lioness_sim}\")\n",
    "her_lion_sim = F.cosine_similarity(her_vec, lion_vec, dim=0)\n",
    "print(f\"Cosine similarity of 'her' and 'lion': {her_lion_sim}\")\n",
    "her_lioness_sim = F.cosine_similarity(her_vec, lioness_vec, dim=0)\n",
    "print(f\"Cosine similarity of 'her' and 'lioness': {her_lioness_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a718d1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Context Words from Untrained Model: ['being', 'company', 'as', 'wells']\n",
      "Predicted Context Words from Trained Model: ['lioness', 'the', 'king', 'to']\n"
     ]
    }
   ],
   "source": [
    "def predict_context_words(skipgram_model, sample_input):\n",
    "    context_pred_idxs = get_prediction(sample_input, skipgram_model, word2index).tolist()\n",
    "    context_preds = [index2word[idx] for idx in context_pred_idxs]\n",
    "    return context_preds\n",
    "\n",
    "# Initialize another untrained SkipGram model to compare against\n",
    "untrained_model = SkipGram(context_size = 2, embedding_dim = 20, vocab_size = len(vocab))\n",
    "untrained_model = untrained_model.cuda()\n",
    "\n",
    "# Get predictions\n",
    "untrained_predictions = predict_context_words(untrained_model, \"lion\")\n",
    "trained_predictions = predict_context_words(model, \"lion\")\n",
    "\n",
    "print(\"Predicted Context Words from Untrained Model: {}\".format(untrained_predictions))\n",
    "print(\"Predicted Context Words from Trained Model: {}\".format(trained_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-banking",
   "metadata": {},
   "source": [
    "### Questions and expected answers for the report\n",
    "\n",
    "The questions listed below are related to Part 2.\n",
    "\n",
    "P2-QA. Copy and paste your SkipGram class code (Task #1 in this notebook).\n",
    "\n",
    "P2-QB. Copy and paste your train function (Task #2 in the notebook), along with any helper functions you might have used (e.g., a function to compute the accuracy of your model after each iteration).\n",
    "Please also copy and paste the function call with the parameters you used for the train() function.\n",
    "\n",
    "P2-QC. Why is the SkipGram model much more difficult to train than the CBoW?\n",
    "Is it problematic if it does not reach a 100% accuracy on the task it is being trained on?\n",
    "\n",
    "P2-QD. If we were to evaluate this model by using intrinsic methods, what could be a possible approach to do so?\n",
    "\n",
    "P2-QE. (Optional) Please submit any additional code you might that will demonstrate the performance/problems of the word embedding you have trained!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-luther",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
